{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../resuneta/src')\n",
    "sys.path.append('../../resuneta/nn/loss')\n",
    "sys.path.append('../../resuneta/models')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../MXNet-ResUNeta/')\n",
    "\n",
    "from bound_dist import get_distance, get_boundary\n",
    "from loss import Tanimoto_with_dual_masked\n",
    "from resunet_d6_causal_mtskcolor_ddist import *\n",
    "from resunet_d7_causal_mtskcolor_ddist import *\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(x, y):\n",
    "    if type(x).__module__ == 'numpy':\n",
    "        intersection = np.logical_and(x, y)\n",
    "        return 2. * np.sum(intersection) / (np.sum(x) + np.sum(y))\n",
    "    else:\n",
    "        intersection = mx.ndarray.op.broadcast_logical_and(x, y)\n",
    "        return 2. * mx.nd.sum(intersection) / (mx.nd.sum(x) + mx.nd.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visdom_visualize_batch(vis, img, extent, boundary, distance,\n",
    "                           extent_pred, boundary_pred, distance_pred,\n",
    "                           hsv, hsv_pred, mask, title=\"Train images\"):\n",
    "\n",
    "    img, extent, boundary, distance = img.asnumpy(), extent.asnumpy(), boundary.asnumpy(), distance.asnumpy()\n",
    "    extent_pred, boundary_pred = extent_pred.asnumpy(), boundary_pred.asnumpy()\n",
    "    distance_pred, hsv, hsv_pred = distance_pred.asnumpy(), hsv.asnumpy(), hsv_pred.asnumpy()\n",
    "    mask = mask.asnumpy()\n",
    "\n",
    "    # put everything in one window\n",
    "    batch_size, nchannels, nrows, ncols = img.shape\n",
    "    padding = 10\n",
    "    items = [img, hsv, hsv_pred, extent, extent_pred, \n",
    "             boundary, boundary_pred, distance, distance_pred,\n",
    "             mask]\n",
    "    result = np.zeros((3, len(items)*nrows + (len(items)-1)*padding, batch_size*ncols + (batch_size-1)*padding))\n",
    "\n",
    "    for j, item in enumerate(items):\n",
    "\n",
    "        if item.shape[1] == 1:\n",
    "            item = np.tile(item, (1,3,1,1)) * 255.\n",
    "\n",
    "        if j == 1 or j == 2: # convert HSV to RGB\n",
    "            item = np.moveaxis(item, 1, -1) * 255.\n",
    "            for i in range(batch_size):\n",
    "                item[i] = cv2.cvtColor(item[i].astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            item = np.moveaxis(item, -1, 1)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            result[:, j*(nrows+padding):(j+1)*nrows+j*padding, i*(ncols+padding):(i+1)*ncols+i*padding] = item[i]\n",
    "    vis.images(result, nrow=1, win=title, opts={'title': title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, model, tanimoto_dual, trainer, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # training set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(train_dataloader, desc='Training epoch {}'.format(epoch))):\n",
    "        \n",
    "        with autograd.record():\n",
    "\n",
    "            img = img.as_in_context(ctx)\n",
    "            extent = extent.as_in_context(ctx)\n",
    "            boundary = boundary.as_in_context(ctx)\n",
    "            distance = distance.as_in_context(ctx)\n",
    "            hsv = hsv.as_in_context(ctx)\n",
    "            mask = mask.as_in_context(ctx)\n",
    "            nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "            \n",
    "            logits, bound, dist, convc = model(img)\n",
    "            \n",
    "            # multi-task loss\n",
    "            # TODO: wrap this in a custom loss function / class\n",
    "            loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "            loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "            loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "            loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask)) # don't mask hsv\n",
    "\n",
    "            loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "            \n",
    "        loss.backward()\n",
    "        trainer.step(args['batch_size'])\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "#         print(\"logits.shape\", logits.shape)\n",
    "#         print(\"extent.shape\", extent.shape)\n",
    "#         print(\"mask.shape\", mask.shape)\n",
    "            \n",
    "        # mask out unlabeled pixels            \n",
    "#         print(\"made it here -3\")\n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "#         print(\"logits_reshaped.shape\", logits_reshaped.shape)\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "#         print(\"nonmask_idx\", nonmask_idx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "#         print(\"logits_masked\", logits_masked)\n",
    "#         print(\"logits_masked.shape\", logits_masked.shape)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "#         print(\"extent_masked\", extent_masked)\n",
    "#         print(\"extent_masked.shape\", extent_masked.shape)\n",
    "\n",
    "#         print(\"made it here -1\")\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5) # logits_masked[[0],:,:]\n",
    "#         print(\"extent_predicted_classes\", extent_predicted_classes)\n",
    "#         print(\"extent_predicted_classes.shape\", extent_predicted_classes.shape)\n",
    "        \n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 1\")\n",
    "        \n",
    "        # f1 score\n",
    "#         prediction = logits[:,0,:,:].reshape(-1)\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 2\")\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 3\")\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 4\")\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dataloader, model, tanimoto_dual, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # validation set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(val_dataloader, desc='Validation epoch {}'.format(epoch))):\n",
    "\n",
    "        img = img.as_in_context(ctx)\n",
    "        extent = extent.as_in_context(ctx)\n",
    "        boundary = boundary.as_in_context(ctx)\n",
    "        distance = distance.as_in_context(ctx)\n",
    "        hsv = hsv.as_in_context(ctx)\n",
    "        mask = mask.as_in_context(ctx)\n",
    "        nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "\n",
    "        logits, bound, dist, convc = model(img)\n",
    "        \n",
    "        # multi-task loss\n",
    "        # TODO: wrap this in a custom loss function / class\n",
    "        loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "        loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "        loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "        loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask))\n",
    "\n",
    "        loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        # mask out unlabeled pixels            \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask, title=\"Val images\")\n",
    "        \n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_africa(country, train_names, val_names, test_names, \n",
    "               train_names_label, val_names_label, test_names_label,\n",
    "               trained_model=None,\n",
    "               epochs=100, lr=0.001, lr_decay=None, n_filters=16, batch_size=8,\n",
    "               n_classes=1, model_type='resunet-d6', month='janFebMar',\n",
    "               codes_to_keep=[1, 2],\n",
    "               folder_suffix='',\n",
    "               boundary_kernel_size=3,\n",
    "               ctx_name='cpu',\n",
    "               gpu_id=0):\n",
    "    \n",
    "    # Set MXNet ctx\n",
    "    if ctx_name == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    elif ctx_name == 'gpu':\n",
    "        ctx = mx.gpu(gpu_id)\n",
    "    \n",
    "    # Set up names of directories and paths for saving\n",
    "    if trained_model is None:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if lr_decay:\n",
    "            folder_name = folder_name + '_lrdecay-'+str(lr_decay)\n",
    "            \n",
    "        # define model\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.initialize()\n",
    "        model.hybridize()\n",
    "        model.collect_params().reset_ctx(ctx)\n",
    "        \n",
    "    else:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix+'_finetuned'\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.load_parameters(trained_model, ctx=ctx)\n",
    "        \n",
    "    save_path = os.path.join('../experiments/', country, folder_name)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_model_name = os.path.join(save_path, \"model.params\")\n",
    "    \n",
    "    # Visdom\n",
    "    env_name = country + '_' + folder_name\n",
    "    vis = visdom.Visdom(port=8097, env=env_name)\n",
    "    \n",
    "    # Arguments\n",
    "    args = {}\n",
    "    args['batch_size'] = batch_size\n",
    "    args['ctx_name'] = ctx_name\n",
    "    args['gpu'] = gpu_id\n",
    "    args['visdom'] = vis\n",
    "    args['visdom_every'] = 20\n",
    "\n",
    "    # Define train/val/test splits\n",
    "    train_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='train', \n",
    "        image_names=train_names, \n",
    "        label_names=train_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    val_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='val', \n",
    "        image_names=val_names, \n",
    "        label_names=val_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    test_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='test', \n",
    "        image_names=test_names, \n",
    "        label_names=test_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = gluon.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # define loss function\n",
    "    tanimoto_dual = Tanimoto_with_dual_masked()\n",
    "    if lr_decay:\n",
    "        schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "    else:\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "    # containers for metrics to log\n",
    "    train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                     'train_mcc': [], 'train_dice': []}\n",
    "    val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "                   'val_mcc': [], 'val_dice': []}\n",
    "    best_mcc = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training set\n",
    "        train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n",
    "            train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
    "\n",
    "        # training set metrics\n",
    "        train_loss_avg = train_loss / len(train_dataset)\n",
    "        train_metrics['train_loss'].append(train_loss_avg)\n",
    "        train_metrics['train_acc'].append(train_accuracy.get()[1])\n",
    "        train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "        train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "        train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "\n",
    "        # validation set\n",
    "        val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model(\n",
    "            val_dataloader, model, tanimoto_dual, epoch, args)\n",
    "\n",
    "        # validation set metrics\n",
    "        val_loss_avg = val_loss / len(val_dataset)\n",
    "        val_metrics['val_loss'].append(val_loss_avg)\n",
    "        val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "        val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "        val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "        val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "        print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "\n",
    "        # save model based on best MCC metric\n",
    "        if val_mcc.get()[1] > best_mcc:\n",
    "            model.save_parameters(save_model_name)\n",
    "            best_mcc = val_mcc.get()[1]\n",
    "\n",
    "        # save metrics\n",
    "        metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "        metrics.to_csv(os.path.join(save_path, 'metrics.csv'), index=False)\n",
    "\n",
    "        # visdom\n",
    "        vis.line(Y=np.stack([train_metrics['train_loss'], val_metrics['val_loss']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"Loss\", \n",
    "                 opts=dict(legend=['train loss', 'val loss'], markers=False, title=\"Losses\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "                )\n",
    "        vis.line(Y=np.stack([train_metrics['train_mcc'], val_metrics['val_mcc']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"MCC\", \n",
    "                 opts=dict(legend=['train MCC', 'val MCC'], markers=False, title=\"MCC\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"MCC\")\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 16\n",
      "depth:= 1, nfilters: 32\n",
      "depth:= 2, nfilters: 64\n",
      "depth:= 3, nfilters: 128\n",
      "depth:= 4, nfilters: 256\n",
      "depth:= 5, nfilters: 512\n",
      "depth:= 6, nfilters: 256\n",
      "depth:= 7, nfilters: 128\n",
      "depth:= 8, nfilters: 64\n",
      "depth:= 9, nfilters: 32\n",
      "depth:= 10, nfilters: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Training epoch 1: 100%|██████████| 38/38 [00:22<00:00,  1.72it/s]\n",
      "Validation epoch 1: 100%|██████████| 579/579 [02:48<00:00,  3.44it/s]\n",
      "Training epoch 2:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Train loss 0.336, accuracy 0.677, F1-score 0.787, MCC: 0.090, Dice: 0.787\n",
      "    Val loss 0.316, accuracy 0.718, F1-score 0.817, MCC: 0.157, Dice: 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 38/38 [00:17<00:00,  2.17it/s]\n",
      "Validation epoch 2: 100%|██████████| 579/579 [02:49<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "    Train loss 0.284, accuracy 0.747, F1-score 0.838, MCC: 0.249, Dice: 0.838\n",
      "    Val loss 0.270, accuracy 0.759, F1-score 0.844, MCC: 0.277, Dice: 0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 38/38 [00:17<00:00,  2.17it/s]\n",
      "Validation epoch 3: 100%|██████████| 579/579 [02:44<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "    Train loss 0.258, accuracy 0.755, F1-score 0.839, MCC: 0.300, Dice: 0.839\n",
      "    Val loss 0.244, accuracy 0.773, F1-score 0.851, MCC: 0.334, Dice: 0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s]\n",
      "Validation epoch 4: 100%|██████████| 579/579 [02:47<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "    Train loss 0.240, accuracy 0.756, F1-score 0.838, MCC: 0.330, Dice: 0.838\n",
      "    Val loss 0.228, accuracy 0.771, F1-score 0.844, MCC: 0.376, Dice: 0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s]\n",
      "Validation epoch 5: 100%|██████████| 579/579 [02:39<00:00,  3.64it/s]\n",
      "Training epoch 6:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "    Train loss 0.231, accuracy 0.766, F1-score 0.845, MCC: 0.351, Dice: 0.845\n",
      "    Val loss 0.230, accuracy 0.780, F1-score 0.855, MCC: 0.358, Dice: 0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 38/38 [00:17<00:00,  2.20it/s]\n",
      "Validation epoch 6: 100%|██████████| 579/579 [02:42<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "    Train loss 0.227, accuracy 0.766, F1-score 0.843, MCC: 0.356, Dice: 0.843\n",
      "    Val loss 0.217, accuracy 0.781, F1-score 0.851, MCC: 0.405, Dice: 0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s]\n",
      "Validation epoch 7: 100%|██████████| 579/579 [03:03<00:00,  3.16it/s]\n",
      "Training epoch 8:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "    Train loss 0.224, accuracy 0.773, F1-score 0.848, MCC: 0.371, Dice: 0.848\n",
      "    Val loss 0.251, accuracy 0.784, F1-score 0.868, MCC: 0.278, Dice: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 38/38 [00:21<00:00,  1.80it/s]\n",
      "Validation epoch 8: 100%|██████████| 579/579 [03:24<00:00,  2.83it/s]\n",
      "Training epoch 9:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "    Train loss 0.224, accuracy 0.776, F1-score 0.850, MCC: 0.378, Dice: 0.850\n",
      "    Val loss 0.224, accuracy 0.772, F1-score 0.846, MCC: 0.373, Dice: 0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 38/38 [00:21<00:00,  1.81it/s]\n",
      "Validation epoch 9: 100%|██████████| 579/579 [03:27<00:00,  2.79it/s]\n",
      "Training epoch 10:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "    Train loss 0.220, accuracy 0.777, F1-score 0.853, MCC: 0.384, Dice: 0.853\n",
      "    Val loss 0.219, accuracy 0.787, F1-score 0.859, MCC: 0.393, Dice: 0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10: 100%|██████████| 38/38 [00:20<00:00,  1.85it/s]\n",
      "Validation epoch 10: 100%|██████████| 579/579 [03:27<00:00,  2.80it/s]\n",
      "Training epoch 11:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "    Train loss 0.221, accuracy 0.777, F1-score 0.851, MCC: 0.380, Dice: 0.851\n",
      "    Val loss 0.218, accuracy 0.796, F1-score 0.867, MCC: 0.394, Dice: 0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11: 100%|██████████| 38/38 [00:20<00:00,  1.84it/s]\n",
      "Validation epoch 11: 100%|██████████| 579/579 [03:26<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "    Train loss 0.216, accuracy 0.781, F1-score 0.852, MCC: 0.390, Dice: 0.852\n",
      "    Val loss 0.212, accuracy 0.782, F1-score 0.851, MCC: 0.408, Dice: 0.851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12: 100%|██████████| 38/38 [00:21<00:00,  1.80it/s]\n",
      "Validation epoch 12: 100%|██████████| 579/579 [03:21<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "    Train loss 0.214, accuracy 0.785, F1-score 0.857, MCC: 0.399, Dice: 0.857\n",
      "    Val loss 0.215, accuracy 0.777, F1-score 0.846, MCC: 0.410, Dice: 0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13: 100%|██████████| 38/38 [00:22<00:00,  1.68it/s]\n",
      "Validation epoch 13: 100%|██████████| 579/579 [03:20<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "    Train loss 0.215, accuracy 0.786, F1-score 0.857, MCC: 0.399, Dice: 0.857\n",
      "    Val loss 0.207, accuracy 0.796, F1-score 0.863, MCC: 0.430, Dice: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14: 100%|██████████| 38/38 [00:21<00:00,  1.73it/s]\n",
      "Validation epoch 14: 100%|██████████| 579/579 [03:12<00:00,  3.01it/s]\n",
      "Training epoch 15:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "    Train loss 0.211, accuracy 0.786, F1-score 0.857, MCC: 0.404, Dice: 0.857\n",
      "    Val loss 0.212, accuracy 0.775, F1-score 0.843, MCC: 0.425, Dice: 0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15: 100%|██████████| 38/38 [00:21<00:00,  1.78it/s]\n",
      "Validation epoch 15: 100%|██████████| 579/579 [03:23<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "    Train loss 0.211, accuracy 0.791, F1-score 0.861, MCC: 0.408, Dice: 0.861\n",
      "    Val loss 0.204, accuracy 0.797, F1-score 0.863, MCC: 0.437, Dice: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 16: 100%|██████████| 38/38 [00:21<00:00,  1.77it/s]\n",
      "Validation epoch 16: 100%|██████████| 579/579 [03:23<00:00,  2.84it/s]\n",
      "Training epoch 17:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "    Train loss 0.210, accuracy 0.790, F1-score 0.860, MCC: 0.413, Dice: 0.860\n",
      "    Val loss 0.207, accuracy 0.791, F1-score 0.857, MCC: 0.433, Dice: 0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 17: 100%|██████████| 38/38 [00:21<00:00,  1.74it/s]\n",
      "Validation epoch 17: 100%|██████████| 579/579 [03:21<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "    Train loss 0.208, accuracy 0.791, F1-score 0.862, MCC: 0.418, Dice: 0.862\n",
      "    Val loss 0.207, accuracy 0.800, F1-score 0.866, MCC: 0.438, Dice: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18: 100%|██████████| 38/38 [00:21<00:00,  1.77it/s]\n",
      "Validation epoch 18: 100%|██████████| 579/579 [03:20<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "    Train loss 0.208, accuracy 0.792, F1-score 0.861, MCC: 0.420, Dice: 0.861\n",
      "    Val loss 0.201, accuracy 0.811, F1-score 0.875, MCC: 0.453, Dice: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 19: 100%|██████████| 38/38 [00:22<00:00,  1.71it/s]\n",
      "Validation epoch 19: 100%|██████████| 579/579 [03:25<00:00,  2.81it/s]\n",
      "Training epoch 20:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "    Train loss 0.206, accuracy 0.793, F1-score 0.863, MCC: 0.423, Dice: 0.863\n",
      "    Val loss 0.202, accuracy 0.800, F1-score 0.865, MCC: 0.443, Dice: 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 20: 100%|██████████| 38/38 [00:20<00:00,  1.81it/s]\n",
      "Validation epoch 20: 100%|██████████| 579/579 [03:26<00:00,  2.80it/s]\n",
      "Training epoch 21:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "    Train loss 0.202, accuracy 0.797, F1-score 0.865, MCC: 0.433, Dice: 0.865\n",
      "    Val loss 0.202, accuracy 0.806, F1-score 0.871, MCC: 0.445, Dice: 0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 21: 100%|██████████| 38/38 [00:21<00:00,  1.76it/s]\n",
      "Validation epoch 21: 100%|██████████| 579/579 [03:17<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "    Train loss 0.204, accuracy 0.796, F1-score 0.864, MCC: 0.425, Dice: 0.864\n",
      "    Val loss 0.197, accuracy 0.812, F1-score 0.874, MCC: 0.463, Dice: 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 22: 100%|██████████| 38/38 [00:21<00:00,  1.78it/s]\n",
      "Validation epoch 22: 100%|██████████| 579/579 [03:23<00:00,  2.85it/s]\n",
      "Training epoch 23:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:\n",
      "    Train loss 0.203, accuracy 0.798, F1-score 0.866, MCC: 0.434, Dice: 0.866\n",
      "    Val loss 0.216, accuracy 0.800, F1-score 0.868, MCC: 0.417, Dice: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 23: 100%|██████████| 38/38 [00:20<00:00,  1.83it/s]\n",
      "Validation epoch 23: 100%|██████████| 579/579 [03:25<00:00,  2.82it/s]\n",
      "Training epoch 24:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:\n",
      "    Train loss 0.202, accuracy 0.800, F1-score 0.867, MCC: 0.432, Dice: 0.867\n",
      "    Val loss 0.201, accuracy 0.800, F1-score 0.865, MCC: 0.448, Dice: 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 24: 100%|██████████| 38/38 [00:21<00:00,  1.80it/s]\n",
      "Validation epoch 24: 100%|██████████| 579/579 [03:24<00:00,  2.84it/s]\n",
      "Training epoch 25:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:\n",
      "    Train loss 0.201, accuracy 0.796, F1-score 0.864, MCC: 0.431, Dice: 0.864\n",
      "    Val loss 0.216, accuracy 0.796, F1-score 0.869, MCC: 0.380, Dice: 0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 25: 100%|██████████| 38/38 [00:22<00:00,  1.71it/s]\n",
      "Validation epoch 25: 100%|██████████| 579/579 [03:21<00:00,  2.87it/s]\n",
      "Training epoch 26:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:\n",
      "    Train loss 0.202, accuracy 0.801, F1-score 0.867, MCC: 0.436, Dice: 0.867\n",
      "    Val loss 0.201, accuracy 0.800, F1-score 0.866, MCC: 0.438, Dice: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 26: 100%|██████████| 38/38 [00:21<00:00,  1.76it/s]\n",
      "Validation epoch 26: 100%|██████████| 579/579 [03:16<00:00,  2.95it/s]\n",
      "Training epoch 27:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:\n",
      "    Train loss 0.200, accuracy 0.799, F1-score 0.865, MCC: 0.435, Dice: 0.865\n",
      "    Val loss 0.199, accuracy 0.805, F1-score 0.869, MCC: 0.449, Dice: 0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 27: 100%|██████████| 38/38 [00:22<00:00,  1.71it/s]\n",
      "Validation epoch 27: 100%|██████████| 579/579 [03:24<00:00,  2.83it/s]\n",
      "Training epoch 28:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:\n",
      "    Train loss 0.199, accuracy 0.801, F1-score 0.868, MCC: 0.446, Dice: 0.868\n",
      "    Val loss 0.197, accuracy 0.799, F1-score 0.862, MCC: 0.457, Dice: 0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 28: 100%|██████████| 38/38 [00:21<00:00,  1.77it/s]\n",
      "Validation epoch 28: 100%|██████████| 579/579 [03:21<00:00,  2.88it/s]\n",
      "Training epoch 29:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:\n",
      "    Train loss 0.197, accuracy 0.803, F1-score 0.869, MCC: 0.451, Dice: 0.869\n",
      "    Val loss 0.196, accuracy 0.796, F1-score 0.859, MCC: 0.462, Dice: 0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 29: 100%|██████████| 38/38 [00:21<00:00,  1.79it/s]\n",
      "Validation epoch 29: 100%|██████████| 579/579 [03:22<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:\n",
      "    Train loss 0.197, accuracy 0.802, F1-score 0.868, MCC: 0.447, Dice: 0.868\n",
      "    Val loss 0.197, accuracy 0.792, F1-score 0.854, MCC: 0.473, Dice: 0.854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 30: 100%|██████████| 38/38 [00:21<00:00,  1.74it/s]\n",
      "Validation epoch 30: 100%|██████████| 579/579 [03:25<00:00,  2.82it/s]\n",
      "Training epoch 31:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:\n",
      "    Train loss 0.197, accuracy 0.804, F1-score 0.870, MCC: 0.450, Dice: 0.870\n",
      "    Val loss 0.198, accuracy 0.800, F1-score 0.863, MCC: 0.463, Dice: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 31: 100%|██████████| 38/38 [00:22<00:00,  1.71it/s]\n",
      "Validation epoch 31: 100%|██████████| 579/579 [03:21<00:00,  2.88it/s]\n",
      "Training epoch 32:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:\n",
      "    Train loss 0.198, accuracy 0.800, F1-score 0.867, MCC: 0.444, Dice: 0.867\n",
      "    Val loss 0.206, accuracy 0.814, F1-score 0.880, MCC: 0.439, Dice: 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 32: 100%|██████████| 38/38 [00:21<00:00,  1.77it/s]\n",
      "Validation epoch 32: 100%|██████████| 579/579 [03:20<00:00,  2.89it/s]\n",
      "Training epoch 33:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:\n",
      "    Train loss 0.197, accuracy 0.802, F1-score 0.868, MCC: 0.447, Dice: 0.868\n",
      "    Val loss 0.201, accuracy 0.792, F1-score 0.856, MCC: 0.447, Dice: 0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 33: 100%|██████████| 38/38 [00:21<00:00,  1.79it/s]\n",
      "Validation epoch 33: 100%|██████████| 579/579 [03:25<00:00,  2.82it/s]\n",
      "Training epoch 34:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:\n",
      "    Train loss 0.195, accuracy 0.803, F1-score 0.869, MCC: 0.453, Dice: 0.869\n",
      "    Val loss 0.193, accuracy 0.808, F1-score 0.870, MCC: 0.471, Dice: 0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 34: 100%|██████████| 38/38 [00:20<00:00,  1.81it/s]\n",
      "Validation epoch 34: 100%|██████████| 579/579 [03:13<00:00,  2.99it/s]\n",
      "Training epoch 35:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:\n",
      "    Train loss 0.195, accuracy 0.804, F1-score 0.868, MCC: 0.453, Dice: 0.868\n",
      "    Val loss 0.193, accuracy 0.804, F1-score 0.865, MCC: 0.472, Dice: 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 35: 100%|██████████| 38/38 [00:20<00:00,  1.83it/s]\n",
      "Validation epoch 35: 100%|██████████| 579/579 [03:22<00:00,  2.86it/s]\n",
      "Training epoch 36:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:\n",
      "    Train loss 0.194, accuracy 0.805, F1-score 0.871, MCC: 0.451, Dice: 0.871\n",
      "    Val loss 0.195, accuracy 0.803, F1-score 0.866, MCC: 0.461, Dice: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 36: 100%|██████████| 38/38 [00:20<00:00,  1.83it/s]\n",
      "Validation epoch 36: 100%|██████████| 579/579 [03:18<00:00,  2.91it/s]\n",
      "Training epoch 37:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:\n",
      "    Train loss 0.195, accuracy 0.807, F1-score 0.872, MCC: 0.453, Dice: 0.872\n",
      "    Val loss 0.199, accuracy 0.806, F1-score 0.871, MCC: 0.449, Dice: 0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 37: 100%|██████████| 38/38 [00:22<00:00,  1.68it/s]\n",
      "Validation epoch 37: 100%|██████████| 579/579 [03:38<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:\n",
      "    Train loss 0.194, accuracy 0.804, F1-score 0.869, MCC: 0.453, Dice: 0.869\n",
      "    Val loss 0.192, accuracy 0.802, F1-score 0.864, MCC: 0.478, Dice: 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 38: 100%|██████████| 38/38 [00:23<00:00,  1.60it/s]\n",
      "Validation epoch 38: 100%|██████████| 579/579 [03:41<00:00,  2.61it/s]\n",
      "Training epoch 39:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:\n",
      "    Train loss 0.194, accuracy 0.805, F1-score 0.869, MCC: 0.453, Dice: 0.869\n",
      "    Val loss 0.195, accuracy 0.806, F1-score 0.868, MCC: 0.470, Dice: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 39: 100%|██████████| 38/38 [00:23<00:00,  1.62it/s]\n",
      "Validation epoch 39: 100%|██████████| 579/579 [03:41<00:00,  2.62it/s]\n",
      "Training epoch 40:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39:\n",
      "    Train loss 0.194, accuracy 0.804, F1-score 0.871, MCC: 0.453, Dice: 0.871\n",
      "    Val loss 0.199, accuracy 0.792, F1-score 0.856, MCC: 0.453, Dice: 0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 40: 100%|██████████| 38/38 [00:22<00:00,  1.68it/s]\n",
      "Validation epoch 54: 100%|██████████| 579/579 [04:09<00:00,  2.32it/s]\n",
      "Training epoch 55:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54:\n",
      "    Train loss 0.188, accuracy 0.810, F1-score 0.874, MCC: 0.467, Dice: 0.874\n",
      "    Val loss 0.196, accuracy 0.812, F1-score 0.874, MCC: 0.466, Dice: 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 55: 100%|██████████| 38/38 [00:24<00:00,  1.57it/s]\n",
      "Validation epoch 81: 100%|██████████| 579/579 [04:05<00:00,  2.36it/s]\n",
      "Training epoch 82:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:\n",
      "    Train loss 0.180, accuracy 0.816, F1-score 0.877, MCC: 0.484, Dice: 0.877\n",
      "    Val loss 0.190, accuracy 0.804, F1-score 0.864, MCC: 0.482, Dice: 0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 82: 100%|██████████| 38/38 [00:25<00:00,  1.50it/s]\n",
      "Validation epoch 82: 100%|██████████| 579/579 [03:59<00:00,  2.41it/s]\n",
      "Training epoch 83:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82:\n",
      "    Train loss 0.181, accuracy 0.815, F1-score 0.877, MCC: 0.483, Dice: 0.877\n",
      "    Val loss 0.187, accuracy 0.818, F1-score 0.877, MCC: 0.496, Dice: 0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 83:  66%|██████▌   | 25/38 [00:18<00:09,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'partial-france'\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "lr_decay = None\n",
    "n_filters = 16\n",
    "batch_size = 8\n",
    "n_classes = 1\n",
    "model_type = 'resunet-d6'\n",
    "month = 'aprJulOctSeparate'\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 0\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "# trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "#     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "trained_model = None\n",
    "splits_path = '../data/splits/sherrie10k_planetImagery_splits_20x20_4x-downsampled_n100.csv'\n",
    "splits_df = pd.read_csv(splits_path)\n",
    "splits_df['image_id'] = splits_df['image_id'].astype(str).str.zfill(5)\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dir = '../data/planet/france/sherrie10k/monthly_mosaics_renamed_clipped_merged/1250px/4x_downsample/'\n",
    "label_dir = '../data/planet/france/sherrie10k/extent_labels/1250px_100field/4x_downsample/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for month in ['2019_04', '2019_07', '2019_10']:\n",
    "    for label_name in label_folder_imgs:\n",
    "        img_name = label_name.split('.')[0] + '_' + month + '.tif'\n",
    "        img_path = os.path.join(img_dir, month, img_name)\n",
    "        all_img_names.append(img_path)\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        all_label_names.append(label_path)\n",
    "\n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[0])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, n_filters=n_filters, batch_size=batch_size,\n",
    "           n_classes=n_classes, model_type=model_type, month=month,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix='_4x-downsampled_100field_n100',\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-generate partial labels for france and africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "mask_j = mx.nd.array([[[1,0,0],\n",
    "                       [0,1,0]],\n",
    "                      [[0,0,1],\n",
    "                       [0,0,0]]])\n",
    "mask_j = mask_j.reshape((mask_j.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. 0. 1. 0.]\n",
       " [0. 0. 1. 0. 0. 0.]]\n",
       "<NDArray 2x6 @cpu(0)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmask_idx = mx.np.nonzero(mask_j.as_np_ndarray())\n",
    "nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.nd.gather_nd(mask_j, nonmask_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.5097876]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanimoto = Tanimoto_with_dual_masked()\n",
    "test_pred = mx.nd.array([[[[0.1,0.1,0.8,0.9]]]])\n",
    "test_label = mx.nd.array([[[[0,0,1,0]]]])\n",
    "test_mask = mx.nd.array([[[[0,1,1,1]]]])\n",
    "tanimoto(test_pred, test_label, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 0.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[0. 0. 0.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 0. 0. 0. 0.]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]):\n",
    "    print(row)\n",
    "    \n",
    "mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.npx.set_np()\n",
    "mx.np.nonzero(mx.nd.array([[0,1,0],\n",
    "                           [0,0,0]]).as_np_ndarray())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = mx.nd.array([[ 0.1, 0.9, 0.5],\n",
    "                          [ 0.2, 0.3, 0.7]])\n",
    "test_array2 = mx.nd.array([[0, 2, 0],\n",
    "                           [0, 1, 0]])\n",
    "nonmask_idx = mx.np.nonzero(test_array2.as_np_ndarray())\n",
    "test_result = mx.nd.gather_nd(test_array, mx.nd.array(nonmask_idx))\n",
    "test_result2 = mx.nd.gather_nd(test_array2, mx.nd.array(nonmask_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.9 0.3]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 0.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result2 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet1.6.0)",
   "language": "python",
   "name": "conda_mxnet1.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
