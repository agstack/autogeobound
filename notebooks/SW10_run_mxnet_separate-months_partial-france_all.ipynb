{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../resuneta/src')\n",
    "sys.path.append('../../resuneta/nn/loss')\n",
    "sys.path.append('../../resuneta/models')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../MXNet-ResUNeta/')\n",
    "\n",
    "from bound_dist import get_distance, get_boundary\n",
    "from loss import Tanimoto_with_dual_masked\n",
    "from resunet_d6_causal_mtskcolor_ddist import *\n",
    "from resunet_d7_causal_mtskcolor_ddist import *\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(x, y):\n",
    "    if type(x).__module__ == 'numpy':\n",
    "        intersection = np.logical_and(x, y)\n",
    "        return 2. * np.sum(intersection) / (np.sum(x) + np.sum(y))\n",
    "    else:\n",
    "        intersection = mx.ndarray.op.broadcast_logical_and(x, y)\n",
    "        return 2. * mx.nd.sum(intersection) / (mx.nd.sum(x) + mx.nd.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visdom_visualize_batch(vis, img, extent, boundary, distance,\n",
    "                           extent_pred, boundary_pred, distance_pred,\n",
    "                           hsv, hsv_pred, mask, title=\"Train images\"):\n",
    "\n",
    "    img, extent, boundary, distance = img.asnumpy(), extent.asnumpy(), boundary.asnumpy(), distance.asnumpy()\n",
    "    extent_pred, boundary_pred = extent_pred.asnumpy(), boundary_pred.asnumpy()\n",
    "    distance_pred, hsv, hsv_pred = distance_pred.asnumpy(), hsv.asnumpy(), hsv_pred.asnumpy()\n",
    "    mask = mask.asnumpy()\n",
    "\n",
    "    # put everything in one window\n",
    "    batch_size, nchannels, nrows, ncols = img.shape\n",
    "    padding = 10\n",
    "    items = [img, hsv, hsv_pred, extent, extent_pred, \n",
    "             boundary, boundary_pred, distance, distance_pred,\n",
    "             mask]\n",
    "    result = np.zeros((3, len(items)*nrows + (len(items)-1)*padding, batch_size*ncols + (batch_size-1)*padding))\n",
    "\n",
    "    for j, item in enumerate(items):\n",
    "\n",
    "        if item.shape[1] == 1:\n",
    "            item = np.tile(item, (1,3,1,1)) * 255.\n",
    "\n",
    "        if j == 1 or j == 2: # convert HSV to RGB\n",
    "            item = np.moveaxis(item, 1, -1) * 255.\n",
    "            for i in range(batch_size):\n",
    "                item[i] = cv2.cvtColor(item[i].astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            item = np.moveaxis(item, -1, 1)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            result[:, j*(nrows+padding):(j+1)*nrows+j*padding, i*(ncols+padding):(i+1)*ncols+i*padding] = item[i]\n",
    "    vis.images(result, nrow=1, win=title, opts={'title': title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, model, tanimoto_dual, trainer, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # training set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(train_dataloader, desc='Training epoch {}'.format(epoch))):\n",
    "        \n",
    "        with autograd.record():\n",
    "\n",
    "            img = img.as_in_context(ctx)\n",
    "            extent = extent.as_in_context(ctx)\n",
    "            boundary = boundary.as_in_context(ctx)\n",
    "            distance = distance.as_in_context(ctx)\n",
    "            hsv = hsv.as_in_context(ctx)\n",
    "            mask = mask.as_in_context(ctx)\n",
    "            nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "            \n",
    "            logits, bound, dist, convc = model(img)\n",
    "            \n",
    "            # multi-task loss\n",
    "            # TODO: wrap this in a custom loss function / class\n",
    "            loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "            loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "            loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "            loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask)) # don't mask hsv\n",
    "\n",
    "            loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "            \n",
    "        loss.backward()\n",
    "        trainer.step(args['batch_size'])\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "#         print(\"logits.shape\", logits.shape)\n",
    "#         print(\"extent.shape\", extent.shape)\n",
    "#         print(\"mask.shape\", mask.shape)\n",
    "            \n",
    "        # mask out unlabeled pixels            \n",
    "#         print(\"made it here -3\")\n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "#         print(\"logits_reshaped.shape\", logits_reshaped.shape)\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "#         print(\"nonmask_idx\", nonmask_idx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "#         print(\"logits_masked\", logits_masked)\n",
    "#         print(\"logits_masked.shape\", logits_masked.shape)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "#         print(\"extent_masked\", extent_masked)\n",
    "#         print(\"extent_masked.shape\", extent_masked.shape)\n",
    "\n",
    "#         print(\"made it here -1\")\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5) # logits_masked[[0],:,:]\n",
    "#         print(\"extent_predicted_classes\", extent_predicted_classes)\n",
    "#         print(\"extent_predicted_classes.shape\", extent_predicted_classes.shape)\n",
    "        \n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 1\")\n",
    "        \n",
    "        # f1 score\n",
    "#         prediction = logits[:,0,:,:].reshape(-1)\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 2\")\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 3\")\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 4\")\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dataloader, model, tanimoto_dual, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # validation set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(val_dataloader, desc='Validation epoch {}'.format(epoch))):\n",
    "\n",
    "        img = img.as_in_context(ctx)\n",
    "        extent = extent.as_in_context(ctx)\n",
    "        boundary = boundary.as_in_context(ctx)\n",
    "        distance = distance.as_in_context(ctx)\n",
    "        hsv = hsv.as_in_context(ctx)\n",
    "        mask = mask.as_in_context(ctx)\n",
    "        nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "\n",
    "        logits, bound, dist, convc = model(img)\n",
    "        \n",
    "        # multi-task loss\n",
    "        # TODO: wrap this in a custom loss function / class\n",
    "        loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "        loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "        loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "        loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask))\n",
    "\n",
    "        loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        # mask out unlabeled pixels            \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask, title=\"Val images\")\n",
    "        \n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_africa(country, train_names, val_names, test_names, \n",
    "               train_names_label, val_names_label, test_names_label,\n",
    "               trained_model=None,\n",
    "               epochs=100, lr=0.001, lr_decay=None, n_filters=16, batch_size=8,\n",
    "               n_classes=1, model_type='resunet-d6', month='janFebMar',\n",
    "               codes_to_keep=[1, 2],\n",
    "               folder_suffix='',\n",
    "               boundary_kernel_size=3,\n",
    "               ctx_name='cpu',\n",
    "               gpu_id=0):\n",
    "    \n",
    "    # Set MXNet ctx\n",
    "    if ctx_name == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    elif ctx_name == 'gpu':\n",
    "        ctx = mx.gpu(gpu_id)\n",
    "    \n",
    "    # Set up names of directories and paths for saving\n",
    "    if trained_model is None:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if lr_decay:\n",
    "            folder_name = folder_name + '_lrdecay-'+str(lr_decay)\n",
    "            \n",
    "        # define model\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.initialize()\n",
    "        model.hybridize()\n",
    "        model.collect_params().reset_ctx(ctx)\n",
    "        \n",
    "    else:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix+'_finetuned'\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.load_parameters(trained_model, ctx=ctx)\n",
    "        \n",
    "    save_path = os.path.join('../experiments/', country, folder_name)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_model_name = os.path.join(save_path, \"model.params\")\n",
    "    \n",
    "    # Visdom\n",
    "    env_name = country + '_' + folder_name\n",
    "    vis = visdom.Visdom(port=8097, env=env_name)\n",
    "    \n",
    "    # Arguments\n",
    "    args = {}\n",
    "    args['batch_size'] = batch_size\n",
    "    args['ctx_name'] = ctx_name\n",
    "    args['gpu'] = gpu_id\n",
    "    args['visdom'] = vis\n",
    "    args['visdom_every'] = 20\n",
    "\n",
    "    # Define train/val/test splits\n",
    "    train_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='train', \n",
    "        image_names=train_names, \n",
    "        label_names=train_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    val_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='val', \n",
    "        image_names=val_names, \n",
    "        label_names=val_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    test_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='test', \n",
    "        image_names=test_names, \n",
    "        label_names=test_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = gluon.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # define loss function\n",
    "    tanimoto_dual = Tanimoto_with_dual_masked()\n",
    "    if lr_decay:\n",
    "        schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "    else:\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "    # containers for metrics to log\n",
    "    train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                     'train_mcc': [], 'train_dice': []}\n",
    "    val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "                   'val_mcc': [], 'val_dice': []}\n",
    "    best_mcc = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training set\n",
    "        train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n",
    "            train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
    "\n",
    "        # training set metrics\n",
    "        train_loss_avg = train_loss / len(train_dataset)\n",
    "        train_metrics['train_loss'].append(train_loss_avg)\n",
    "        train_metrics['train_acc'].append(train_accuracy.get()[1])\n",
    "        train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "        train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "        train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "\n",
    "        # validation set\n",
    "        val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model(\n",
    "            val_dataloader, model, tanimoto_dual, epoch, args)\n",
    "\n",
    "        # validation set metrics\n",
    "        val_loss_avg = val_loss / len(val_dataset)\n",
    "        val_metrics['val_loss'].append(val_loss_avg)\n",
    "        val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "        val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "        val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "        val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "        print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "\n",
    "        # save model based on best MCC metric\n",
    "        if val_mcc.get()[1] > best_mcc:\n",
    "            model.save_parameters(save_model_name)\n",
    "            best_mcc = val_mcc.get()[1]\n",
    "\n",
    "        # save metrics\n",
    "        metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "        metrics.to_csv(os.path.join(save_path, 'metrics.csv'), index=False)\n",
    "\n",
    "        # visdom\n",
    "        vis.line(Y=np.stack([train_metrics['train_loss'], val_metrics['val_loss']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"Loss\", \n",
    "                 opts=dict(legend=['train loss', 'val loss'], markers=False, title=\"Losses\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "                )\n",
    "        vis.line(Y=np.stack([train_metrics['train_mcc'], val_metrics['val_mcc']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"MCC\", \n",
    "                 opts=dict(legend=['train MCC', 'val MCC'], markers=False, title=\"MCC\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"MCC\")\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ============================ #\n",
    "# # user-specified hyperparameters\n",
    "# # ============================ #\n",
    "# country = 'partial-france'\n",
    "# epochs = 100\n",
    "# lr = 0.001\n",
    "# lr_decay = None\n",
    "# n_filters = 16\n",
    "# batch_size = 8\n",
    "# n_classes = 1\n",
    "# model_type = 'resunet-d6'\n",
    "# month = 'aprJulOctSeparate'\n",
    "# codes_to_keep = [1]\n",
    "# ctx_name = 'gpu'\n",
    "# gpu_id = 0\n",
    "# boundary_kernel_size = (2,2)\n",
    "\n",
    "# # trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "# #     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "# trained_model = None\n",
    "# splits_path = '../data/splits/sherrie10k_planetImagery_splits_20x20_4x-downsampled.csv'\n",
    "# splits_df = pd.read_csv(splits_path, dtype=str)\n",
    "\n",
    "# # get all img and labels\n",
    "# all_img_names = []\n",
    "# all_label_names = []\n",
    "# img_dir = '../data/planet/france/sherrie10k/monthly_mosaics_renamed_clipped_merged/1250px/4x_downsample/'\n",
    "# label_dir = '../data/planet/france/sherrie10k/extent_labels/1250px_1field/4x_downsample/'\n",
    "\n",
    "# label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "# for month in ['2019_04', '2019_07', '2019_10']:\n",
    "#     for label_name in label_folder_imgs:\n",
    "#         img_name = label_name.split('.')[0] + '_' + month + '.tif'\n",
    "#         img_path = os.path.join(img_dir, month, img_name)\n",
    "#         all_img_names.append(img_path)\n",
    "#         label_path = os.path.join(label_dir, label_name)\n",
    "#         all_label_names.append(label_path)\n",
    "\n",
    "# # split imgs and labels into train/val/test\n",
    "# all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "# all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "#     lambda x: x[-1]).str.split('.').apply(\n",
    "#     lambda x: x[0]).str.split('_').apply(\n",
    "#     lambda x: x[0])\n",
    "# all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "# train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "# val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "# test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "# all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "# all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "#     lambda x: x[-1]).str.split('.').apply(\n",
    "#     lambda x: x[0])\n",
    "# all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "# train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "# val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "# test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# # ============================ #\n",
    "\n",
    "# run_africa(country, train_names, val_names, test_names,\n",
    "#            train_names_label, val_names_label, test_names_label,\n",
    "#            trained_model=trained_model,\n",
    "#            epochs=epochs, lr=lr, lr_decay=lr_decay, n_filters=n_filters, batch_size=batch_size,\n",
    "#            n_classes=n_classes, model_type=model_type, month=month,\n",
    "#            codes_to_keep=codes_to_keep, \n",
    "#            ctx_name=ctx_name,\n",
    "#            gpu_id=gpu_id, \n",
    "#            folder_suffix='_4x-downsampled',\n",
    "#            boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 16\n",
      "depth:= 1, nfilters: 32\n",
      "depth:= 2, nfilters: 64\n",
      "depth:= 3, nfilters: 128\n",
      "depth:= 4, nfilters: 256\n",
      "depth:= 5, nfilters: 512\n",
      "depth:= 6, nfilters: 256\n",
      "depth:= 7, nfilters: 128\n",
      "depth:= 8, nfilters: 64\n",
      "depth:= 9, nfilters: 32\n",
      "depth:= 10, nfilters: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:visdom:Setting up a new session...\n",
      "Training epoch 2: 100%|██████████| 2535/2535 [34:39<00:00,  1.22it/s] \n",
      "Training epoch 5: 100%|██████████| 2535/2535 [34:20<00:00,  1.23it/s]\n",
      "Validation epoch 13: 100%|██████████| 580/580 [04:12<00:00,  2.30it/s]\n",
      "Training epoch 14:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "    Train loss 0.162, accuracy 0.834, F1-score 0.889, MCC: 0.547, Dice: 0.889\n",
      "    Val loss 0.157, accuracy 0.845, F1-score 0.899, MCC: 0.558, Dice: 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14: 100%|██████████| 2535/2535 [24:15<00:00,  1.74it/s]\n",
      "Validation epoch 14:  37%|███▋      | 215/580 [01:01<01:28,  4.11it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 16: 100%|██████████| 2535/2535 [20:17<00:00,  2.08it/s]\n",
      "Validation epoch 16: 100%|██████████| 580/580 [03:00<00:00,  3.22it/s]\n",
      "Training epoch 17:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "    Train loss 0.161, accuracy 0.836, F1-score 0.890, MCC: 0.550, Dice: 0.890\n",
      "    Val loss 0.155, accuracy 0.840, F1-score 0.894, MCC: 0.552, Dice: 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 17: 100%|██████████| 2535/2535 [20:04<00:00,  2.10it/s]\n",
      "Validation epoch 17: 100%|██████████| 580/580 [02:52<00:00,  3.37it/s]\n",
      "Training epoch 18:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "    Train loss 0.161, accuracy 0.836, F1-score 0.890, MCC: 0.551, Dice: 0.890\n",
      "    Val loss 0.158, accuracy 0.841, F1-score 0.896, MCC: 0.549, Dice: 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18: 100%|██████████| 2535/2535 [20:09<00:00,  2.10it/s]\n",
      "Validation epoch 18: 100%|██████████| 580/580 [02:57<00:00,  3.27it/s]\n",
      "Training epoch 19:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "    Train loss 0.161, accuracy 0.836, F1-score 0.891, MCC: 0.551, Dice: 0.891\n",
      "    Val loss 0.152, accuracy 0.840, F1-score 0.892, MCC: 0.566, Dice: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 19: 100%|██████████| 2535/2535 [20:07<00:00,  2.10it/s]\n",
      "Validation epoch 19: 100%|██████████| 580/580 [02:50<00:00,  3.41it/s]\n",
      "Training epoch 20:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "    Train loss 0.161, accuracy 0.837, F1-score 0.891, MCC: 0.552, Dice: 0.891\n",
      "    Val loss 0.158, accuracy 0.844, F1-score 0.898, MCC: 0.559, Dice: 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 20: 100%|██████████| 2535/2535 [20:04<00:00,  2.10it/s]\n",
      "Validation epoch 20: 100%|██████████| 580/580 [02:46<00:00,  3.47it/s]\n",
      "Training epoch 21:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "    Train loss 0.160, accuracy 0.837, F1-score 0.891, MCC: 0.553, Dice: 0.891\n",
      "    Val loss 0.151, accuracy 0.846, F1-score 0.897, MCC: 0.573, Dice: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 21: 100%|██████████| 2535/2535 [20:09<00:00,  2.10it/s]\n",
      "Validation epoch 21: 100%|██████████| 580/580 [02:54<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "    Train loss 0.160, accuracy 0.837, F1-score 0.891, MCC: 0.553, Dice: 0.891\n",
      "    Val loss 0.148, accuracy 0.842, F1-score 0.892, MCC: 0.583, Dice: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 22: 100%|██████████| 2535/2535 [20:07<00:00,  2.10it/s]\n",
      "Validation epoch 22: 100%|██████████| 580/580 [02:54<00:00,  3.32it/s]\n",
      "Training epoch 23:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:\n",
      "    Train loss 0.160, accuracy 0.837, F1-score 0.891, MCC: 0.553, Dice: 0.891\n",
      "    Val loss 0.150, accuracy 0.841, F1-score 0.893, MCC: 0.574, Dice: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 23: 100%|██████████| 2535/2535 [20:03<00:00,  2.11it/s]\n",
      "Validation epoch 23: 100%|██████████| 580/580 [02:48<00:00,  3.44it/s]\n",
      "Training epoch 24:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:\n",
      "    Train loss 0.160, accuracy 0.838, F1-score 0.892, MCC: 0.555, Dice: 0.892\n",
      "    Val loss 0.150, accuracy 0.839, F1-score 0.889, MCC: 0.581, Dice: 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 24: 100%|██████████| 2535/2535 [20:01<00:00,  2.11it/s]\n",
      "Validation epoch 24: 100%|██████████| 580/580 [02:59<00:00,  3.23it/s]\n",
      "Training epoch 25:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:\n",
      "    Train loss 0.160, accuracy 0.838, F1-score 0.892, MCC: 0.555, Dice: 0.892\n",
      "    Val loss 0.148, accuracy 0.846, F1-score 0.897, MCC: 0.579, Dice: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 25: 100%|██████████| 2535/2535 [20:05<00:00,  2.10it/s]\n",
      "Validation epoch 25: 100%|██████████| 580/580 [02:44<00:00,  3.53it/s]\n",
      "Training epoch 26:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:\n",
      "    Train loss 0.159, accuracy 0.838, F1-score 0.892, MCC: 0.556, Dice: 0.892\n",
      "    Val loss 0.154, accuracy 0.839, F1-score 0.892, MCC: 0.563, Dice: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 26: 100%|██████████| 2535/2535 [20:05<00:00,  2.10it/s]\n",
      "Validation epoch 26: 100%|██████████| 580/580 [02:53<00:00,  3.34it/s]\n",
      "Training epoch 27:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26:\n",
      "    Train loss 0.159, accuracy 0.838, F1-score 0.892, MCC: 0.556, Dice: 0.892\n",
      "    Val loss 0.152, accuracy 0.840, F1-score 0.893, MCC: 0.567, Dice: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 27: 100%|██████████| 2535/2535 [20:03<00:00,  2.11it/s]\n",
      "Validation epoch 27: 100%|██████████| 580/580 [02:58<00:00,  3.25it/s]\n",
      "Training epoch 28:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:\n",
      "    Train loss 0.159, accuracy 0.838, F1-score 0.892, MCC: 0.557, Dice: 0.892\n",
      "    Val loss 0.149, accuracy 0.846, F1-score 0.898, MCC: 0.576, Dice: 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 28: 100%|██████████| 2535/2535 [20:03<00:00,  2.11it/s]\n",
      "Validation epoch 28: 100%|██████████| 580/580 [03:02<00:00,  3.17it/s]\n",
      "Training epoch 29:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28:\n",
      "    Train loss 0.159, accuracy 0.838, F1-score 0.892, MCC: 0.557, Dice: 0.892\n",
      "    Val loss 0.151, accuracy 0.845, F1-score 0.896, MCC: 0.575, Dice: 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 29: 100%|██████████| 2535/2535 [20:14<00:00,  2.09it/s]\n",
      "Validation epoch 29: 100%|██████████| 580/580 [03:03<00:00,  3.16it/s]\n",
      "Training epoch 30:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29:\n",
      "    Train loss 0.159, accuracy 0.838, F1-score 0.892, MCC: 0.557, Dice: 0.892\n",
      "    Val loss 0.150, accuracy 0.848, F1-score 0.899, MCC: 0.577, Dice: 0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 30: 100%|██████████| 2535/2535 [21:38<00:00,  1.95it/s]\n",
      "Validation epoch 30: 100%|██████████| 580/580 [03:58<00:00,  2.43it/s]\n",
      "Training epoch 31:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.892, MCC: 0.557, Dice: 0.892\n",
      "    Val loss 0.149, accuracy 0.846, F1-score 0.897, MCC: 0.581, Dice: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 31: 100%|██████████| 2535/2535 [26:28<00:00,  1.60it/s]\n",
      "Validation epoch 31: 100%|██████████| 580/580 [03:59<00:00,  2.42it/s]\n",
      "Training epoch 32:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.892, MCC: 0.558, Dice: 0.892\n",
      "    Val loss 0.150, accuracy 0.849, F1-score 0.900, MCC: 0.579, Dice: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 32: 100%|██████████| 2535/2535 [26:35<00:00,  1.59it/s]\n",
      "Validation epoch 32: 100%|██████████| 580/580 [03:59<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.892, MCC: 0.557, Dice: 0.892\n",
      "    Val loss 0.152, accuracy 0.837, F1-score 0.887, MCC: 0.584, Dice: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 33: 100%|██████████| 2535/2535 [26:31<00:00,  1.59it/s]\n",
      "Validation epoch 33: 100%|██████████| 580/580 [03:55<00:00,  2.46it/s]\n",
      "Training epoch 34:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.892, MCC: 0.558, Dice: 0.892\n",
      "    Val loss 0.148, accuracy 0.842, F1-score 0.892, MCC: 0.583, Dice: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 34: 100%|██████████| 2535/2535 [26:34<00:00,  1.59it/s]\n",
      "Validation epoch 34: 100%|██████████| 580/580 [04:01<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.893, MCC: 0.559, Dice: 0.893\n",
      "    Val loss 0.146, accuracy 0.846, F1-score 0.895, MCC: 0.591, Dice: 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 35: 100%|██████████| 2535/2535 [26:40<00:00,  1.58it/s]\n",
      "Validation epoch 35: 100%|██████████| 580/580 [03:58<00:00,  2.43it/s]\n",
      "Training epoch 36:   0%|          | 0/2535 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.893, MCC: 0.558, Dice: 0.893\n",
      "    Val loss 0.149, accuracy 0.849, F1-score 0.900, MCC: 0.578, Dice: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 36: 100%|██████████| 2535/2535 [26:32<00:00,  1.59it/s]\n",
      "Validation epoch 36: 100%|██████████| 580/580 [04:17<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:\n",
      "    Train loss 0.159, accuracy 0.839, F1-score 0.892, MCC: 0.558, Dice: 0.892\n",
      "    Val loss 0.147, accuracy 0.844, F1-score 0.894, MCC: 0.585, Dice: 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/partial-france/resunet-d6_2019_10_nfilter-16_bs-8_lr-0.001_4x-downsampled_allfields_n6759/metrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-64c32ef7b538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m            \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m            \u001b[0mfolder_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_4x-downsampled_allfields_n6759'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m            boundary_kernel_size=boundary_kernel_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-233931e44f85>\u001b[0m in \u001b[0;36mrun_africa\u001b[0;34m(country, train_names, val_names, test_names, train_names_label, val_names_label, test_names_label, trained_model, epochs, lr, lr_decay, n_filters, batch_size, n_classes, model_type, month, codes_to_keep, folder_suffix, boundary_kernel_size, ctx_name, gpu_id)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# save metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# visdom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         )\n\u001b[0;32m-> 3170\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             )\n\u001b[1;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/partial-france/resunet-d6_2019_10_nfilter-16_bs-8_lr-0.001_4x-downsampled_allfields_n6759/metrics.csv'"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'partial-france'\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "lr_decay = None\n",
    "n_filters = 16\n",
    "batch_size = 8\n",
    "n_classes = 1\n",
    "model_type = 'resunet-d6'\n",
    "month = 'aprJulOctSeparate'\n",
    "# codes_to_keep = [1]\n",
    "codes_to_keep = list(range(1,10)) + [11,14,15,16,17,18,19,21,24,25,26,28]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 2\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "# trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "#     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "trained_model = None\n",
    "splits_path = '../data/splits/sherrie10k_planetImagery_splits_20x20_4x-downsampled.csv'\n",
    "splits_df = pd.read_csv(splits_path)\n",
    "splits_df['image_id'] = splits_df['image_id'].astype(str).str.zfill(5)\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dir = '../data/planet/france/sherrie10k/monthly_mosaics_renamed_clipped_merged/1250px/4x_downsample/'\n",
    "label_dir = '../data/planet/france/sherrie10k/extent_labels/1250px/4x_downsample/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for month in ['2019_04', '2019_07', '2019_10']:\n",
    "    for label_name in label_folder_imgs:\n",
    "        img_name = label_name.split('.')[0] + '_' + month + '.tif'\n",
    "        img_path = os.path.join(img_dir, month, img_name)\n",
    "        all_img_names.append(img_path)\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        all_label_names.append(label_path)\n",
    "\n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[0])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, n_filters=n_filters, batch_size=batch_size,\n",
    "           n_classes=n_classes, model_type=model_type, month=month,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix='_4x-downsampled_allfields_n6759',\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "mask_j = mx.nd.array([[[1,0,0],\n",
    "                       [0,1,0]],\n",
    "                      [[0,0,1],\n",
    "                       [0,0,0]]])\n",
    "mask_j = mask_j.reshape((mask_j.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. 0. 1. 0.]\n",
       " [0. 0. 1. 0. 0. 0.]]\n",
       "<NDArray 2x6 @cpu(0)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmask_idx = mx.np.nonzero(mask_j.as_np_ndarray())\n",
    "nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.nd.gather_nd(mask_j, nonmask_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.5097876]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanimoto = Tanimoto_with_dual_masked()\n",
    "test_pred = mx.nd.array([[[[0.1,0.1,0.8,0.9]]]])\n",
    "test_label = mx.nd.array([[[[0,0,1,0]]]])\n",
    "test_mask = mx.nd.array([[[[0,1,1,1]]]])\n",
    "tanimoto(test_pred, test_label, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 0.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[0. 0. 0.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 0. 0. 0. 0.]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]):\n",
    "    print(row)\n",
    "    \n",
    "mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.npx.set_np()\n",
    "mx.np.nonzero(mx.nd.array([[0,1,0],\n",
    "                           [0,0,0]]).as_np_ndarray())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = mx.nd.array([[ 0.1, 0.9, 0.5],\n",
    "                          [ 0.2, 0.3, 0.7]])\n",
    "test_array2 = mx.nd.array([[0, 2, 0],\n",
    "                           [0, 1, 0]])\n",
    "nonmask_idx = mx.np.nonzero(test_array2.as_np_ndarray())\n",
    "test_result = mx.nd.gather_nd(test_array, mx.nd.array(nonmask_idx))\n",
    "test_result2 = mx.nd.gather_nd(test_array2, mx.nd.array(nonmask_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.9 0.3]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 0.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result2 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet1.6.0)",
   "language": "python",
   "name": "conda_mxnet1.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
