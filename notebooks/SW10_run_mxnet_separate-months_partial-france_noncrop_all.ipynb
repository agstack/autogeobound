{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../resuneta/src')\n",
    "sys.path.append('../../resuneta/nn/loss')\n",
    "sys.path.append('../../resuneta/models')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../MXNet-ResUNeta/')\n",
    "\n",
    "from bound_dist import get_distance, get_boundary\n",
    "from loss import Tanimoto_with_dual_masked\n",
    "from resunet_d6_causal_mtskcolor_ddist import *\n",
    "from resunet_d7_causal_mtskcolor_ddist import *\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(x, y):\n",
    "    if type(x).__module__ == 'numpy':\n",
    "        intersection = np.logical_and(x, y)\n",
    "        return 2. * np.sum(intersection) / (np.sum(x) + np.sum(y))\n",
    "    else:\n",
    "        intersection = mx.ndarray.op.broadcast_logical_and(x, y)\n",
    "        return 2. * mx.nd.sum(intersection) / (mx.nd.sum(x) + mx.nd.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visdom_visualize_batch(vis, img, extent, boundary, distance,\n",
    "                           extent_pred, boundary_pred, distance_pred,\n",
    "                           hsv, hsv_pred, mask, title=\"Train images\"):\n",
    "\n",
    "    img, extent, boundary, distance = img.asnumpy(), extent.asnumpy(), boundary.asnumpy(), distance.asnumpy()\n",
    "    extent_pred, boundary_pred = extent_pred.asnumpy(), boundary_pred.asnumpy()\n",
    "    distance_pred, hsv, hsv_pred = distance_pred.asnumpy(), hsv.asnumpy(), hsv_pred.asnumpy()\n",
    "    mask = mask.asnumpy()\n",
    "\n",
    "    # put everything in one window\n",
    "    batch_size, nchannels, nrows, ncols = img.shape\n",
    "    padding = 10\n",
    "    items = [img, hsv, hsv_pred, extent, extent_pred, \n",
    "             boundary, boundary_pred, distance, distance_pred,\n",
    "             mask]\n",
    "    result = np.zeros((3, len(items)*nrows + (len(items)-1)*padding, batch_size*ncols + (batch_size-1)*padding))\n",
    "\n",
    "    for j, item in enumerate(items):\n",
    "\n",
    "        if item.shape[1] == 1:\n",
    "            item = np.tile(item, (1,3,1,1)) * 255.\n",
    "\n",
    "        if j == 1 or j == 2: # convert HSV to RGB\n",
    "            item = np.moveaxis(item, 1, -1) * 255.\n",
    "            for i in range(batch_size):\n",
    "                item[i] = cv2.cvtColor(item[i].astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            item = np.moveaxis(item, -1, 1)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            result[:, j*(nrows+padding):(j+1)*nrows+j*padding, i*(ncols+padding):(i+1)*ncols+i*padding] = item[i]\n",
    "    vis.images(result, nrow=1, win=title, opts={'title': title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, model, tanimoto_dual, trainer, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # training set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(train_dataloader, desc='Training epoch {}'.format(epoch))):\n",
    "        \n",
    "        with autograd.record():\n",
    "\n",
    "            img = img.as_in_context(ctx)\n",
    "            extent = extent.as_in_context(ctx)\n",
    "            boundary = boundary.as_in_context(ctx)\n",
    "            distance = distance.as_in_context(ctx)\n",
    "            hsv = hsv.as_in_context(ctx)\n",
    "            mask = mask.as_in_context(ctx)\n",
    "            nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "            \n",
    "            logits, bound, dist, convc = model(img)\n",
    "            \n",
    "            # multi-task loss\n",
    "            # TODO: wrap this in a custom loss function / class\n",
    "            loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "            loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "            loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "            loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask)) # don't mask hsv\n",
    "\n",
    "            loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "            \n",
    "        loss.backward()\n",
    "        trainer.step(args['batch_size'])\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "#         print(\"logits.shape\", logits.shape)\n",
    "#         print(\"extent.shape\", extent.shape)\n",
    "#         print(\"mask.shape\", mask.shape)\n",
    "            \n",
    "        # mask out unlabeled pixels            \n",
    "#         print(\"made it here -3\")\n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "#         print(\"logits_reshaped.shape\", logits_reshaped.shape)\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "#         print(\"nonmask_idx\", nonmask_idx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "#         print(\"logits_masked\", logits_masked)\n",
    "#         print(\"logits_masked.shape\", logits_masked.shape)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "#         print(\"extent_masked\", extent_masked)\n",
    "#         print(\"extent_masked.shape\", extent_masked.shape)\n",
    "\n",
    "#         print(\"made it here -1\")\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5) # logits_masked[[0],:,:]\n",
    "#         print(\"extent_predicted_classes\", extent_predicted_classes)\n",
    "#         print(\"extent_predicted_classes.shape\", extent_predicted_classes.shape)\n",
    "        \n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 1\")\n",
    "        \n",
    "        # f1 score\n",
    "#         prediction = logits[:,0,:,:].reshape(-1)\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 2\")\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 3\")\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 4\")\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dataloader, model, tanimoto_dual, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # validation set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(val_dataloader, desc='Validation epoch {}'.format(epoch))):\n",
    "\n",
    "        img = img.as_in_context(ctx)\n",
    "        extent = extent.as_in_context(ctx)\n",
    "        boundary = boundary.as_in_context(ctx)\n",
    "        distance = distance.as_in_context(ctx)\n",
    "        hsv = hsv.as_in_context(ctx)\n",
    "        mask = mask.as_in_context(ctx)\n",
    "        nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "\n",
    "        logits, bound, dist, convc = model(img)\n",
    "        \n",
    "        # multi-task loss\n",
    "        # TODO: wrap this in a custom loss function / class\n",
    "        loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "        loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "        loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "        loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask))\n",
    "\n",
    "        loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        # mask out unlabeled pixels            \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask, title=\"Val images\")\n",
    "        \n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_africa(country, train_names, val_names, test_names, \n",
    "               train_names_label, val_names_label, test_names_label,\n",
    "               trained_model=None,\n",
    "               epochs=100, lr=0.001, lr_decay=None, n_filters=16, batch_size=8,\n",
    "               n_classes=1, model_type='resunet-d6', month='janFebMar',\n",
    "               codes_to_keep=[1],\n",
    "               noncrop_codes=[2],\n",
    "               folder_suffix='',\n",
    "               boundary_kernel_size=3,\n",
    "               ctx_name='cpu',\n",
    "               gpu_id=0):\n",
    "    \n",
    "    # Set MXNet ctx\n",
    "    if ctx_name == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    elif ctx_name == 'gpu':\n",
    "        ctx = mx.gpu(gpu_id)\n",
    "    \n",
    "    # Set up names of directories and paths for saving\n",
    "    if trained_model is None:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if lr_decay:\n",
    "            folder_name = folder_name + '_lrdecay-'+str(lr_decay)\n",
    "            \n",
    "        # define model\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.initialize()\n",
    "        model.hybridize()\n",
    "        model.collect_params().reset_ctx(ctx)\n",
    "        \n",
    "    else:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix+'_finetuned'\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.load_parameters(trained_model, ctx=ctx)\n",
    "        \n",
    "    save_path = os.path.join('../experiments/', country, folder_name)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_model_name = os.path.join(save_path, \"model.params\")\n",
    "    \n",
    "    # Visdom\n",
    "    env_name = country + '_' + folder_name\n",
    "    vis = visdom.Visdom(port=8097, env=env_name)\n",
    "    \n",
    "    # Arguments\n",
    "    args = {}\n",
    "    args['batch_size'] = batch_size\n",
    "    args['ctx_name'] = ctx_name\n",
    "    args['gpu'] = gpu_id\n",
    "    args['visdom'] = vis\n",
    "    args['visdom_every'] = 20\n",
    "\n",
    "    # Define train/val/test splits\n",
    "    train_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='train', \n",
    "        image_names=train_names, \n",
    "        label_names=train_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        noncrop_classes=noncrop_codes,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    val_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='val', \n",
    "        image_names=val_names, \n",
    "        label_names=val_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        noncrop_classes=noncrop_codes,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    test_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='test', \n",
    "        image_names=test_names, \n",
    "        label_names=test_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        noncrop_classes=noncrop_codes,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = gluon.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # define loss function\n",
    "    tanimoto_dual = Tanimoto_with_dual_masked()\n",
    "    if lr_decay:\n",
    "        schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "    else:\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "    # containers for metrics to log\n",
    "    train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                     'train_mcc': [], 'train_dice': []}\n",
    "    val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "                   'val_mcc': [], 'val_dice': []}\n",
    "    best_mcc = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training set\n",
    "        train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n",
    "            train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
    "\n",
    "        # training set metrics\n",
    "        train_loss_avg = train_loss / len(train_dataset)\n",
    "        train_metrics['train_loss'].append(train_loss_avg)\n",
    "        train_metrics['train_acc'].append(train_accuracy.get()[1])\n",
    "        train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "        train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "        train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "\n",
    "        # validation set\n",
    "        val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model(\n",
    "            val_dataloader, model, tanimoto_dual, epoch, args)\n",
    "\n",
    "        # validation set metrics\n",
    "        val_loss_avg = val_loss / len(val_dataset)\n",
    "        val_metrics['val_loss'].append(val_loss_avg)\n",
    "        val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "        val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "        val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "        val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "        print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "\n",
    "        # save model based on best MCC metric\n",
    "        if val_mcc.get()[1] > best_mcc:\n",
    "            model.save_parameters(save_model_name)\n",
    "            best_mcc = val_mcc.get()[1]\n",
    "\n",
    "        # save metrics\n",
    "        metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "        metrics.to_csv(os.path.join(save_path, 'metrics.csv'), index=False)\n",
    "\n",
    "        # visdom\n",
    "        vis.line(Y=np.stack([train_metrics['train_loss'], val_metrics['val_loss']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"Loss\", \n",
    "                 opts=dict(legend=['train loss', 'val loss'], markers=False, title=\"Losses\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "                )\n",
    "        vis.line(Y=np.stack([train_metrics['train_mcc'], val_metrics['val_mcc']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"MCC\", \n",
    "                 opts=dict(legend=['train MCC', 'val MCC'], markers=False, title=\"MCC\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"MCC\")\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 16\n",
      "depth:= 1, nfilters: 32\n",
      "depth:= 2, nfilters: 64\n",
      "depth:= 3, nfilters: 128\n",
      "depth:= 4, nfilters: 256\n",
      "depth:= 5, nfilters: 512\n",
      "depth:= 6, nfilters: 256\n",
      "depth:= 7, nfilters: 128\n",
      "depth:= 8, nfilters: 64\n",
      "depth:= 9, nfilters: 32\n",
      "depth:= 10, nfilters: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Training epoch 1: 100%|██████████| 2532/2532 [24:53<00:00,  1.70it/s]\n",
      "Validation epoch 1: 100%|██████████| 580/580 [03:31<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Train loss 0.330, accuracy 0.555, F1-score 0.488, MCC: 0.233, Dice: 0.488\n",
      "    Val loss 0.280, accuracy 0.716, F1-score 0.695, MCC: 0.488, Dice: 0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 2532/2532 [24:59<00:00,  1.69it/s]\n",
      "Validation epoch 2: 100%|██████████| 580/580 [03:29<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "    Train loss 0.312, accuracy 0.581, F1-score 0.562, MCC: 0.301, Dice: 0.562\n",
      "    Val loss 0.272, accuracy 0.698, F1-score 0.687, MCC: 0.494, Dice: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 2532/2532 [24:58<00:00,  1.69it/s]\n",
      "Validation epoch 3: 100%|██████████| 580/580 [03:28<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "    Train loss 0.305, accuracy 0.589, F1-score 0.577, MCC: 0.322, Dice: 0.577\n",
      "    Val loss 0.262, accuracy 0.687, F1-score 0.669, MCC: 0.502, Dice: 0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 2532/2532 [24:54<00:00,  1.69it/s]\n",
      "Validation epoch 4: 100%|██████████| 580/580 [03:29<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "    Train loss 0.301, accuracy 0.580, F1-score 0.577, MCC: 0.325, Dice: 0.577\n",
      "    Val loss 0.254, accuracy 0.726, F1-score 0.698, MCC: 0.520, Dice: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 2532/2532 [24:59<00:00,  1.69it/s]\n",
      "Validation epoch 5: 100%|██████████| 580/580 [03:28<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "    Train loss 0.298, accuracy 0.587, F1-score 0.583, MCC: 0.329, Dice: 0.583\n",
      "    Val loss 0.255, accuracy 0.712, F1-score 0.699, MCC: 0.527, Dice: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 2532/2532 [24:51<00:00,  1.70it/s]\n",
      "Validation epoch 6: 100%|██████████| 580/580 [03:29<00:00,  2.77it/s]\n",
      "Training epoch 7:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "    Train loss 0.298, accuracy 0.593, F1-score 0.579, MCC: 0.327, Dice: 0.579\n",
      "    Val loss 0.250, accuracy 0.712, F1-score 0.706, MCC: 0.526, Dice: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 2532/2532 [25:00<00:00,  1.69it/s]\n",
      "Validation epoch 7: 100%|██████████| 580/580 [03:31<00:00,  2.74it/s]\n",
      "Training epoch 8:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "    Train loss 0.295, accuracy 0.602, F1-score 0.591, MCC: 0.338, Dice: 0.591\n",
      "    Val loss 0.250, accuracy 0.717, F1-score 0.714, MCC: 0.525, Dice: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 2532/2532 [25:12<00:00,  1.67it/s]\n",
      "Validation epoch 8: 100%|██████████| 580/580 [03:30<00:00,  2.75it/s]\n",
      "Training epoch 9:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "    Train loss 0.294, accuracy 0.597, F1-score 0.587, MCC: 0.336, Dice: 0.587\n",
      "    Val loss 0.246, accuracy 0.688, F1-score 0.683, MCC: 0.505, Dice: 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 2532/2532 [25:12<00:00,  1.67it/s]\n",
      "Validation epoch 9: 100%|██████████| 580/580 [03:05<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "    Train loss 0.292, accuracy 0.593, F1-score 0.587, MCC: 0.338, Dice: 0.587\n",
      "    Val loss 0.245, accuracy 0.712, F1-score 0.706, MCC: 0.534, Dice: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10: 100%|██████████| 2532/2532 [22:40<00:00,  1.86it/s]\n",
      "Validation epoch 10: 100%|██████████| 580/580 [03:09<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "    Train loss 0.290, accuracy 0.604, F1-score 0.600, MCC: 0.351, Dice: 0.600\n",
      "    Val loss 0.244, accuracy 0.734, F1-score 0.722, MCC: 0.538, Dice: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11: 100%|██████████| 2532/2532 [22:46<00:00,  1.85it/s]\n",
      "Validation epoch 11: 100%|██████████| 580/580 [03:06<00:00,  3.10it/s]\n",
      "Training epoch 12:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "    Train loss 0.290, accuracy 0.603, F1-score 0.598, MCC: 0.349, Dice: 0.598\n",
      "    Val loss 0.244, accuracy 0.704, F1-score 0.685, MCC: 0.512, Dice: 0.685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12: 100%|██████████| 2532/2532 [23:23<00:00,  1.80it/s]\n",
      "Validation epoch 12: 100%|██████████| 580/580 [03:06<00:00,  3.11it/s]\n",
      "Training epoch 13:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "    Train loss 0.290, accuracy 0.606, F1-score 0.596, MCC: 0.346, Dice: 0.596\n",
      "    Val loss 0.248, accuracy 0.725, F1-score 0.709, MCC: 0.517, Dice: 0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13: 100%|██████████| 2532/2532 [23:22<00:00,  1.81it/s]\n",
      "Validation epoch 13: 100%|██████████| 580/580 [03:27<00:00,  2.79it/s]\n",
      "Training epoch 14:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "    Train loss 0.288, accuracy 0.602, F1-score 0.598, MCC: 0.350, Dice: 0.598\n",
      "    Val loss 0.239, accuracy 0.704, F1-score 0.705, MCC: 0.531, Dice: 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14: 100%|██████████| 2532/2532 [23:33<00:00,  1.79it/s]\n",
      "Validation epoch 14: 100%|██████████| 580/580 [03:37<00:00,  2.67it/s]\n",
      "Training epoch 15:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "    Train loss 0.288, accuracy 0.602, F1-score 0.597, MCC: 0.350, Dice: 0.597\n",
      "    Val loss 0.243, accuracy 0.724, F1-score 0.692, MCC: 0.518, Dice: 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15: 100%|██████████| 2532/2532 [25:02<00:00,  1.69it/s]\n",
      "Validation epoch 15: 100%|██████████| 580/580 [03:22<00:00,  2.87it/s]\n",
      "Training epoch 16:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "    Train loss 0.287, accuracy 0.602, F1-score 0.598, MCC: 0.350, Dice: 0.598\n",
      "    Val loss 0.239, accuracy 0.714, F1-score 0.708, MCC: 0.529, Dice: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 16: 100%|██████████| 2532/2532 [24:19<00:00,  1.74it/s]\n",
      "Validation epoch 16: 100%|██████████| 580/580 [03:21<00:00,  2.88it/s]\n",
      "Training epoch 17:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "    Train loss 0.286, accuracy 0.605, F1-score 0.603, MCC: 0.356, Dice: 0.603\n",
      "    Val loss 0.243, accuracy 0.711, F1-score 0.698, MCC: 0.526, Dice: 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 17: 100%|██████████| 2532/2532 [24:07<00:00,  1.75it/s]\n",
      "Validation epoch 17: 100%|██████████| 580/580 [03:22<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "    Train loss 0.286, accuracy 0.606, F1-score 0.601, MCC: 0.353, Dice: 0.601\n",
      "    Val loss 0.239, accuracy 0.721, F1-score 0.716, MCC: 0.541, Dice: 0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 18: 100%|██████████| 2532/2532 [23:47<00:00,  1.77it/s]\n",
      "Validation epoch 18: 100%|██████████| 580/580 [03:19<00:00,  2.91it/s]\n",
      "Training epoch 19:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "    Train loss 0.285, accuracy 0.603, F1-score 0.603, MCC: 0.356, Dice: 0.603\n",
      "    Val loss 0.246, accuracy 0.700, F1-score 0.689, MCC: 0.503, Dice: 0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 19: 100%|██████████| 2532/2532 [24:26<00:00,  1.73it/s]\n",
      "Validation epoch 19: 100%|██████████| 580/580 [03:30<00:00,  2.75it/s]\n",
      "Training epoch 20:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "    Train loss 0.284, accuracy 0.605, F1-score 0.601, MCC: 0.356, Dice: 0.601\n",
      "    Val loss 0.236, accuracy 0.718, F1-score 0.699, MCC: 0.526, Dice: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 20: 100%|██████████| 2532/2532 [22:30<00:00,  1.87it/s]\n",
      "Validation epoch 20: 100%|██████████| 580/580 [03:01<00:00,  3.19it/s]\n",
      "Training epoch 21:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "    Train loss 0.285, accuracy 0.603, F1-score 0.602, MCC: 0.358, Dice: 0.602\n",
      "    Val loss 0.242, accuracy 0.707, F1-score 0.660, MCC: 0.501, Dice: 0.660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 21: 100%|██████████| 2532/2532 [22:05<00:00,  1.91it/s]\n",
      "Validation epoch 21: 100%|██████████| 580/580 [03:02<00:00,  3.18it/s]\n",
      "Training epoch 22:   0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21:\n",
      "    Train loss 0.284, accuracy 0.602, F1-score 0.602, MCC: 0.355, Dice: 0.602\n",
      "    Val loss 0.243, accuracy 0.688, F1-score 0.662, MCC: 0.487, Dice: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 22:  88%|████████▊ | 2216/2532 [20:10<02:19,  2.26it/s]"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'partial-france'\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "lr_decay = None\n",
    "n_filters = 16\n",
    "batch_size = 8\n",
    "n_classes = 1\n",
    "model_type = 'resunet-d6'\n",
    "month = 'aprJulOctSeparate'\n",
    "codes_to_keep = [1]\n",
    "noncrop_codes = [2]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 1\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "# trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "#     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "trained_model = None\n",
    "splits_path = '../data/splits/sherrie10k_planetImagery_splits_20x20_4x-downsampled.csv'\n",
    "splits_df = pd.read_csv(splits_path)\n",
    "splits_df['image_id'] = splits_df['image_id'].astype(str).str.zfill(5)\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dir = '../data/planet/france/sherrie10k/monthly_mosaics_renamed_clipped_merged/1250px/4x_downsample/'\n",
    "label_dir = '../data/planet/france/sherrie10k/extent_labels/1250px_1field_with-noncrop/4x_downsample/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for month in ['2019_04', '2019_07', '2019_10']:\n",
    "    for label_name in label_folder_imgs:\n",
    "        img_name = label_name.split('.')[0] + '_' + month + '.tif'\n",
    "        img_path = os.path.join(img_dir, month, img_name)\n",
    "        all_img_names.append(img_path)\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        all_label_names.append(label_path)\n",
    "\n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[0])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, n_filters=n_filters, batch_size=batch_size,\n",
    "           n_classes=n_classes, model_type=model_type, month=month,\n",
    "           codes_to_keep=codes_to_keep,\n",
    "           noncrop_codes=noncrop_codes,\n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix='_4x-downsampled_1field_n6759_with-noncrop',\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-generate partial labels for france and africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "mask_j = mx.nd.array([[[1,0,0],\n",
    "                       [0,1,0]],\n",
    "                      [[0,0,1],\n",
    "                       [0,0,0]]])\n",
    "mask_j = mask_j.reshape((mask_j.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. 0. 1. 0.]\n",
       " [0. 0. 1. 0. 0. 0.]]\n",
       "<NDArray 2x6 @cpu(0)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmask_idx = mx.np.nonzero(mask_j.as_np_ndarray())\n",
    "nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.nd.gather_nd(mask_j, nonmask_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.5097876]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanimoto = Tanimoto_with_dual_masked()\n",
    "test_pred = mx.nd.array([[[[0.1,0.1,0.8,0.9]]]])\n",
    "test_label = mx.nd.array([[[[0,0,1,0]]]])\n",
    "test_mask = mx.nd.array([[[[0,1,1,1]]]])\n",
    "tanimoto(test_pred, test_label, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 0.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[0. 0. 0.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 0. 0. 0. 0.]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]):\n",
    "    print(row)\n",
    "    \n",
    "mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.npx.set_np()\n",
    "mx.np.nonzero(mx.nd.array([[0,1,0],\n",
    "                           [0,0,0]]).as_np_ndarray())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = mx.nd.array([[ 0.1, 0.9, 0.5],\n",
    "                          [ 0.2, 0.3, 0.7]])\n",
    "test_array2 = mx.nd.array([[0, 2, 0],\n",
    "                           [0, 1, 0]])\n",
    "nonmask_idx = mx.np.nonzero(test_array2.as_np_ndarray())\n",
    "test_result = mx.nd.gather_nd(test_array, mx.nd.array(nonmask_idx))\n",
    "test_result2 = mx.nd.gather_nd(test_array2, mx.nd.array(nonmask_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.9 0.3]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 0.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result2 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet1.6.0)",
   "language": "python",
   "name": "conda_mxnet1.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
