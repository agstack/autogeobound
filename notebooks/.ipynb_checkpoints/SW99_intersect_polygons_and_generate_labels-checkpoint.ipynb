{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import shapefile\n",
    "import fiona\n",
    "from json import dumps\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "from functools import partial\n",
    "from shapely.ops import transform\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Get all polygons that intersect with each bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(shape_file, readCSV):\n",
    "    \"\"\"Read the coordinate of the bounding boxes and constructs and R-Tree data structure\n",
    "\n",
    "    Args:\n",
    "      shape_file : polygons\n",
    "      readCSV: pandas dataframe containing bounding boxes\n",
    "\n",
    "    Returns:\n",
    "    dict, r-tree: dict of bounding boxes for each image id and r-tree\n",
    "    \"\"\"\n",
    "    shapes = fiona.open(shape_file)\n",
    "    if len(shapes.crs) != 0:\n",
    "        destination = Proj(shapes.crs)\n",
    "    else:\n",
    "        destination = Proj('+init=EPSG:4326')\n",
    "    original = Proj('+init=EPSG:4326')\n",
    "\n",
    "    grid = dict()\n",
    "    keys = ['max_lat', 'max_lon', 'min_lat', 'min_lon']\n",
    "    poly_list = []\n",
    "    \n",
    "    for index, row in readCSV.iterrows():\n",
    "        if index not in grid:\n",
    "            grid[index] = dict()\n",
    "        grid[index]['image_id'] = row['image_id']\n",
    "        grid[index]['max_lat'] = float(row['max_lat'])\n",
    "        grid[index]['max_lon'] = float(row['max_lon'])\n",
    "        grid[index]['min_lat'] = float(row['min_lat'])\n",
    "        grid[index]['min_lon'] = float(row['min_lon'])\n",
    "\n",
    "        grid[index]['poly'] = shapely.geometry.box(\n",
    "            grid[index]['min_lon'], grid[index]['min_lat'], grid[index]['max_lon'], grid[index]['max_lat'])\n",
    "        \n",
    "        # project boxes from WSG 84 to parcel projection\n",
    "        project = partial(pyproj.transform, original, destination)\n",
    "        grid[index]['poly'] = transform(project, grid[index]['poly'])\n",
    "\n",
    "        # populating r-tree\n",
    "        poly_obj = grid[index]['poly']\n",
    "        poly_obj.name = grid[index]['image_id'] # useful for retrival in search phase\n",
    "        poly_list.append(poly_obj)\n",
    "        \n",
    "    tree = STRtree(poly_list) # constructing R-Tree\n",
    "    return grid, tree\n",
    "\n",
    "def listit(t):\n",
    "    # convert to appropriate list type \n",
    "    return list(map(listit, t)) if isinstance(t, (list, tuple)) else t\n",
    "\n",
    "\n",
    "def check_polygon_in_bounds(poly, tree):\n",
    "    \"\"\"\n",
    "    find image corrspinding to the existance of a field in the list of \n",
    "    image bounding boxes\n",
    "\n",
    "    Args:\n",
    "      poly (polygon): field\n",
    "      tree (r-tree): r-tree of images\n",
    "\n",
    "    Returns:\n",
    "      List: List of intersecting images with a field\n",
    "    \"\"\"\n",
    "    results = tree.query(poly)\n",
    "    return results\n",
    "\n",
    "\n",
    "def field_imageId_list(polys, count_parcels):\n",
    "    \"\"\"\n",
    "    extract name of the intersecting polygons\n",
    "\n",
    "    Args:\n",
    "      polys (polygons): intersecting fields\n",
    "      count_parcels (dict): the sanity check summary of # of fields in image ids  \n",
    "    Returns:\n",
    "      list: list of the image ids\n",
    "    \"\"\"\n",
    "    list_image_ids = []\n",
    "    for element in polys:\n",
    "        list_image_ids.append(element.name)\n",
    "        count_parcels[element.name] += 1\n",
    "    return list_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_shp_to_json(shape_file, grid, tree, output_json='../data/planet/france/sherrie10k/test_json'):\n",
    "    \"\"\"\n",
    "    find intersecting polygons in the list of available images and save the GeoJSON\n",
    "\n",
    "    Args:\n",
    "      shape_file (polygons): fields\n",
    "      grid (dict): image bounding boxes \n",
    "      tree (r-tre): r-tree of images\n",
    "      output_json (str): output path of json file\n",
    "    \"\"\"\n",
    "    # coordinate transformation\n",
    "    reader = shapefile.Reader(shape_file)\n",
    "    shapes = fiona.open(shape_file)\n",
    "    if len(shapes.crs) != 0:\n",
    "        original = Proj(shapes.crs)\n",
    "    else:\n",
    "        original = Proj('+init=EPSG:4326')\n",
    "\n",
    "    # list of properties of features\n",
    "    fields = reader.fields[1:]\n",
    "    field_names = [field[0] for field in fields]\n",
    "    field_names.append('image_id')\n",
    "\n",
    "    buffer = []\n",
    "    \n",
    "    # sanity check counters\n",
    "    count_parcels = defaultdict(int)\n",
    "    counter_method1 = 0\n",
    "    counter_method2 = 0\n",
    "    num_matched = 0\n",
    "    failed_projection = 0\n",
    "  \n",
    "    # loop through the polygon fields\n",
    "    for sr in tqdm(reader.iterShapeRecords(), total=9517878):\n",
    "        \n",
    "        geom = sr.shape.__geo_interface__\n",
    "        shp_geom = shape(geom)\n",
    "        intersect = check_polygon_in_bounds(shp_geom, tree)\n",
    "        if len(intersect) != 0:\n",
    "            num_matched += len(intersect)\n",
    "      \n",
    "            id_list = field_imageId_list(intersect, count_parcels)\n",
    "            sr.record.append(id_list)\n",
    "            atr = dict(zip(field_names, sr.record))\n",
    "            \n",
    "            geom['coordinates'] = listit(geom['coordinates'])\n",
    "            try: # protection at polygons that fail at projection\n",
    "                if len(geom['coordinates']) == 1: # for single polygon\n",
    "                    counter_method1 += 1\n",
    "                    x, y = zip(*geom['coordinates'][0])\n",
    "                    lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                    geom['coordinates'] = [listit(list(zip(lat, long)))]\n",
    "                else: # for multipolygons\n",
    "                    counter_method2 += 1\n",
    "                    for index_coord in range(0, len(geom['coordinates'])):\n",
    "                        for counter in range(0,len(geom['coordinates'][index_coord])):\n",
    "                            x, y = geom['coordinates'][index_coord][counter]\n",
    "                            lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                            geom['coordinates'][index_coord][counter] = [lat, long] #(long, lat)\n",
    "            except:\n",
    "                failed_projection =+ 1\n",
    "            buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr))\n",
    "      \n",
    "      \n",
    "    # write the GeoJSON file\n",
    "    output_json_interval = output_json + str(num_matched) + '.json'\n",
    "    print(\"saving json\")\n",
    "    with open(output_json_interval, 'w') as geojson:\n",
    "        geojson.write(dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2) + \"\\n\")\n",
    "        geojson.close()\n",
    "        print('saved', output_json_interval)\n",
    "    \n",
    "    print('method one count:', counter_method1)\n",
    "    print('method two count:', counter_method2)\n",
    "    print(\"Number matched:\", num_matched)\n",
    "    print('failed count', failed_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/planet/france/sherrie10k/'\n",
    "csv_file = os.path.join(base_dir, 'bbox10k_2500px.csv')\n",
    "\n",
    "shape_file = '../data/parcels/france/RPG_2-0__SHP_LAMB93_FR-2018_2018-01-15/RPG/1_DONNEES_LIVRAISON_2018/RPG_2-0_SHP_LAMB93_FR-2018/PARCELLES_GRAPHIQUES.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "for start in np.arange(0, 1500, 250):\n",
    "    end = start + 250\n",
    "    images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "    images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "    grid, tree = read_csv(shape_file, images_df)\n",
    "    \n",
    "    dump_shp_to_json(shape_file, grid, tree, \n",
    "                     '../data/planet/france/sherrie10k/json_polys/bbox10k_2500px_{}_'.format(int(start/250)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Generate raster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Expects a csv file with image id, maxlat, maxlon, minlat, minlon of each satellite image\n",
    "Expects json files as a folder with the polygons parsed from the shapefile\n",
    "Expects images for overlaying masks on images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_overlays(grid, orig_images_dir, prefix='', suffix='', n=100):\n",
    "    \"\"\"\n",
    "    Initiate empty masks and original images into the output folder based on the\n",
    "    \n",
    "    Args:\n",
    "        grid (list): information from csv, so the empty masks to be overwritten later\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in grid.keys():\n",
    "        image_id = grid[index]['image_id']\n",
    "        if count > n:\n",
    "            break\n",
    "            \n",
    "        im_name = os.path.join(orig_images_dir, str(prefix) + str(image_id) + str(suffix) + '.tif')\n",
    "        orig_image = cv2.imread(im_name)\n",
    "        overlay_path = os.path.join(base_dir, overlay_folder, str(image_id) + '.jpeg')\n",
    "        cv2.imwrite(overlay_path, orig_image)\n",
    "        count += 1\n",
    "\n",
    "def create_dict_parcels(shp_dict):\n",
    "    \"\"\"\n",
    "    Put the found fields from the parsed json into dict of image ids\n",
    "    Args:\n",
    "        shp_dict (dict): json dict containing the polygon fields\n",
    "\n",
    "    Returns:\n",
    "        dict: contains image id to polygons\n",
    "    \"\"\"\n",
    "    dict_parcels = defaultdict(list) \n",
    "    for sh_index, sh in enumerate(shp_dict['features']):\n",
    "        id_list = sh['properties']['image_id']\n",
    "        for image_id in id_list:\n",
    "            dict_parcels[image_id].append(sh)\n",
    "    return dict_parcels\n",
    "\n",
    "def scale_coords(shape_size, geom, grid, index):\n",
    "    \"\"\"\n",
    "    scales the polygons lat/lon to pixel \n",
    "    Args:\n",
    "        shape_size (tuple): size of the image to be scaled to\n",
    "        geom (polygon): field polygon\n",
    "        grid (list): values of min/max lat/lon for each image id\n",
    "        index (int): Index of each image id in grid\n",
    "\n",
    "    Returns:\n",
    "        list: scaled coordinates\n",
    "    \"\"\"\n",
    "    w, h = shape_size\n",
    "    min_lat, min_lon, max_lat, max_lon = grid[index]['min_lat'], grid[index]['min_lon'], \\\n",
    "        grid[index]['max_lat'], grid[index]['max_lon']\n",
    "    x = geom[:,0]\n",
    "    y = geom[:,1]\n",
    "    scale_lon = w/(max_lon - min_lon)\n",
    "    scale_lat = h/(max_lat-min_lat)\n",
    "    scaled_x = (x - min_lon) * scale_lon # lon-> x, lat->y\n",
    "    scaled_y = h - ((y - min_lat) * scale_lat)\n",
    "    if any(val > w for val in scaled_x) or any(val > h for val in scaled_y) \\\n",
    "        or any(val < 0 for val in scaled_x) or any (val < 0 for val in scaled_y):\n",
    "        return np.concatenate([scaled_x[:,None], scaled_y[:,None]],axis=1)\n",
    "    return np.concatenate([scaled_x[:,None], scaled_y[:,None]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(df):\n",
    "    \"\"\"\n",
    "    Read the CSV file containing minmax lat/lon and stores it to a 2D list\n",
    "\n",
    "    Args:\n",
    "        df: pandas dataframe of bounding boxes\n",
    "\n",
    "    Returns:\n",
    "        list: contains the imag_id and lat/long information\n",
    "    \"\"\"\n",
    "    grid = dict()\n",
    "    keys = ['max_lat', 'max_lon', 'min_lat', 'min_lon']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if index not in grid:\n",
    "            grid[index] = dict()\n",
    "        grid[index]['image_id'] = row['image_id']\n",
    "        grid[index]['max_lat'] = float(row['max_lat'])\n",
    "        grid[index]['max_lon'] = float(row['max_lon'])\n",
    "        grid[index]['min_lat'] = float(row['min_lat'])\n",
    "        grid[index]['min_lon'] = float(row['min_lon'])\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== USER SETUP ======================== #\n",
    "# Directories to read the necessary files from\n",
    "base_dir = '../data/planet/india/GeneralBlockchain/'\n",
    "label_folder = 'extent_labels/'\n",
    "overlay_folder = 'overlays/'\n",
    "\n",
    "csv_file = os.path.join(base_dir, 'bbox_india.csv')\n",
    "json_file = os.path.join(base_dir, 'json_polys/bbox_images26205.json')\n",
    "orig_images_dir = os.path.join(base_dir, 'monthly_mosaics_renamed_clipped_merged', '2020_02')\n",
    "\n",
    "year_month = '_2020_02'\n",
    "thickness = 2\n",
    "n_overlays = 200\n",
    "# ============================================================ #\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df['image_id'] = df['image_id'].astype(str)\n",
    "grid = get_grid(df)\n",
    "\n",
    "# create output directory if not exist\n",
    "if os.path.exists(os.path.join(base_dir, label_folder)) == False:\n",
    "    os.makedirs(os.path.join(base_dir, label_folder))\n",
    "if os.path.exists(os.path.join(base_dir, overlay_folder)) == False:\n",
    "    os.makedirs(os.path.join(base_dir, overlay_folder))\n",
    "\n",
    "create_empty_overlays(new_grid, orig_images_dir, suffix='_2020_02', n=n_overlays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parcels = defaultdict(int)\n",
    "num_fields_parsed = 0\n",
    "\n",
    "# read multiple json files\n",
    "print('Read json', json_file)\n",
    "\n",
    "# open the saved json file for the found parcels in the images \n",
    "with open(json_file) as f:\n",
    "    shp_dict = json.load(f)\n",
    "\n",
    "# create dictionary of polygons in each image for fast indexing\n",
    "parcels_dict = create_dict_parcels(shp_dict)\n",
    "\n",
    "# find the polygons of each image and plot them\n",
    "for index in new_grid.keys():\n",
    "\n",
    "    image_id = new_grid[index]['image_id']\n",
    "    polys = []\n",
    "    if image_id in parcels_dict:\n",
    "        img = imageio.imread(os.path.join(orig_images_dir, str(image_id) + '_2020_02.tif'))\n",
    "        shape_size = (img.shape[0], img.shape[1])\n",
    "        extent_path = os.path.join(base_dir, label_folder, str(image_id) + '.png')\n",
    "        extent_label = np.zeros(shape_size)\n",
    "\n",
    "        for sh_index, sh in enumerate(parcels_dict[image_id]):\n",
    "            count_parcels[image_id] += 1 \n",
    "            for coord_idx in range(len(sh['geometry']['coordinates'])):\n",
    "                geom = np.array(sh['geometry']['coordinates'][coord_idx])\n",
    "                try:\n",
    "                    geom_fixed = scale_coords(shape_size, geom, new_grid, index)\n",
    "                except:\n",
    "                    print(\"Exception in image {}\".format(image_id))\n",
    "                    print(geom)\n",
    "                pts = geom_fixed.astype(int)\n",
    "                cv2.fillPoly(extent_label, [pts], color=(255,255,255))\n",
    "                polys.append(pts)\n",
    "\n",
    "        # Save the extent label\n",
    "        cv2.polylines(extent_label, polys, True, color=(0,0,0), thickness=thickness)\n",
    "        cv2.imwrite(extent_path, extent_label)\n",
    "\n",
    "        # Saves the overlay file\n",
    "        overlay_path = os.path.join(base_dir, overlay_folder, str(image_id) + '.jpeg')\n",
    "        if os.path.exists(overlay_path):\n",
    "            orig_image = cv2.imread(os.path.join(orig_images_dir, str(image_id) + year_month + '.tif'))\n",
    "            cv2.imwrite(overlay_path, orig_image)\n",
    "            orig_image = cv2.imread(overlay_path)\n",
    "            cv2.polylines(orig_image, polys, True, color=(255,255,255), thickness=thickness)\n",
    "            cv2.imwrite(overlay_path, orig_image)\n",
    "            print('saved image ', image_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_geopython)",
   "language": "python",
   "name": "conda_geopython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
