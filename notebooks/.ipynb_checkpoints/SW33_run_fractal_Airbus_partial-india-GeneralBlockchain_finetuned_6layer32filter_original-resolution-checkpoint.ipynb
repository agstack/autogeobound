{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../resuneta/src')\n",
    "sys.path.append('../../decode/FracTAL_ResUNet/models/semanticsegmentation')\n",
    "sys.path.append('../../decode/FracTAL_ResUNet/nn/loss')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../MXNet-ResUNeta/')\n",
    "\n",
    "from bound_dist import get_distance, get_boundary\n",
    "from FracTAL_ResUNet import FracTAL_ResUNet_cmtsk\n",
    "from ftnmt_loss import ftnmt_loss_masked\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(x, y):\n",
    "    if type(x).__module__ == 'numpy':\n",
    "        intersection = np.logical_and(x, y)\n",
    "        return 2. * np.sum(intersection) / (np.sum(x) + np.sum(y))\n",
    "    else:\n",
    "        intersection = mx.ndarray.op.broadcast_logical_and(x, y)\n",
    "        return 2. * mx.nd.sum(intersection) / (mx.nd.sum(x) + mx.nd.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visdom_visualize_batch(vis, img, extent, boundary, distance,\n",
    "                           extent_pred, boundary_pred, distance_pred,\n",
    "                           hsv, hsv_pred, mask, title=\"Train images\"):\n",
    "\n",
    "    img, extent, boundary, distance = img.asnumpy(), extent.asnumpy(), boundary.asnumpy(), distance.asnumpy()\n",
    "    extent_pred, boundary_pred = extent_pred.asnumpy(), boundary_pred.asnumpy()\n",
    "    distance_pred, hsv, hsv_pred = distance_pred.asnumpy(), hsv.asnumpy(), hsv_pred.asnumpy()\n",
    "    mask = mask.asnumpy()\n",
    "\n",
    "    # put everything in one window\n",
    "    batch_size, nchannels, nrows, ncols = img.shape\n",
    "    padding = 10\n",
    "    items = [img, hsv, hsv_pred, extent, extent_pred, \n",
    "             boundary, boundary_pred, distance, distance_pred,\n",
    "             mask]\n",
    "    result = np.zeros((3, len(items)*nrows + (len(items)-1)*padding, batch_size*ncols + (batch_size-1)*padding))\n",
    "\n",
    "    for j, item in enumerate(items):\n",
    "\n",
    "        if item.shape[1] == 1:\n",
    "            item = np.tile(item, (1,3,1,1)) * 255.\n",
    "\n",
    "        if j == 1 or j == 2: # convert HSV to RGB\n",
    "            item = np.moveaxis(item, 1, -1) * 255.\n",
    "            for i in range(batch_size):\n",
    "                item[i] = cv2.cvtColor(item[i].astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            item = np.moveaxis(item, -1, 1)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            result[:, j*(nrows+padding):(j+1)*nrows+j*padding, i*(ncols+padding):(i+1)*ncols+i*padding] = item[i]\n",
    "    vis.images(result, nrow=1, win=title, opts={'title': title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, model, tanimoto_dual, trainer, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # training set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(train_dataloader, desc='Training epoch {}'.format(epoch))):\n",
    "        \n",
    "        with autograd.record():\n",
    "\n",
    "            img = img.as_in_context(ctx)\n",
    "            extent = extent.as_in_context(ctx)\n",
    "            boundary = boundary.as_in_context(ctx)\n",
    "            distance = distance.as_in_context(ctx)\n",
    "            hsv = hsv.as_in_context(ctx)\n",
    "            mask = mask.as_in_context(ctx)\n",
    "            nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "            \n",
    "            # logits, bound, dist, convc = model(img)\n",
    "            logits, bound, dist = model(img)\n",
    "            \n",
    "            # multi-task loss\n",
    "            # TODO: wrap this in a custom loss function / class\n",
    "            loss_extent = mx.nd.sum(tanimoto_dual(logits, extent, mask))\n",
    "            loss_boundary = mx.nd.sum(tanimoto_dual(bound, boundary, mask))\n",
    "            loss_distance = mx.nd.sum(tanimoto_dual(dist, distance, mask))\n",
    "\n",
    "            loss = 0.33 * (loss_extent + loss_boundary + loss_distance) # + loss_hsv)\n",
    "            \n",
    "        loss.backward()\n",
    "        trainer.step(args['batch_size'])\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TEMPORARY to make visdom work\n",
    "        convc = hsv\n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dataloader, model, tanimoto_dual, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # validation set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(val_dataloader, desc='Validation epoch {}'.format(epoch))):\n",
    "\n",
    "        img = img.as_in_context(ctx)\n",
    "        extent = extent.as_in_context(ctx)\n",
    "        boundary = boundary.as_in_context(ctx)\n",
    "        distance = distance.as_in_context(ctx)\n",
    "        hsv = hsv.as_in_context(ctx)\n",
    "        mask = mask.as_in_context(ctx)\n",
    "        nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "\n",
    "        # logits, bound, dist, convc = model(img)\n",
    "        logits, bound, dist = model(img)\n",
    "        \n",
    "        # multi-task loss\n",
    "        # TODO: wrap this in a custom loss function / class\n",
    "        loss_extent = mx.nd.sum(tanimoto_dual(logits, extent, mask))\n",
    "        loss_boundary = mx.nd.sum(tanimoto_dual(bound, boundary, mask))\n",
    "        loss_distance = mx.nd.sum(tanimoto_dual(dist, distance, mask))\n",
    "\n",
    "        loss = 0.33 * (loss_extent + loss_boundary + loss_distance) # + loss_hsv)\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        # mask out unlabeled pixels            \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TEMPORARY to make visdom work\n",
    "        convc = hsv\n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask, title=\"Val images\")\n",
    "        \n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_africa(country, train_names, val_names, test_names, \n",
    "               train_names_label, val_names_label, test_names_label,\n",
    "               trained_model=None, month='Airbus',\n",
    "               epochs=100, lr=0.001, lr_decay=None, n_filters=16, batch_size=8,\n",
    "               model_type='fractal-resunet', depth=5, n_classes=1, \n",
    "               codes_to_keep=[1, 2],\n",
    "               folder_suffix='',\n",
    "               boundary_kernel_size=3,\n",
    "               ctx_name='cpu',\n",
    "               gpu_id=0):\n",
    "    \n",
    "    # Set MXNet ctx\n",
    "    if ctx_name == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    elif ctx_name == 'gpu':\n",
    "        ctx = mx.gpu(gpu_id)\n",
    "    \n",
    "    # Set up names of directories and paths for saving\n",
    "    if trained_model is None:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+'_depth-'+str(depth)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if lr_decay:\n",
    "            folder_name = folder_name + '_lrdecay-'+str(lr_decay)\n",
    "            \n",
    "        # define model\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'fractal-resunet':\n",
    "            model = FracTAL_ResUNet_cmtsk(nfilters_init=n_filters, depth=depth, NClasses=n_classes)\n",
    "        model.initialize()\n",
    "        model.hybridize()\n",
    "        model.collect_params().reset_ctx(ctx)\n",
    "        \n",
    "    else:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+'_depth-'+str(depth)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'fractal-resunet':\n",
    "            model = FracTAL_ResUNet_cmtsk(nfilters_init=n_filters, depth=depth, NClasses=n_classes)\n",
    "        model.load_parameters(trained_model, ctx=ctx)\n",
    "        \n",
    "    save_path = os.path.join('../experiments/', country, folder_name)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_model_name = os.path.join(save_path, \"model.params\")\n",
    "    \n",
    "    # Visdom\n",
    "    env_name = country + '_' + folder_name\n",
    "    vis = visdom.Visdom(port=8097, env=env_name)\n",
    "    \n",
    "    # Arguments\n",
    "    args = {}\n",
    "    args['batch_size'] = batch_size\n",
    "    args['ctx_name'] = ctx_name\n",
    "    args['gpu'] = gpu_id\n",
    "    args['visdom'] = vis\n",
    "    args['visdom_every'] = 20\n",
    "\n",
    "    # Define train/val/test splits\n",
    "    train_dataset = AirbusMasked(\n",
    "        fold='train', \n",
    "        image_names=train_names, \n",
    "        label_names=train_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    val_dataset = AirbusMasked(\n",
    "        fold='val', \n",
    "        image_names=val_names, \n",
    "        label_names=val_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    test_dataset = AirbusMasked(\n",
    "        fold='test', \n",
    "        image_names=test_names, \n",
    "        label_names=test_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = gluon.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # define loss function\n",
    "    tanimoto_dual = ftnmt_loss_masked(depth=0) # Tanimoto_with_dual_masked()\n",
    "    if lr_decay:\n",
    "        schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "    else:\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "    # containers for metrics to log\n",
    "    train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                     'train_mcc': [], 'train_dice': []}\n",
    "    val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "                   'val_mcc': [], 'val_dice': []}\n",
    "    best_mcc = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training set\n",
    "        train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n",
    "            train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
    "\n",
    "        # training set metrics\n",
    "        train_loss_avg = train_loss / len(train_dataset)\n",
    "        train_metrics['train_loss'].append(train_loss_avg)\n",
    "        train_metrics['train_acc'].append(train_accuracy.get()[1])\n",
    "        train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "        train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "        train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "\n",
    "        # validation set\n",
    "        val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model(\n",
    "            val_dataloader, model, tanimoto_dual, epoch, args)\n",
    "\n",
    "        # validation set metrics\n",
    "        val_loss_avg = val_loss / len(val_dataset)\n",
    "        val_metrics['val_loss'].append(val_loss_avg)\n",
    "        val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "        val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "        val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "        val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "        print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "\n",
    "        # save model based on best MCC metric\n",
    "        if val_mcc.get()[1] > best_mcc:\n",
    "            model.save_parameters(save_model_name)\n",
    "            best_mcc = val_mcc.get()[1]\n",
    "\n",
    "        # save metrics\n",
    "        metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "        metrics.to_csv(os.path.join(save_path, 'metrics.csv'), index=False)\n",
    "\n",
    "        # visdom\n",
    "        vis.line(Y=np.stack([train_metrics['train_loss'], val_metrics['val_loss']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"Loss\", \n",
    "                 opts=dict(legend=['train loss', 'val loss'], markers=False, title=\"Losses\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "                )\n",
    "        vis.line(Y=np.stack([train_metrics['train_mcc'], val_metrics['val_mcc']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"MCC\", \n",
    "                 opts=dict(legend=['train MCC', 'val MCC'], markers=False, title=\"MCC\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"MCC\")\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on 1x-3x downsampled Airbus imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Training epoch 1:  14%|█▍        | 107/769 [01:52<11:35,  1.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89174eaac6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m            \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m            \u001b[0mfolder_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder_suffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m            boundary_kernel_size=boundary_kernel_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-63fd61006de6>\u001b[0m in \u001b[0;36mrun_africa\u001b[0;34m(country, train_names, val_names, test_names, train_names_label, val_names_label, test_names_label, trained_model, month, epochs, lr, lr_decay, n_filters, batch_size, model_type, depth, n_classes, codes_to_keep, folder_suffix, boundary_kernel_size, ctx_name, gpu_id)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n\u001b[0;32m--> 109\u001b[0;31m             train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# training set metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5ee6e94e3209>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, tanimoto_dual, trainer, epoch, args)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mcumulative_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mlogits_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'india'\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "lr_decay = None\n",
    "n_filters = 32\n",
    "depth = 6\n",
    "n_classes = 1\n",
    "batch_size = 5\n",
    "model_type = 'fractal-resunet' # 'resunet-d6'\n",
    "month = 'all13'\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 0\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "trained_model = '../experiments/partial-france/fractal-resunet_3month-separate_nfilter-32_depth-6_bs-8_lr-0.001_2x-3x-downsampled_allfields_n6759/model.params'\n",
    "# trained_model = '../experiments/india/fractal-resunet_Airbus_nfilter-32_depth-6_bs-5_lr-0.001_3x-downsampled-erosion2px_n200_fromscratch/model.params'\n",
    "# trained_model = None\n",
    "\n",
    "folder_suffix = '_1x-3x-downsampled-erosion2px'\n",
    "if trained_model is None:\n",
    "    folder_suffix += '_fromscratch'\n",
    "elif 'india' in trained_model:\n",
    "    folder_suffix += '_fromscratch-continued'\n",
    "elif 'france' in trained_model:\n",
    "    folder_suffix += '_finetuned'\n",
    "    \n",
    "month_name = 'Airbus'\n",
    "splits_path = '../data/splits/india_planetImagery_splits_20x20_v2.csv'\n",
    "splits_df = pd.read_csv(splits_path, dtype=str)\n",
    "splits_df['image_id'] = splits_df['image_id'].str.zfill(4)\n",
    "splits_df = splits_df.drop_duplicates('image_id')\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dirs = ['../data/general_blockchain/airbus_false_color/large/original/',\n",
    "            '../data/general_blockchain/airbus_false_color/large/2x_downsample/',\n",
    "            '../data/general_blockchain/airbus_false_color/large/3x_downsample/']\n",
    "label_dirs = ['../data/general_blockchain/airbus_labels/large/original/',\n",
    "              '../data/general_blockchain/airbus_labels/large/2x_downsample_erosion1px/',\n",
    "              '../data/general_blockchain/airbus_labels/large/3x_downsample_erosion2px/']\n",
    "\n",
    "for img_dir, label_dir in zip(img_dirs, label_dirs):\n",
    "    label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "    for label_name in label_folder_imgs:\n",
    "        img_name = 'airbus_geowiki_' + label_name.split('_')[-1].split('.')[0] + '.png'\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        all_img_names.append(img_path)\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        all_label_names.append(label_path)\n",
    "    \n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[-1][1:])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[-1][1:])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, \n",
    "           model_type=model_type, n_filters=n_filters, depth=depth, n_classes=n_classes,\n",
    "           batch_size=batch_size, month=month_name,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix=folder_suffix,\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only original resolution imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Training epoch 1: 100%|██████████| 257/257 [14:25<00:00,  3.37s/it]\n",
      "Validation epoch 1: 100%|██████████| 60/60 [01:46<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Train loss 0.230, accuracy 0.901, F1-score 0.940, MCC: 0.464, Dice: 0.940\n",
      "    Val loss 0.227, accuracy 0.912, F1-score 0.949, MCC: 0.473, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 257/257 [14:33<00:00,  3.40s/it]\n",
      "Validation epoch 2: 100%|██████████| 60/60 [02:04<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "    Train loss 0.226, accuracy 0.904, F1-score 0.940, MCC: 0.473, Dice: 0.940\n",
      "    Val loss 0.223, accuracy 0.913, F1-score 0.948, MCC: 0.482, Dice: 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 257/257 [15:14<00:00,  3.56s/it]\n",
      "Validation epoch 3: 100%|██████████| 60/60 [02:01<00:00,  2.02s/it]\n",
      "Training epoch 4:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "    Train loss 0.226, accuracy 0.904, F1-score 0.940, MCC: 0.471, Dice: 0.940\n",
      "    Val loss 0.219, accuracy 0.918, F1-score 0.951, MCC: 0.475, Dice: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 257/257 [15:27<00:00,  3.61s/it]\n",
      "Validation epoch 4: 100%|██████████| 60/60 [02:08<00:00,  2.15s/it]\n",
      "Training epoch 5:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "    Train loss 0.227, accuracy 0.906, F1-score 0.941, MCC: 0.478, Dice: 0.941\n",
      "    Val loss 0.225, accuracy 0.914, F1-score 0.950, MCC: 0.470, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 257/257 [15:40<00:00,  3.66s/it]\n",
      "Validation epoch 5: 100%|██████████| 60/60 [02:11<00:00,  2.19s/it]\n",
      "Training epoch 6:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "    Train loss 0.226, accuracy 0.904, F1-score 0.941, MCC: 0.475, Dice: 0.941\n",
      "    Val loss 0.224, accuracy 0.913, F1-score 0.948, MCC: 0.476, Dice: 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 257/257 [15:41<00:00,  3.67s/it]\n",
      "Validation epoch 6: 100%|██████████| 60/60 [02:10<00:00,  2.18s/it]\n",
      "Training epoch 7:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "    Train loss 0.228, accuracy 0.905, F1-score 0.941, MCC: 0.478, Dice: 0.941\n",
      "    Val loss 0.220, accuracy 0.918, F1-score 0.951, MCC: 0.479, Dice: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 257/257 [15:46<00:00,  3.68s/it]\n",
      "Validation epoch 7: 100%|██████████| 60/60 [02:10<00:00,  2.17s/it]\n",
      "Training epoch 8:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "    Train loss 0.226, accuracy 0.904, F1-score 0.942, MCC: 0.474, Dice: 0.942\n",
      "    Val loss 0.219, accuracy 0.915, F1-score 0.949, MCC: 0.479, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 8: 100%|██████████| 257/257 [15:45<00:00,  3.68s/it]\n",
      "Validation epoch 8: 100%|██████████| 60/60 [02:10<00:00,  2.17s/it]\n",
      "Training epoch 9:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "    Train loss 0.228, accuracy 0.903, F1-score 0.940, MCC: 0.473, Dice: 0.940\n",
      "    Val loss 0.220, accuracy 0.915, F1-score 0.950, MCC: 0.475, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 9: 100%|██████████| 257/257 [15:43<00:00,  3.67s/it]\n",
      "Validation epoch 9: 100%|██████████| 60/60 [02:10<00:00,  2.17s/it]\n",
      "Training epoch 10:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "    Train loss 0.224, accuracy 0.907, F1-score 0.942, MCC: 0.475, Dice: 0.942\n",
      "    Val loss 0.219, accuracy 0.915, F1-score 0.950, MCC: 0.468, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 10: 100%|██████████| 257/257 [15:26<00:00,  3.60s/it]\n",
      "Validation epoch 10: 100%|██████████| 60/60 [02:06<00:00,  2.12s/it]\n",
      "Training epoch 11:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "    Train loss 0.226, accuracy 0.904, F1-score 0.940, MCC: 0.477, Dice: 0.940\n",
      "    Val loss 0.226, accuracy 0.919, F1-score 0.953, MCC: 0.477, Dice: 0.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 11: 100%|██████████| 257/257 [15:09<00:00,  3.54s/it]\n",
      "Validation epoch 11: 100%|██████████| 60/60 [02:00<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "    Train loss 0.224, accuracy 0.905, F1-score 0.941, MCC: 0.473, Dice: 0.941\n",
      "    Val loss 0.222, accuracy 0.914, F1-score 0.950, MCC: 0.484, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 12: 100%|██████████| 257/257 [15:11<00:00,  3.55s/it]\n",
      "Validation epoch 12: 100%|██████████| 60/60 [02:02<00:00,  2.04s/it]\n",
      "Training epoch 13:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "    Train loss 0.228, accuracy 0.908, F1-score 0.941, MCC: 0.476, Dice: 0.941\n",
      "    Val loss 0.216, accuracy 0.915, F1-score 0.952, MCC: 0.483, Dice: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 13: 100%|██████████| 257/257 [15:04<00:00,  3.52s/it]\n",
      "Validation epoch 13: 100%|██████████| 60/60 [02:05<00:00,  2.09s/it]\n",
      "Training epoch 14:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "    Train loss 0.224, accuracy 0.906, F1-score 0.942, MCC: 0.478, Dice: 0.942\n",
      "    Val loss 0.219, accuracy 0.913, F1-score 0.950, MCC: 0.472, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 14: 100%|██████████| 257/257 [15:09<00:00,  3.54s/it]\n",
      "Validation epoch 14: 100%|██████████| 60/60 [02:03<00:00,  2.05s/it]\n",
      "Training epoch 15:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "    Train loss 0.225, accuracy 0.907, F1-score 0.941, MCC: 0.479, Dice: 0.941\n",
      "    Val loss 0.223, accuracy 0.912, F1-score 0.949, MCC: 0.475, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 15: 100%|██████████| 257/257 [15:04<00:00,  3.52s/it]\n",
      "Validation epoch 15: 100%|██████████| 60/60 [01:59<00:00,  2.00s/it]\n",
      "Training epoch 16:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "    Train loss 0.223, accuracy 0.903, F1-score 0.941, MCC: 0.476, Dice: 0.941\n",
      "    Val loss 0.230, accuracy 0.909, F1-score 0.946, MCC: 0.472, Dice: 0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 16:  91%|█████████▏| 235/257 [13:45<01:12,  3.31s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 31: 100%|██████████| 257/257 [14:59<00:00,  3.50s/it]\n",
      "Validation epoch 31: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it]\n",
      "Training epoch 32:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31:\n",
      "    Train loss 0.225, accuracy 0.906, F1-score 0.942, MCC: 0.480, Dice: 0.942\n",
      "    Val loss 0.219, accuracy 0.916, F1-score 0.951, MCC: 0.481, Dice: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 32: 100%|██████████| 257/257 [15:15<00:00,  3.56s/it]\n",
      "Validation epoch 32: 100%|██████████| 60/60 [02:04<00:00,  2.07s/it]\n",
      "Training epoch 33:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32:\n",
      "    Train loss 0.222, accuracy 0.906, F1-score 0.941, MCC: 0.483, Dice: 0.941\n",
      "    Val loss 0.223, accuracy 0.911, F1-score 0.949, MCC: 0.479, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 33: 100%|██████████| 257/257 [15:12<00:00,  3.55s/it]\n",
      "Validation epoch 33: 100%|██████████| 60/60 [02:04<00:00,  2.07s/it]\n",
      "Training epoch 34:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33:\n",
      "    Train loss 0.223, accuracy 0.907, F1-score 0.940, MCC: 0.479, Dice: 0.940\n",
      "    Val loss 0.224, accuracy 0.914, F1-score 0.950, MCC: 0.484, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 34: 100%|██████████| 257/257 [15:13<00:00,  3.55s/it]\n",
      "Validation epoch 34: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it]\n",
      "Training epoch 35:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:\n",
      "    Train loss 0.224, accuracy 0.904, F1-score 0.940, MCC: 0.470, Dice: 0.940\n",
      "    Val loss 0.221, accuracy 0.916, F1-score 0.950, MCC: 0.472, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 35: 100%|██████████| 257/257 [15:09<00:00,  3.54s/it]\n",
      "Validation epoch 35: 100%|██████████| 60/60 [02:03<00:00,  2.05s/it]\n",
      "Training epoch 36:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35:\n",
      "    Train loss 0.225, accuracy 0.903, F1-score 0.940, MCC: 0.478, Dice: 0.940\n",
      "    Val loss 0.219, accuracy 0.916, F1-score 0.952, MCC: 0.475, Dice: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 36: 100%|██████████| 257/257 [15:02<00:00,  3.51s/it]\n",
      "Validation epoch 36: 100%|██████████| 60/60 [02:02<00:00,  2.04s/it]\n",
      "Training epoch 37:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36:\n",
      "    Train loss 0.221, accuracy 0.907, F1-score 0.942, MCC: 0.482, Dice: 0.942\n",
      "    Val loss 0.214, accuracy 0.913, F1-score 0.949, MCC: 0.471, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 37: 100%|██████████| 257/257 [15:13<00:00,  3.55s/it]\n",
      "Validation epoch 37: 100%|██████████| 60/60 [02:05<00:00,  2.10s/it]\n",
      "Training epoch 38:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37:\n",
      "    Train loss 0.223, accuracy 0.907, F1-score 0.942, MCC: 0.475, Dice: 0.942\n",
      "    Val loss 0.221, accuracy 0.914, F1-score 0.949, MCC: 0.480, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 38: 100%|██████████| 257/257 [15:14<00:00,  3.56s/it]\n",
      "Validation epoch 38: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it]\n",
      "Training epoch 39:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38:\n",
      "    Train loss 0.222, accuracy 0.908, F1-score 0.940, MCC: 0.481, Dice: 0.940\n",
      "    Val loss 0.225, accuracy 0.913, F1-score 0.947, MCC: 0.468, Dice: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 39: 100%|██████████| 257/257 [15:13<00:00,  3.55s/it]\n",
      "Validation epoch 39: 100%|██████████| 60/60 [02:03<00:00,  2.05s/it]\n",
      "Training epoch 40:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39:\n",
      "    Train loss 0.223, accuracy 0.907, F1-score 0.943, MCC: 0.481, Dice: 0.943\n",
      "    Val loss 0.216, accuracy 0.913, F1-score 0.949, MCC: 0.475, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 40:  70%|███████   | 180/257 [10:30<04:18,  3.36s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 54: 100%|██████████| 257/257 [15:04<00:00,  3.52s/it]\n",
      "Validation epoch 54: 100%|██████████| 60/60 [02:03<00:00,  2.06s/it]\n",
      "Training epoch 55:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54:\n",
      "    Train loss 0.221, accuracy 0.906, F1-score 0.943, MCC: 0.482, Dice: 0.943\n",
      "    Val loss 0.222, accuracy 0.918, F1-score 0.952, MCC: 0.478, Dice: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 55: 100%|██████████| 257/257 [15:11<00:00,  3.55s/it]\n",
      "Validation epoch 55: 100%|██████████| 60/60 [01:58<00:00,  1.97s/it]\n",
      "Training epoch 56:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55:\n",
      "    Train loss 0.222, accuracy 0.906, F1-score 0.942, MCC: 0.478, Dice: 0.942\n",
      "    Val loss 0.226, accuracy 0.915, F1-score 0.951, MCC: 0.472, Dice: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 56: 100%|██████████| 257/257 [14:58<00:00,  3.49s/it]\n",
      "Validation epoch 56: 100%|██████████| 60/60 [02:03<00:00,  2.06s/it]\n",
      "Training epoch 57:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56:\n",
      "    Train loss 0.220, accuracy 0.905, F1-score 0.941, MCC: 0.484, Dice: 0.941\n",
      "    Val loss 0.223, accuracy 0.913, F1-score 0.948, MCC: 0.478, Dice: 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 57: 100%|██████████| 257/257 [15:02<00:00,  3.51s/it]\n",
      "Validation epoch 57: 100%|██████████| 60/60 [02:06<00:00,  2.10s/it]\n",
      "Training epoch 58:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:\n",
      "    Train loss 0.222, accuracy 0.907, F1-score 0.942, MCC: 0.479, Dice: 0.942\n",
      "    Val loss 0.224, accuracy 0.911, F1-score 0.947, MCC: 0.483, Dice: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 58: 100%|██████████| 257/257 [14:56<00:00,  3.49s/it]\n",
      "Validation epoch 58: 100%|██████████| 60/60 [02:04<00:00,  2.07s/it]\n",
      "Training epoch 59:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58:\n",
      "    Train loss 0.223, accuracy 0.907, F1-score 0.942, MCC: 0.484, Dice: 0.942\n",
      "    Val loss 0.225, accuracy 0.917, F1-score 0.950, MCC: 0.476, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 59: 100%|██████████| 257/257 [15:10<00:00,  3.54s/it]\n",
      "Validation epoch 59: 100%|██████████| 60/60 [02:08<00:00,  2.13s/it]\n",
      "Training epoch 60:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59:\n",
      "    Train loss 0.220, accuracy 0.907, F1-score 0.943, MCC: 0.486, Dice: 0.943\n",
      "    Val loss 0.219, accuracy 0.914, F1-score 0.950, MCC: 0.477, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 60: 100%|██████████| 257/257 [15:17<00:00,  3.57s/it]\n",
      "Validation epoch 60: 100%|██████████| 60/60 [02:09<00:00,  2.16s/it]\n",
      "Training epoch 61:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60:\n",
      "    Train loss 0.224, accuracy 0.906, F1-score 0.941, MCC: 0.477, Dice: 0.941\n",
      "    Val loss 0.224, accuracy 0.910, F1-score 0.948, MCC: 0.468, Dice: 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 61: 100%|██████████| 257/257 [15:41<00:00,  3.66s/it]\n",
      "Validation epoch 61: 100%|██████████| 60/60 [02:15<00:00,  2.25s/it]\n",
      "Training epoch 62:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61:\n",
      "    Train loss 0.224, accuracy 0.905, F1-score 0.942, MCC: 0.485, Dice: 0.942\n",
      "    Val loss 0.217, accuracy 0.915, F1-score 0.951, MCC: 0.485, Dice: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 62: 100%|██████████| 257/257 [15:27<00:00,  3.61s/it]\n",
      "Validation epoch 62: 100%|██████████| 60/60 [02:14<00:00,  2.24s/it]\n",
      "Training epoch 63:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62:\n",
      "    Train loss 0.222, accuracy 0.904, F1-score 0.941, MCC: 0.485, Dice: 0.941\n",
      "    Val loss 0.223, accuracy 0.911, F1-score 0.947, MCC: 0.472, Dice: 0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 63:  47%|████▋     | 122/257 [07:26<09:04,  4.03s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 77: 100%|██████████| 257/257 [14:56<00:00,  3.49s/it]\n",
      "Validation epoch 77: 100%|██████████| 60/60 [02:05<00:00,  2.09s/it]\n",
      "Training epoch 78:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77:\n",
      "    Train loss 0.221, accuracy 0.905, F1-score 0.941, MCC: 0.482, Dice: 0.941\n",
      "    Val loss 0.221, accuracy 0.913, F1-score 0.950, MCC: 0.482, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 78: 100%|██████████| 257/257 [15:10<00:00,  3.54s/it]\n",
      "Validation epoch 78: 100%|██████████| 60/60 [02:03<00:00,  2.06s/it]\n",
      "Training epoch 79:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78:\n",
      "    Train loss 0.221, accuracy 0.905, F1-score 0.942, MCC: 0.482, Dice: 0.942\n",
      "    Val loss 0.219, accuracy 0.915, F1-score 0.950, MCC: 0.470, Dice: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 79: 100%|██████████| 257/257 [15:06<00:00,  3.53s/it]\n",
      "Validation epoch 79: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it]\n",
      "Training epoch 80:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79:\n",
      "    Train loss 0.222, accuracy 0.907, F1-score 0.943, MCC: 0.487, Dice: 0.943\n",
      "    Val loss 0.221, accuracy 0.916, F1-score 0.949, MCC: 0.477, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 80: 100%|██████████| 257/257 [15:11<00:00,  3.55s/it]\n",
      "Validation epoch 80: 100%|██████████| 60/60 [02:04<00:00,  2.07s/it]\n",
      "Training epoch 81:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80:\n",
      "    Train loss 0.219, accuracy 0.908, F1-score 0.943, MCC: 0.487, Dice: 0.943\n",
      "    Val loss 0.217, accuracy 0.913, F1-score 0.949, MCC: 0.475, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 81: 100%|██████████| 257/257 [15:04<00:00,  3.52s/it]\n",
      "Validation epoch 81: 100%|██████████| 60/60 [02:04<00:00,  2.08s/it]\n",
      "Training epoch 82:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81:\n",
      "    Train loss 0.220, accuracy 0.906, F1-score 0.942, MCC: 0.481, Dice: 0.942\n",
      "    Val loss 0.222, accuracy 0.916, F1-score 0.949, MCC: 0.480, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 82: 100%|██████████| 257/257 [15:16<00:00,  3.57s/it]\n",
      "Validation epoch 82: 100%|██████████| 60/60 [02:03<00:00,  2.06s/it]\n",
      "Training epoch 83:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82:\n",
      "    Train loss 0.225, accuracy 0.909, F1-score 0.943, MCC: 0.484, Dice: 0.943\n",
      "    Val loss 0.219, accuracy 0.915, F1-score 0.949, MCC: 0.474, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 83: 100%|██████████| 257/257 [15:15<00:00,  3.56s/it]\n",
      "Validation epoch 83: 100%|██████████| 60/60 [01:58<00:00,  1.98s/it]\n",
      "Training epoch 84:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83:\n",
      "    Train loss 0.221, accuracy 0.907, F1-score 0.942, MCC: 0.488, Dice: 0.942\n",
      "    Val loss 0.221, accuracy 0.915, F1-score 0.951, MCC: 0.463, Dice: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 84: 100%|██████████| 257/257 [14:59<00:00,  3.50s/it]\n",
      "Validation epoch 84: 100%|██████████| 60/60 [02:03<00:00,  2.05s/it]\n",
      "Training epoch 85:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84:\n",
      "    Train loss 0.223, accuracy 0.908, F1-score 0.942, MCC: 0.483, Dice: 0.942\n",
      "    Val loss 0.223, accuracy 0.915, F1-score 0.949, MCC: 0.473, Dice: 0.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 85: 100%|██████████| 257/257 [15:09<00:00,  3.54s/it]\n",
      "Validation epoch 85: 100%|██████████| 60/60 [02:01<00:00,  2.02s/it]\n",
      "Training epoch 86:   0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85:\n",
      "    Train loss 0.221, accuracy 0.906, F1-score 0.942, MCC: 0.484, Dice: 0.942\n",
      "    Val loss 0.222, accuracy 0.918, F1-score 0.952, MCC: 0.486, Dice: 0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 86: 100%|██████████| 257/257 [15:05<00:00,  3.52s/it]\n",
      "Validation epoch 86:  92%|█████████▏| 55/60 [01:53<00:09,  1.87s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'india'\n",
    "epochs = 200\n",
    "lr = 0.0003\n",
    "lr_decay = None\n",
    "n_filters = 32\n",
    "depth = 6\n",
    "n_classes = 1\n",
    "batch_size = 5\n",
    "model_type = 'fractal-resunet' # 'resunet-d6'\n",
    "month = 'all13'\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 0\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "# trained_model = '../experiments/partial-france/fractal-resunet_3month-separate_nfilter-32_depth-6_bs-8_lr-0.001_2x-3x-downsampled_allfields_n6759/model.params'\n",
    "trained_model = '../experiments/india/fractal-resunet_Airbus_nfilter-32_depth-6_bs-5_lr-0.001_original-resolution_fromscratch-continued/model.params'\n",
    "# trained_model = None\n",
    "\n",
    "folder_suffix = '_original-resolution'\n",
    "if trained_model is None:\n",
    "    folder_suffix += '_fromscratch'\n",
    "elif 'india' in trained_model:\n",
    "    folder_suffix += '_fromscratch-continued2'\n",
    "elif 'france' in trained_model:\n",
    "    folder_suffix += '_finetuned'\n",
    "    \n",
    "month_name = 'Airbus'\n",
    "splits_path = '../data/splits/india_planetImagery_splits_20x20_v2.csv'\n",
    "splits_df = pd.read_csv(splits_path, dtype=str)\n",
    "splits_df['image_id'] = splits_df['image_id'].str.zfill(4)\n",
    "splits_df = splits_df.drop_duplicates('image_id')\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dir = '../data/general_blockchain/airbus_false_color/large/original/'\n",
    "label_dir = '../data/general_blockchain/airbus_labels/large/original/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for label_name in label_folder_imgs:\n",
    "    img_name = 'airbus_geowiki_' + label_name.split('_')[-1].split('.')[0] + '.png'\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    all_img_names.append(img_path)\n",
    "    label_path = os.path.join(label_dir, label_name)\n",
    "    all_label_names.append(label_path)\n",
    "    \n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[-1][1:])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[-1][1:])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, \n",
    "           model_type=model_type, n_filters=n_filters, depth=depth, n_classes=n_classes,\n",
    "           batch_size=batch_size, month=month_name,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix=folder_suffix,\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/connection.py\", line 160, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/util/connection.py\", line 84, in create_connection\n",
      "    raise err\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/util/connection.py\", line 74, in create_connection\n",
      "    sock.connect(sa)\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 677, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 392, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/http/client.py\", line 1272, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/http/client.py\", line 1318, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/http/client.py\", line 1267, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/http/client.py\", line 1038, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/http/client.py\", line 976, in send\n",
      "    self.connect()\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/connection.py\", line 187, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/connection.py\", line 172, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f76182ac4e0>: Failed to establish a new connection: [Errno 111] Connection refused\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 727, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/urllib3/util/retry.py\", line 446, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/india_fractal-resunet_Airbus_nfilter-32_depth-6_bs-8_lr-0.0001_original-resolution_n40_finetuned-continued (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f76182ac4e0>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/visdom/__init__.py\", line 711, in _send\n",
      "    data=json.dumps(msg),\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/visdom/__init__.py\", line 677, in _handle_post\n",
      "    r = self.session.post(url, data=data)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/requests/sessions.py\", line 590, in post\n",
      "    return self.request('POST', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/requests/adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/india_fractal-resunet_Airbus_nfilter-32_depth-6_bs-8_lr-0.0001_original-resolution_n40_finetuned-continued (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f76182ac4e0>: Failed to establish a new connection: [Errno 111] Connection refused',))\n",
      "[Errno 111] Connection refused\n",
      "Training epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in user code:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1:  40%|████      | 2/5 [00:13<00:20,  6.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7ca92d12ef9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m            \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m            \u001b[0mfolder_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder_suffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m            boundary_kernel_size=boundary_kernel_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-63fd61006de6>\u001b[0m in \u001b[0;36mrun_africa\u001b[0;34m(country, train_names, val_names, test_names, train_names_label, val_names_label, test_names_label, trained_model, month, epochs, lr, lr_decay, n_filters, batch_size, model_type, depth, n_classes, codes_to_keep, folder_suffix, boundary_kernel_size, ctx_name, gpu_id)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n\u001b[0;32m--> 109\u001b[0;31m             train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# training set metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5ee6e94e3209>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, tanimoto_dual, trainer, epoch, args)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mcumulative_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mlogits_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'india'\n",
    "epochs = 300\n",
    "lr = 0.0001\n",
    "lr_decay = None\n",
    "n_filters = 32\n",
    "depth = 6\n",
    "n_classes = 1\n",
    "batch_size = 8\n",
    "model_type = 'fractal-resunet' # 'resunet-d6'\n",
    "month = 'all13'\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 1\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "# trained_model = '../experiments/partial-france/fractal-resunet_3month-separate_nfilter-32_depth-6_bs-8_lr-0.001_2x-3x-downsampled_allfields_n6759/model.params'\n",
    "trained_model = '../experiments/india/fractal-resunet_Airbus_nfilter-32_depth-6_bs-8_lr-0.0001_original-resolution_n40_finetuned/model.params'\n",
    "# trained_model = None\n",
    "\n",
    "folder_suffix = '_original-resolution_n40'\n",
    "if trained_model is None:\n",
    "    folder_suffix += '_fromscratch'\n",
    "elif 'india' in trained_model:\n",
    "    folder_suffix += '_finetuned-continued'\n",
    "elif 'france' in trained_model:\n",
    "    folder_suffix += '_finetuned'\n",
    "    \n",
    "month_name = 'Airbus'\n",
    "splits_path = '../data/splits/india_planetImagery_splits_20x20_n40.csv'\n",
    "splits_df = pd.read_csv(splits_path, dtype=str)\n",
    "splits_df['image_id'] = splits_df['image_id'].str.zfill(4)\n",
    "splits_df = splits_df.drop_duplicates('image_id')\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dir = '../data/general_blockchain/airbus_false_color/large/original/'\n",
    "label_dir = '../data/general_blockchain/airbus_labels/large/original/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for label_name in label_folder_imgs:\n",
    "    img_name = 'airbus_geowiki_' + label_name.split('_')[-1].split('.')[0] + '.png'\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    all_img_names.append(img_path)\n",
    "    label_path = os.path.join(label_dir, label_name)\n",
    "    all_label_names.append(label_path)\n",
    "    \n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[-1][1:])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[-1][1:])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, \n",
    "           model_type=model_type, n_filters=n_filters, depth=depth, n_classes=n_classes,\n",
    "           batch_size=batch_size, month=month_name,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix=folder_suffix,\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet1.6.0)",
   "language": "python",
   "name": "conda_mxnet1.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
