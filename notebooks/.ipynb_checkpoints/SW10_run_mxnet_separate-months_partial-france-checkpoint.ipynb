{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../resuneta/src')\n",
    "sys.path.append('../../resuneta/nn/loss')\n",
    "sys.path.append('../../resuneta/models')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../MXNet-ResUNeta/')\n",
    "\n",
    "from bound_dist import get_distance, get_boundary\n",
    "from loss import Tanimoto_with_dual_masked\n",
    "from resunet_d6_causal_mtskcolor_ddist import *\n",
    "from resunet_d7_causal_mtskcolor_ddist import *\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(x, y):\n",
    "    if type(x).__module__ == 'numpy':\n",
    "        intersection = np.logical_and(x, y)\n",
    "        return 2. * np.sum(intersection) / (np.sum(x) + np.sum(y))\n",
    "    else:\n",
    "        intersection = mx.ndarray.op.broadcast_logical_and(x, y)\n",
    "        return 2. * mx.nd.sum(intersection) / (mx.nd.sum(x) + mx.nd.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visdom_visualize_batch(vis, img, extent, boundary, distance,\n",
    "                           extent_pred, boundary_pred, distance_pred,\n",
    "                           hsv, hsv_pred, mask, title=\"Train images\"):\n",
    "\n",
    "    img, extent, boundary, distance = img.asnumpy(), extent.asnumpy(), boundary.asnumpy(), distance.asnumpy()\n",
    "    extent_pred, boundary_pred = extent_pred.asnumpy(), boundary_pred.asnumpy()\n",
    "    distance_pred, hsv, hsv_pred = distance_pred.asnumpy(), hsv.asnumpy(), hsv_pred.asnumpy()\n",
    "    mask = mask.asnumpy()\n",
    "\n",
    "    # put everything in one window\n",
    "    batch_size, nchannels, nrows, ncols = img.shape\n",
    "    padding = 10\n",
    "    items = [img, hsv, hsv_pred, extent, extent_pred, \n",
    "             boundary, boundary_pred, distance, distance_pred,\n",
    "             mask]\n",
    "    result = np.zeros((3, len(items)*nrows + (len(items)-1)*padding, batch_size*ncols + (batch_size-1)*padding))\n",
    "\n",
    "    for j, item in enumerate(items):\n",
    "\n",
    "        if item.shape[1] == 1:\n",
    "            item = np.tile(item, (1,3,1,1)) * 255.\n",
    "\n",
    "        if j == 1 or j == 2: # convert HSV to RGB\n",
    "            item = np.moveaxis(item, 1, -1) * 255.\n",
    "            for i in range(batch_size):\n",
    "                item[i] = cv2.cvtColor(item[i].astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            item = np.moveaxis(item, -1, 1)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            result[:, j*(nrows+padding):(j+1)*nrows+j*padding, i*(ncols+padding):(i+1)*ncols+i*padding] = item[i]\n",
    "    vis.images(result, nrow=1, win=title, opts={'title': title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, model, tanimoto_dual, trainer, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # training set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(train_dataloader, desc='Training epoch {}'.format(epoch))):\n",
    "        \n",
    "        with autograd.record():\n",
    "\n",
    "            img = img.as_in_context(ctx)\n",
    "            extent = extent.as_in_context(ctx)\n",
    "            boundary = boundary.as_in_context(ctx)\n",
    "            distance = distance.as_in_context(ctx)\n",
    "            hsv = hsv.as_in_context(ctx)\n",
    "            mask = mask.as_in_context(ctx)\n",
    "            nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "            \n",
    "            logits, bound, dist, convc = model(img)\n",
    "            \n",
    "            # multi-task loss\n",
    "            # TODO: wrap this in a custom loss function / class\n",
    "            loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "            loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "            loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "            loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask)) # don't mask hsv\n",
    "\n",
    "            loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "            \n",
    "        loss.backward()\n",
    "        trainer.step(args['batch_size'])\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "#         print(\"logits.shape\", logits.shape)\n",
    "#         print(\"extent.shape\", extent.shape)\n",
    "#         print(\"mask.shape\", mask.shape)\n",
    "            \n",
    "        # mask out unlabeled pixels            \n",
    "#         print(\"made it here -3\")\n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "#         print(\"logits_reshaped.shape\", logits_reshaped.shape)\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "#         print(\"nonmask_idx\", nonmask_idx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "#         print(\"logits_masked\", logits_masked)\n",
    "#         print(\"logits_masked.shape\", logits_masked.shape)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "#         print(\"extent_masked\", extent_masked)\n",
    "#         print(\"extent_masked.shape\", extent_masked.shape)\n",
    "\n",
    "#         print(\"made it here -1\")\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5) # logits_masked[[0],:,:]\n",
    "#         print(\"extent_predicted_classes\", extent_predicted_classes)\n",
    "#         print(\"extent_predicted_classes.shape\", extent_predicted_classes.shape)\n",
    "        \n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 1\")\n",
    "        \n",
    "        # f1 score\n",
    "#         prediction = logits[:,0,:,:].reshape(-1)\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 2\")\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "#         print(\"made it here 3\")\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "#         print(\"made it here 4\")\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dataloader, model, tanimoto_dual, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # validation set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(val_dataloader, desc='Validation epoch {}'.format(epoch))):\n",
    "\n",
    "        img = img.as_in_context(ctx)\n",
    "        extent = extent.as_in_context(ctx)\n",
    "        boundary = boundary.as_in_context(ctx)\n",
    "        distance = distance.as_in_context(ctx)\n",
    "        hsv = hsv.as_in_context(ctx)\n",
    "        mask = mask.as_in_context(ctx)\n",
    "        nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "\n",
    "        logits, bound, dist, convc = model(img)\n",
    "        \n",
    "        # multi-task loss\n",
    "        # TODO: wrap this in a custom loss function / class\n",
    "        loss_extent = mx.nd.sum(1 - tanimoto_dual(logits, extent, mask))\n",
    "        loss_boundary = mx.nd.sum(1 - tanimoto_dual(bound, boundary, mask))\n",
    "        loss_distance = mx.nd.sum(1 - tanimoto_dual(dist, distance, mask))\n",
    "        loss_hsv = mx.nd.sum(1 - tanimoto_dual(convc, hsv, nonmask))\n",
    "\n",
    "        loss = 0.25 * (loss_extent + loss_boundary + loss_distance + loss_hsv)\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        # mask out unlabeled pixels            \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TODO: eccentricity\n",
    "        # TODO: ...\n",
    "        \n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask, title=\"Val images\")\n",
    "        \n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_africa(country, train_names, val_names, test_names, \n",
    "               train_names_label, val_names_label, test_names_label,\n",
    "               trained_model=None,\n",
    "               epochs=100, lr=0.001, lr_decay=None, n_filters=16, batch_size=8,\n",
    "               n_classes=1, model_type='resunet-d6', month='janFebMar',\n",
    "               codes_to_keep=[1, 2],\n",
    "               folder_suffix='',\n",
    "               boundary_kernel_size=3,\n",
    "               ctx_name='cpu',\n",
    "               gpu_id=0):\n",
    "    \n",
    "    # Set MXNet ctx\n",
    "    if ctx_name == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    elif ctx_name == 'gpu':\n",
    "        ctx = mx.gpu(gpu_id)\n",
    "    \n",
    "    # Set up names of directories and paths for saving\n",
    "    if trained_model is None:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if lr_decay:\n",
    "            folder_name = folder_name + '_lrdecay-'+str(lr_decay)\n",
    "            \n",
    "        # define model\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.initialize()\n",
    "        model.hybridize()\n",
    "        model.collect_params().reset_ctx(ctx)\n",
    "        \n",
    "    else:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix+'_finetuned'\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.load_parameters(trained_model, ctx=ctx)\n",
    "        \n",
    "    save_path = os.path.join('../experiments/', country, folder_name)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_model_name = os.path.join(save_path, \"model.params\")\n",
    "    \n",
    "    # Visdom\n",
    "    env_name = country + '_' + folder_name\n",
    "    vis = visdom.Visdom(port=8097, env=env_name)\n",
    "    \n",
    "    # Arguments\n",
    "    args = {}\n",
    "    args['batch_size'] = batch_size\n",
    "    args['ctx_name'] = ctx_name\n",
    "    args['gpu'] = gpu_id\n",
    "    args['visdom'] = vis\n",
    "    args['visdom_every'] = 20\n",
    "\n",
    "    # Define train/val/test splits\n",
    "    train_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='train', \n",
    "        image_names=train_names, \n",
    "        label_names=train_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    val_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='val', \n",
    "        image_names=val_names, \n",
    "        label_names=val_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "    test_dataset = PlanetDatasetWithClassesFullPathsMasked(\n",
    "        fold='test', \n",
    "        image_names=test_names, \n",
    "        label_names=test_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size)\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = gluon.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # define loss function\n",
    "    tanimoto_dual = Tanimoto_with_dual_masked()\n",
    "    if lr_decay:\n",
    "        schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "    else:\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "    # containers for metrics to log\n",
    "    train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                     'train_mcc': [], 'train_dice': []}\n",
    "    val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "                   'val_mcc': [], 'val_dice': []}\n",
    "    best_mcc = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training set\n",
    "        train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n",
    "            train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
    "\n",
    "        # training set metrics\n",
    "        train_loss_avg = train_loss / len(train_dataset)\n",
    "        train_metrics['train_loss'].append(train_loss_avg)\n",
    "        train_metrics['train_acc'].append(train_accuracy.get()[1])\n",
    "        train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "        train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "        train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "\n",
    "        # validation set\n",
    "        val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model(\n",
    "            val_dataloader, model, tanimoto_dual, epoch, args)\n",
    "\n",
    "        # validation set metrics\n",
    "        val_loss_avg = val_loss / len(val_dataset)\n",
    "        val_metrics['val_loss'].append(val_loss_avg)\n",
    "        val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "        val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "        val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "        val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "        print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "\n",
    "        # save model based on best MCC metric\n",
    "        if val_mcc.get()[1] > best_mcc:\n",
    "            model.save_parameters(save_model_name)\n",
    "            best_mcc = val_mcc.get()[1]\n",
    "\n",
    "        # save metrics\n",
    "        metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "        metrics.to_csv(os.path.join(save_path, 'metrics.csv'), index=False)\n",
    "\n",
    "        # visdom\n",
    "        vis.line(Y=np.stack([train_metrics['train_loss'], val_metrics['val_loss']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"Loss\", \n",
    "                 opts=dict(legend=['train loss', 'val loss'], markers=False, title=\"Losses\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "                )\n",
    "        vis.line(Y=np.stack([train_metrics['train_mcc'], val_metrics['val_mcc']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"MCC\", \n",
    "                 opts=dict(legend=['train MCC', 'val MCC'], markers=False, title=\"MCC\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"MCC\")\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ============================ #\n",
    "# # user-specified hyperparameters\n",
    "# # ============================ #\n",
    "# country = 'partial-france'\n",
    "# epochs = 100\n",
    "# lr = 0.001\n",
    "# lr_decay = None\n",
    "# n_filters = 16\n",
    "# batch_size = 8\n",
    "# n_classes = 1\n",
    "# model_type = 'resunet-d6'\n",
    "# month = 'aprJulOctSeparate'\n",
    "# codes_to_keep = [1]\n",
    "# ctx_name = 'gpu'\n",
    "# gpu_id = 0\n",
    "# boundary_kernel_size = (2,2)\n",
    "\n",
    "# # trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "# #     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "# trained_model = None\n",
    "# splits_path = '../data/splits/sherrie10k_planetImagery_splits_20x20_4x-downsampled.csv'\n",
    "# splits_df = pd.read_csv(splits_path, dtype=str)\n",
    "\n",
    "# # get all img and labels\n",
    "# all_img_names = []\n",
    "# all_label_names = []\n",
    "# img_dir = '../data/planet/france/sherrie10k/monthly_mosaics_renamed_clipped_merged/1250px/4x_downsample/'\n",
    "# label_dir = '../data/planet/france/sherrie10k/extent_labels/1250px_1field/4x_downsample/'\n",
    "\n",
    "# label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "# for month in ['2019_04', '2019_07', '2019_10']:\n",
    "#     for label_name in label_folder_imgs:\n",
    "#         img_name = label_name.split('.')[0] + '_' + month + '.tif'\n",
    "#         img_path = os.path.join(img_dir, month, img_name)\n",
    "#         all_img_names.append(img_path)\n",
    "#         label_path = os.path.join(label_dir, label_name)\n",
    "#         all_label_names.append(label_path)\n",
    "\n",
    "# # split imgs and labels into train/val/test\n",
    "# all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "# all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "#     lambda x: x[-1]).str.split('.').apply(\n",
    "#     lambda x: x[0]).str.split('_').apply(\n",
    "#     lambda x: x[0])\n",
    "# all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "# train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "# val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "# test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "# all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "# all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "#     lambda x: x[-1]).str.split('.').apply(\n",
    "#     lambda x: x[0])\n",
    "# all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "# train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "# val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "# test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# # ============================ #\n",
    "\n",
    "# run_africa(country, train_names, val_names, test_names,\n",
    "#            train_names_label, val_names_label, test_names_label,\n",
    "#            trained_model=trained_model,\n",
    "#            epochs=epochs, lr=lr, lr_decay=lr_decay, n_filters=n_filters, batch_size=batch_size,\n",
    "#            n_classes=n_classes, model_type=model_type, month=month,\n",
    "#            codes_to_keep=codes_to_keep, \n",
    "#            ctx_name=ctx_name,\n",
    "#            gpu_id=gpu_id, \n",
    "#            folder_suffix='_4x-downsampled',\n",
    "#            boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 16\n",
      "depth:= 1, nfilters: 32\n",
      "depth:= 2, nfilters: 64\n",
      "depth:= 3, nfilters: 128\n",
      "depth:= 4, nfilters: 256\n",
      "depth:= 5, nfilters: 512\n",
      "depth:= 6, nfilters: 256\n",
      "depth:= 7, nfilters: 128\n",
      "depth:= 8, nfilters: 64\n",
      "depth:= 9, nfilters: 32\n",
      "depth:= 10, nfilters: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Training epoch 1: 100%|██████████| 75/75 [00:47<00:00,  1.59it/s]\n",
      "Validation epoch 1: 100%|██████████| 580/580 [03:33<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Train loss 0.355, accuracy 0.680, F1-score 0.687, MCC: 0.098, Dice: 0.687\n",
      "    Val loss 0.331, accuracy 0.688, F1-score 0.769, MCC: 0.193, Dice: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 75/75 [00:45<00:00,  1.66it/s]\n",
      "Validation epoch 2: 100%|██████████| 580/580 [03:26<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "    Train loss 0.325, accuracy 0.762, F1-score 0.799, MCC: 0.238, Dice: 0.799\n",
      "    Val loss 0.318, accuracy 0.742, F1-score 0.801, MCC: 0.276, Dice: 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 75/75 [00:45<00:00,  1.63it/s]\n",
      "Validation epoch 3: 100%|██████████| 580/580 [03:29<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "    Train loss 0.315, accuracy 0.755, F1-score 0.793, MCC: 0.295, Dice: 0.793\n",
      "    Val loss 0.306, accuracy 0.753, F1-score 0.814, MCC: 0.326, Dice: 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 75/75 [00:54<00:00,  1.39it/s]\n",
      "Validation epoch 4: 100%|██████████| 580/580 [04:30<00:00,  2.15it/s]\n",
      "Training epoch 5:   0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "    Train loss 0.306, accuracy 0.740, F1-score 0.799, MCC: 0.326, Dice: 0.799\n",
      "    Val loss 0.308, accuracy 0.721, F1-score 0.795, MCC: 0.305, Dice: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 75/75 [00:57<00:00,  1.29it/s]\n",
      "Validation epoch 5: 100%|██████████| 580/580 [04:28<00:00,  2.16it/s]\n",
      "Training epoch 6:   0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "    Train loss 0.302, accuracy 0.725, F1-score 0.782, MCC: 0.328, Dice: 0.782\n",
      "    Val loss 0.303, accuracy 0.721, F1-score 0.808, MCC: 0.325, Dice: 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 6: 100%|██████████| 75/75 [01:03<00:00,  1.19it/s]\n",
      "Validation epoch 6: 100%|██████████| 580/580 [04:19<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "    Train loss 0.295, accuracy 0.735, F1-score 0.805, MCC: 0.366, Dice: 0.805\n",
      "    Val loss 0.304, accuracy 0.648, F1-score 0.745, MCC: 0.334, Dice: 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 7: 100%|██████████| 75/75 [01:01<00:00,  1.22it/s]\n",
      "Validation epoch 7: 100%|██████████| 580/580 [04:27<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "    Train loss 0.291, accuracy 0.725, F1-score 0.799, MCC: 0.357, Dice: 0.799\n",
      "    Val loss 0.294, accuracy 0.753, F1-score 0.819, MCC: 0.363, Dice: 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation epoch 15: 100%|██████████| 580/580 [04:30<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "    Train loss 0.280, accuracy 0.725, F1-score 0.798, MCC: 0.363, Dice: 0.798\n",
      "    Val loss 0.286, accuracy 0.727, F1-score 0.794, MCC: 0.367, Dice: 0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 16: 100%|██████████| 75/75 [00:58<00:00,  1.29it/s]\n",
      "Validation epoch 60: 100%|██████████| 580/580 [04:12<00:00,  2.30it/s]\n",
      "Training epoch 61:   0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60:\n",
      "    Train loss 0.248, accuracy 0.757, F1-score 0.817, MCC: 0.408, Dice: 0.817\n",
      "    Val loss 0.293, accuracy 0.669, F1-score 0.754, MCC: 0.342, Dice: 0.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 61: 100%|██████████| 75/75 [00:52<00:00,  1.44it/s]\n",
      "Training epoch 98: 100%|██████████| 75/75 [00:49<00:00,  1.51it/s]/s]\n",
      "Validation epoch 98: 100%|██████████| 580/580 [03:48<00:00,  2.54it/s]\n",
      "Training epoch 99:   0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98:\n",
      "    Train loss 0.229, accuracy 0.773, F1-score 0.829, MCC: 0.449, Dice: 0.829\n",
      "    Val loss 0.293, accuracy 0.727, F1-score 0.802, MCC: 0.357, Dice: 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 99: 100%|██████████| 75/75 [00:57<00:00,  1.29it/s]\n",
      "Validation epoch 99: 100%|██████████| 580/580 [03:51<00:00,  2.50it/s]\n",
      "Training epoch 100:   0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99:\n",
      "    Train loss 0.229, accuracy 0.772, F1-score 0.832, MCC: 0.443, Dice: 0.832\n",
      "    Val loss 0.293, accuracy 0.661, F1-score 0.763, MCC: 0.327, Dice: 0.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 100: 100%|██████████| 75/75 [00:56<00:00,  1.34it/s]\n",
      "Validation epoch 100: 100%|██████████| 580/580 [03:48<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100:\n",
      "    Train loss 0.231, accuracy 0.771, F1-score 0.835, MCC: 0.450, Dice: 0.835\n",
      "    Val loss 0.293, accuracy 0.738, F1-score 0.812, MCC: 0.360, Dice: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'partial-france'\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "lr_decay = None\n",
    "n_filters = 16\n",
    "batch_size = 8\n",
    "n_classes = 1\n",
    "model_type = 'resunet-d6'\n",
    "month = 'aprJulOctSeparate'\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 2\n",
    "boundary_kernel_size = (2,2)\n",
    "\n",
    "# trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "#     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "trained_model = None\n",
    "splits_path = '../data/splits/sherrie10k_planetImagery_splits_20x20_4x-downsampled_n200.csv'\n",
    "splits_df = pd.read_csv(splits_path)\n",
    "splits_df['image_id'] = splits_df['image_id'].astype(str).str.zfill(5)\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "img_dir = '../data/planet/france/sherrie10k/monthly_mosaics_renamed_clipped_merged/1250px/4x_downsample/'\n",
    "label_dir = '../data/planet/france/sherrie10k/extent_labels/1250px_1field/4x_downsample/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for month in ['2019_04', '2019_07', '2019_10']:\n",
    "    for label_name in label_folder_imgs:\n",
    "        img_name = label_name.split('.')[0] + '_' + month + '.tif'\n",
    "        img_path = os.path.join(img_dir, month, img_name)\n",
    "        all_img_names.append(img_path)\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        all_label_names.append(label_path)\n",
    "\n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'img_path': all_img_names})\n",
    "all_images['image_id'] = all_images['img_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0]).str.split('_').apply(\n",
    "    lambda x: x[0])\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['img_path'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['img_path'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['img_path'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, n_filters=n_filters, batch_size=batch_size,\n",
    "           n_classes=n_classes, model_type=model_type, month=month,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix='_4x-downsampled_1field_n200',\n",
    "           boundary_kernel_size=boundary_kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "mask_j = mx.nd.array([[[1,0,0],\n",
    "                       [0,1,0]],\n",
    "                      [[0,0,1],\n",
    "                       [0,0,0]]])\n",
    "mask_j = mask_j.reshape((mask_j.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. 0. 1. 0.]\n",
       " [0. 0. 1. 0. 0. 0.]]\n",
       "<NDArray 2x6 @cpu(0)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonmask_idx = mx.np.nonzero(mask_j.as_np_ndarray())\n",
    "nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 1. 1.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.nd.gather_nd(mask_j, nonmask_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.5097876]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanimoto = Tanimoto_with_dual_masked()\n",
    "test_pred = mx.nd.array([[[[0.1,0.1,0.8,0.9]]]])\n",
    "test_label = mx.nd.array([[[[0,0,1,0]]]])\n",
    "test_mask = mx.nd.array([[[[0,1,1,1]]]])\n",
    "tanimoto(test_pred, test_label, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0. 1. 0.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[0. 0. 0.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 1. 0. 0. 0. 0.]\n",
       "<NDArray 6 @cpu(0)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row in mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]):\n",
    "    print(row)\n",
    "    \n",
    "mx.nd.array([[0,1,0],\n",
    "             [0,0,0]]).reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.npx.set_np()\n",
    "mx.np.nonzero(mx.nd.array([[0,1,0],\n",
    "                           [0,0,0]]).as_np_ndarray())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = mx.nd.array([[ 0.1, 0.9, 0.5],\n",
    "                          [ 0.2, 0.3, 0.7]])\n",
    "test_array2 = mx.nd.array([[0, 2, 0],\n",
    "                           [0, 1, 0]])\n",
    "nonmask_idx = mx.np.nonzero(test_array2.as_np_ndarray())\n",
    "test_result = mx.nd.gather_nd(test_array, mx.nd.array(nonmask_idx))\n",
    "test_result2 = mx.nd.gather_nd(test_array2, mx.nd.array(nonmask_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.9 0.3]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[1. 0.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result2 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet1.6.0)",
   "language": "python",
   "name": "conda_mxnet1.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
