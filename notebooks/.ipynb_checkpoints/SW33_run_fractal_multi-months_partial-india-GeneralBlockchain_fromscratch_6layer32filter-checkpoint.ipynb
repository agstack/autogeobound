{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import visdom\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../resuneta/src')\n",
    "sys.path.append('../../decode/FracTAL_ResUNet/models/semanticsegmentation')\n",
    "sys.path.append('../../decode/FracTAL_ResUNet/nn/loss')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../MXNet-ResUNeta/')\n",
    "\n",
    "from bound_dist import get_distance, get_boundary\n",
    "from FracTAL_ResUNet import FracTAL_ResUNet_cmtsk\n",
    "from ftnmt_loss import ftnmt_loss_masked\n",
    "from datasets import *\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(x, y):\n",
    "    if type(x).__module__ == 'numpy':\n",
    "        intersection = np.logical_and(x, y)\n",
    "        return 2. * np.sum(intersection) / (np.sum(x) + np.sum(y))\n",
    "    else:\n",
    "        intersection = mx.ndarray.op.broadcast_logical_and(x, y)\n",
    "        return 2. * mx.nd.sum(intersection) / (mx.nd.sum(x) + mx.nd.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visdom_visualize_batch(vis, img, extent, boundary, distance,\n",
    "                           extent_pred, boundary_pred, distance_pred,\n",
    "                           hsv, hsv_pred, mask, title=\"Train images\"):\n",
    "\n",
    "    img, extent, boundary, distance = img.asnumpy(), extent.asnumpy(), boundary.asnumpy(), distance.asnumpy()\n",
    "    extent_pred, boundary_pred = extent_pred.asnumpy(), boundary_pred.asnumpy()\n",
    "    distance_pred, hsv, hsv_pred = distance_pred.asnumpy(), hsv.asnumpy(), hsv_pred.asnumpy()\n",
    "    mask = mask.asnumpy()\n",
    "\n",
    "    # put everything in one window\n",
    "    batch_size, nchannels, nrows, ncols = img.shape\n",
    "    imgs = []\n",
    "    for i in range(nchannels // 3):\n",
    "        imgs.append(img[:,i*3:(i+1)*3])\n",
    "    padding = 10\n",
    "    items = imgs + [extent, extent_pred, \n",
    "             boundary, boundary_pred, distance, distance_pred,\n",
    "             mask]\n",
    "    result = np.zeros((3, len(items)*nrows + (len(items)-1)*padding, batch_size*ncols + (batch_size-1)*padding))\n",
    "\n",
    "    for j, item in enumerate(items):\n",
    "\n",
    "        if item.shape[1] == 1:\n",
    "            item = np.tile(item, (1,3,1,1)) * 255.\n",
    "\n",
    "#         if j == len(items) - 8 or j == len(items) - 9: # convert HSV to RGB\n",
    "#             item = np.moveaxis(item, 1, -1) * 255.\n",
    "#             for i in range(batch_size):\n",
    "#                 item[i] = cv2.cvtColor(item[i].astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "#             item = np.moveaxis(item, -1, 1)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            result[:, j*(nrows+padding):(j+1)*nrows+j*padding, i*(ncols+padding):(i+1)*ncols+i*padding] = item[i]\n",
    "    vis.images(result, nrow=1, win=title, opts={'title': title})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, model, tanimoto_dual, trainer, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # training set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(train_dataloader, desc='Training epoch {}'.format(epoch))):\n",
    "#         print(\"batch {}\".format(batch_i))\n",
    "        \n",
    "        with autograd.record():\n",
    "\n",
    "            img = img.as_in_context(ctx)\n",
    "            extent = extent.as_in_context(ctx)\n",
    "            boundary = boundary.as_in_context(ctx)\n",
    "            distance = distance.as_in_context(ctx)\n",
    "            hsv = hsv.as_in_context(ctx)\n",
    "            mask = mask.as_in_context(ctx)\n",
    "            nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "            \n",
    "            # logits, bound, dist, convc = model(img)\n",
    "            logits, bound, dist = model(img)\n",
    "            \n",
    "            # multi-task loss\n",
    "            # TODO: wrap this in a custom loss function / class\n",
    "            loss_extent = mx.nd.sum(tanimoto_dual(logits, extent, mask))\n",
    "            loss_boundary = mx.nd.sum(tanimoto_dual(bound, boundary, mask))\n",
    "            loss_distance = mx.nd.sum(tanimoto_dual(dist, distance, mask))\n",
    "\n",
    "            loss = 0.33 * (loss_extent + loss_boundary + loss_distance) # + loss_hsv)\n",
    "            \n",
    "#         print(\"backprop\")\n",
    "        loss.backward()\n",
    "        trainer.step(args['batch_size'])\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "#         print(\"computing metrics\")\n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TEMPORARY to make visdom work\n",
    "#         print(\"visdom\")\n",
    "        convc = hsv\n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dataloader, model, tanimoto_dual, epoch, args):\n",
    "    \n",
    "    # initialize metrics\n",
    "    cumulative_loss = 0\n",
    "    accuracy = mx.metric.Accuracy()\n",
    "    f1 = mx.metric.F1()\n",
    "    mcc = mx.metric.MCC()\n",
    "    dice = mx.metric.CustomMetric(feval=dice_coef, name=\"Dice\")\n",
    "    if args['ctx_name'] == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    else:\n",
    "        ctx = mx.gpu(args['gpu'])\n",
    "    \n",
    "    # validation set\n",
    "    for batch_i, (img, extent, boundary, distance, hsv, mask) in enumerate(\n",
    "        tqdm(val_dataloader, desc='Validation epoch {}'.format(epoch))):\n",
    "\n",
    "        img = img.as_in_context(ctx)\n",
    "        extent = extent.as_in_context(ctx)\n",
    "        boundary = boundary.as_in_context(ctx)\n",
    "        distance = distance.as_in_context(ctx)\n",
    "        hsv = hsv.as_in_context(ctx)\n",
    "        mask = mask.as_in_context(ctx)\n",
    "        nonmask = mx.nd.ones(extent.shape).as_in_context(ctx)\n",
    "\n",
    "        # logits, bound, dist, convc = model(img)\n",
    "        logits, bound, dist = model(img)\n",
    "        \n",
    "        # multi-task loss\n",
    "        # TODO: wrap this in a custom loss function / class\n",
    "        loss_extent = mx.nd.sum(tanimoto_dual(logits, extent, mask))\n",
    "        loss_boundary = mx.nd.sum(tanimoto_dual(bound, boundary, mask))\n",
    "        loss_distance = mx.nd.sum(tanimoto_dual(dist, distance, mask))\n",
    "\n",
    "        loss = 0.33 * (loss_extent + loss_boundary + loss_distance) # + loss_hsv)\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        cumulative_loss += mx.nd.sum(loss).asscalar()\n",
    "        \n",
    "        # update metrics based on every batch\n",
    "        # mask out unlabeled pixels            \n",
    "        logits_reshaped = logits.reshape((logits.shape[0], -1))\n",
    "        extent_reshaped = extent.reshape((extent.shape[0], -1))\n",
    "        mask_reshaped = mask.reshape((mask.shape[0], -1))\n",
    "        \n",
    "        nonmask_idx = mx.np.nonzero(mask_reshaped.as_np_ndarray())\n",
    "        nonmask_idx = mx.np.stack(nonmask_idx).as_nd_ndarray().as_in_context(ctx)\n",
    "        logits_masked = mx.nd.gather_nd(logits_reshaped, nonmask_idx)\n",
    "        extent_masked = mx.nd.gather_nd(extent_reshaped, nonmask_idx)\n",
    "\n",
    "        # accuracy\n",
    "        extent_predicted_classes = mx.nd.ceil(logits_masked - 0.5)\n",
    "        accuracy.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # f1 score\n",
    "        probabilities = mx.nd.stack(1 - logits_masked, logits_masked, axis=1)\n",
    "        f1.update(extent_masked, probabilities)\n",
    "        \n",
    "        # MCC metric\n",
    "        mcc.update(extent_masked, probabilities)\n",
    "        \n",
    "        # Dice score\n",
    "        dice.update(extent_masked, extent_predicted_classes)\n",
    "        \n",
    "        # TEMPORARY to make visdom work\n",
    "        convc = hsv\n",
    "        if batch_i % args['visdom_every'] == 0:\n",
    "            visdom_visualize_batch(args['visdom'], img, extent, boundary, distance,\n",
    "                                   logits, bound, dist, hsv, convc, mask, title=\"Val images\")\n",
    "        \n",
    "    return cumulative_loss, accuracy, f1, mcc, dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Africa datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_africa(country, train_names, val_names, test_names, \n",
    "               train_names_label, val_names_label, test_names_label,\n",
    "               image_directories,\n",
    "               trained_model=None,\n",
    "               epochs=100, lr=0.001, lr_decay=None, \n",
    "               model_type='resunet-d6',\n",
    "               n_filters=16, batch_size=8,\n",
    "               depth=5, n_classes=1, \n",
    "               month='janFebMar',\n",
    "               codes_to_keep=[1, 2],\n",
    "               folder_suffix='',\n",
    "               boundary_kernel_size=3,\n",
    "               n_channels=3,\n",
    "               image_suffixes=None,\n",
    "               shuffle_directories=False,\n",
    "               ctx_name='cpu',\n",
    "               gpu_id=0):\n",
    "    \n",
    "    # Set MXNet ctx\n",
    "    if ctx_name == 'cpu':\n",
    "        ctx = mx.cpu()\n",
    "    elif ctx_name == 'gpu':\n",
    "        ctx = mx.gpu(gpu_id)\n",
    "    \n",
    "    # Set up names of directories and paths for saving\n",
    "    if trained_model is None:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_depth-'+str(depth)+'_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix\n",
    "        if lr_decay:\n",
    "            folder_name = folder_name + '_lrdecay-'+str(lr_decay)\n",
    "            \n",
    "        # define model\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'fractal-resunet':\n",
    "            model = FracTAL_ResUNet_cmtsk(nfilters_init=n_filters, depth=depth, NClasses=n_classes)\n",
    "        model.initialize()\n",
    "        model.hybridize()\n",
    "        model.collect_params().reset_ctx(ctx)\n",
    "        \n",
    "    else:\n",
    "        folder_name = model_type+'_'+month+'_nfilter-'+str(n_filters)+ \\\n",
    "                      '_bs-'+str(batch_size)+'_lr-'+str(lr)+folder_suffix+'_finetuned'\n",
    "        if model_type == 'resunet-d6':\n",
    "            model = ResUNet_d6(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        elif model_type == 'resunet-d7':\n",
    "            model = ResUNet_d7(_nfilters_init=n_filters, _NClasses=n_classes)\n",
    "        model.load_parameters(trained_model, ctx=ctx)\n",
    "        \n",
    "    save_path = os.path.join('../experiments/', country, folder_name)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    save_model_name = os.path.join(save_path, \"model.params\")\n",
    "    \n",
    "    # Visdom\n",
    "    env_name = country + '_' + folder_name\n",
    "    vis = visdom.Visdom(port=8097, env=env_name)\n",
    "    \n",
    "    # Arguments\n",
    "    args = {}\n",
    "    args['batch_size'] = batch_size\n",
    "    args['ctx_name'] = ctx_name\n",
    "    args['gpu'] = gpu_id\n",
    "    args['visdom'] = vis\n",
    "    args['visdom_every'] = 20\n",
    "\n",
    "    # Define train/val/test splits\n",
    "    train_dataset = MultitempMasked(\n",
    "        image_directories=image_directories,\n",
    "        fold='train',\n",
    "        image_names=train_names, \n",
    "        label_names=train_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size,\n",
    "        n_channels=n_channels,\n",
    "        image_suffixes=image_suffixes,\n",
    "        shuffle_directories=shuffle_directories)\n",
    "    val_dataset = MultitempMasked(\n",
    "        image_directories=image_directories,\n",
    "        fold='val', \n",
    "        image_names=val_names, \n",
    "        label_names=val_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size,\n",
    "        n_channels=n_channels,\n",
    "        image_suffixes=image_suffixes)\n",
    "    test_dataset = MultitempMasked(\n",
    "        image_directories=image_directories,\n",
    "        fold='test', \n",
    "        image_names=test_names, \n",
    "        label_names=test_names_label, \n",
    "        classes=codes_to_keep,\n",
    "        boundary_kernel_size=boundary_kernel_size,\n",
    "        n_channels=n_channels,\n",
    "        image_suffixes=image_suffixes)\n",
    "\n",
    "    train_dataloader = gluon.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = gluon.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = gluon.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # define loss function\n",
    "    tanimoto_dual = ftnmt_loss_masked(depth=0) # Tanimoto_with_dual_masked()\n",
    "    if lr_decay:\n",
    "        schedule = mx.lr_scheduler.FactorScheduler(step=1, factor=lr_decay)\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr, lr_scheduler=schedule)\n",
    "    else:\n",
    "        adam_optimizer = mx.optimizer.Adam(learning_rate=lr)\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer=adam_optimizer)\n",
    "\n",
    "    # containers for metrics to log\n",
    "    train_metrics = {'train_loss': [], 'train_acc': [], 'train_f1': [], \n",
    "                     'train_mcc': [], 'train_dice': []}\n",
    "    val_metrics = {'val_loss': [], 'val_acc': [], 'val_f1': [], \n",
    "                   'val_mcc': [], 'val_dice': []}\n",
    "    best_mcc = 0.0\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # training set\n",
    "        train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n",
    "            train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
    "\n",
    "        # training set metrics\n",
    "        train_loss_avg = train_loss / len(train_dataset)\n",
    "        train_metrics['train_loss'].append(train_loss_avg)\n",
    "        train_metrics['train_acc'].append(train_accuracy.get()[1])\n",
    "        train_metrics['train_f1'].append(train_f1.get()[1])\n",
    "        train_metrics['train_mcc'].append(train_mcc.get()[1])\n",
    "        train_metrics['train_dice'].append(train_dice.get()[1])\n",
    "\n",
    "        # validation set\n",
    "        val_loss, val_accuracy, val_f1, val_mcc, val_dice = evaluate_model(\n",
    "            val_dataloader, model, tanimoto_dual, epoch, args)\n",
    "\n",
    "        # validation set metrics\n",
    "        val_loss_avg = val_loss / len(val_dataset)\n",
    "        val_metrics['val_loss'].append(val_loss_avg)\n",
    "        val_metrics['val_acc'].append(val_accuracy.get()[1])\n",
    "        val_metrics['val_f1'].append(val_f1.get()[1])\n",
    "        val_metrics['val_mcc'].append(val_mcc.get()[1])\n",
    "        val_metrics['val_dice'].append(val_dice.get()[1])\n",
    "\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"    Train loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            train_loss_avg, train_accuracy.get()[1], train_f1.get()[1], train_mcc.get()[1], train_dice.get()[1]))\n",
    "        print(\"    Val loss {:0.3f}, accuracy {:0.3f}, F1-score {:0.3f}, MCC: {:0.3f}, Dice: {:0.3f}\".format(\n",
    "            val_loss_avg, val_accuracy.get()[1], val_f1.get()[1], val_mcc.get()[1], val_dice.get()[1]))\n",
    "\n",
    "        # save model based on best MCC metric\n",
    "        if val_mcc.get()[1] > best_mcc:\n",
    "            model.save_parameters(save_model_name)\n",
    "            best_mcc = val_mcc.get()[1]\n",
    "\n",
    "        # save metrics\n",
    "        metrics = pd.concat([pd.DataFrame(train_metrics), pd.DataFrame(val_metrics)], axis=1)\n",
    "        metrics.to_csv(os.path.join(save_path, 'metrics.csv'), index=False)\n",
    "\n",
    "        # visdom\n",
    "        vis.line(Y=np.stack([train_metrics['train_loss'], val_metrics['val_loss']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"Loss\", \n",
    "                 opts=dict(legend=['train loss', 'val loss'], markers=False, title=\"Losses\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "                )\n",
    "        vis.line(Y=np.stack([train_metrics['train_mcc'], val_metrics['val_mcc']], axis=1), \n",
    "                 X=np.arange(1, epoch+1), win=\"MCC\", \n",
    "                 opts=dict(legend=['train MCC', 'val MCC'], markers=False, title=\"MCC\",\n",
    "                           xlabel=\"Epoch\", ylabel=\"MCC\")\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:= 0, nfilters: 32, nheads::8, widths::1\n",
      "depth:= 1, nfilters: 64, nheads::16, widths::1\n",
      "depth:= 2, nfilters: 128, nheads::32, widths::1\n",
      "depth:= 3, nfilters: 256, nheads::64, widths::1\n",
      "depth:= 4, nfilters: 512, nheads::128, widths::1\n",
      "depth:= 5, nfilters: 1024, nheads::256, widths::1\n",
      "depth:= 6, nfilters: 512, nheads::256, widths::1\n",
      "depth:= 7, nfilters: 256, nheads::128, widths::1\n",
      "depth:= 8, nfilters: 128, nheads::64, widths::1\n",
      "depth:= 9, nfilters: 64, nheads::32, widths::1\n",
      "depth:= 10, nfilters: 32, nheads::16, widths::1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Training epoch 1:  44%|████▍     | 6453/14677 [2:56:37<3:37:50,  1.59s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 1:  74%|███████▍  | 10913/14677 [4:57:24<1:39:27,  1.59s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 1: 100%|██████████| 14677/14677 [6:39:18<00:00,  1.63s/it]  \n",
      "Validation epoch 1:  19%|█▉        | 692/3562 [08:02<31:08,  1.54it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Validation epoch 1: 100%|██████████| 3562/3562 [41:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "    Train loss 0.383, accuracy 0.751, F1-score 0.795, MCC: 0.305, Dice: 0.795\n",
      "    Val loss 0.347, accuracy 0.789, F1-score 0.813, MCC: 0.351, Dice: 0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2:   4%|▍         | 590/14677 [16:10<6:09:50,  1.58s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training epoch 2: 100%|██████████| 14677/14677 [6:37:53<00:00,  1.63s/it]  \n",
      "Validation epoch 2: 100%|██████████| 3562/3562 [41:36<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "    Train loss 0.355, accuracy 0.777, F1-score 0.808, MCC: 0.377, Dice: 0.808\n",
      "    Val loss 0.336, accuracy 0.793, F1-score 0.808, MCC: 0.375, Dice: 0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3:   1%|▏         | 201/14677 [05:38<6:46:18,  1.68s/it]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-49a059f0388e>\", line 89, in <module>\n",
      "    shuffle_directories=shuffle_directories)\n",
      "  File \"<ipython-input-6-57cfe25d4204>\", line 124, in run_africa\n",
      "    train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
      "  File \"<ipython-input-4-192e81753948>\", line 41, in train_model\n",
      "    loss.backward()\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\", line 2839, in backward\n",
      "    ctypes.c_void_p(0)))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-49a059f0388e>\", line 89, in <module>\n",
      "    shuffle_directories=shuffle_directories)\n",
      "  File \"<ipython-input-6-57cfe25d4204>\", line 124, in run_africa\n",
      "    train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n",
      "  File \"<ipython-input-4-192e81753948>\", line 41, in train_model\n",
      "    loss.backward()\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\", line 2839, in backward\n",
      "    ctypes.c_void_p(0)))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/swang222/anaconda3/envs/mxnet1.6.0/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-49a059f0388e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m            \u001b[0mimage_suffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_suffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m            shuffle_directories=shuffle_directories)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-57cfe25d4204>\u001b[0m in \u001b[0;36mrun_africa\u001b[0;34m(country, train_names, val_names, test_names, train_names_label, val_names_label, test_names_label, image_directories, trained_model, epochs, lr, lr_decay, model_type, n_filters, batch_size, depth, n_classes, month, codes_to_keep, folder_suffix, boundary_kernel_size, n_channels, image_suffixes, shuffle_directories, ctx_name, gpu_id)\u001b[0m\n\u001b[1;32m    123\u001b[0m         train_loss, train_accuracy, train_f1, train_mcc, train_dice = train_model(\n\u001b[0;32m--> 124\u001b[0;31m             train_dataloader, model, tanimoto_dual, trainer, epoch, args)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-192e81753948>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, tanimoto_dual, trainer, epoch, args)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#         print(\"backprop\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, out_grad, retain_graph, train_mode)\u001b[0m\n\u001b[1;32m   2838\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2839\u001b[0;31m             ctypes.c_void_p(0)))\n\u001b[0m\u001b[1;32m   2840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3262\u001b[0m                         \u001b[0masy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3263\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0masync_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3264\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TypeError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3071\u001b[0m                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n\u001b[0;32m-> 3072\u001b[0;31m                        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[0m\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_execution_succeeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_before_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n\u001b[0;32m-> 1211\u001b[0;31m                                                                      chained_exceptions_tb_offset)\n\u001b[0m\u001b[1;32m   1212\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet1.6.0/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# ============================ #\n",
    "# user-specified hyperparameters\n",
    "# ============================ #\n",
    "country = 'india'\n",
    "months = ['2020_08', '2020_11', '2021_02']\n",
    "image_directories = ['../data/planet/india/GeneralBlockchain/monthly_mosaics_renamed_clipped_merged_samples/' \n",
    "                     + month for month in months]\n",
    "image_suffixes = ['_' + x + '.tif' for x in months]\n",
    "shuffle_directories = False\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "lr_decay = None\n",
    "n_filters = 32\n",
    "depth = 6\n",
    "n_classes = 1\n",
    "batch_size = 3\n",
    "model_type = 'fractal-resunet' # 'resunet-d6'\n",
    "month = 'all13'\n",
    "codes_to_keep = [1]\n",
    "ctx_name = 'gpu'\n",
    "gpu_id = 0\n",
    "boundary_kernel_size = (2,2)\n",
    "n_channels = 3\n",
    "\n",
    "# trained_model = '../experiments/france/sherrie10k/' + \\\n",
    "#     'resunet-d6_2019_10_class-notreeexceptvines_nfilter-16_bs-8_lr-0.001_1x-8x-downsampled/model.params'\n",
    "trained_model = None\n",
    "\n",
    "if trained_model is None:\n",
    "    folder_suffix = '_fromscratch'\n",
    "else:\n",
    "    folder_suffix = '_finetuned'\n",
    "    \n",
    "if country == 'india':\n",
    "    months = ['2020_{}'.format(str(x).zfill(2)) for x in range(7, 13)] + \\\n",
    "             ['2021_{}'.format(str(x).zfill(2)) for x in range(1, 8)]\n",
    "    month_name = '3month-stacked'\n",
    "    splits_path = '../data/splits/india_planetImagery_splits_20x20.csv'\n",
    "splits_df = pd.read_csv(splits_path)\n",
    "splits_df['image_id'] = splits_df['image_id'].astype(str).str.zfill(5)\n",
    "\n",
    "# get all img and labels\n",
    "all_img_names = []\n",
    "all_label_names = []\n",
    "if country == 'india':\n",
    "    img_dir = '../data/planet/india/GeneralBlockchain/monthly_mosaics_renamed_clipped_merged_samples/'\n",
    "    label_dir = '../data/planet/india/GeneralBlockchain/extent_labels/'\n",
    "\n",
    "label_folder_imgs = sorted(os.listdir(label_dir))\n",
    "for month in months:\n",
    "    for label_name in label_folder_imgs:\n",
    "        img_name = label_name.split('.')[0]\n",
    "        all_img_names.append(img_name)\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        all_label_names.append(label_path)\n",
    "\n",
    "# split imgs and labels into train/val/test\n",
    "all_images = pd.DataFrame({'image_id': all_img_names})\n",
    "all_images = all_images.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names = all_images[all_images['fold'] == 'train']['image_id'].values\n",
    "val_names = all_images[all_images['fold'] == 'val']['image_id'].values\n",
    "test_names = all_images[all_images['fold'] == 'test']['image_id'].values\n",
    "\n",
    "all_labels = pd.DataFrame({'label_path': all_label_names})\n",
    "all_labels['image_id'] = all_labels['label_path'].str.split('/').apply(\n",
    "    lambda x: x[-1]).str.split('.').apply(\n",
    "    lambda x: x[0])\n",
    "all_labels = all_labels.merge(splits_df[['image_id', 'fold']], on='image_id', how='left')\n",
    "train_names_label = all_labels[all_labels['fold'] == 'train']['label_path'].values\n",
    "val_names_label = all_labels[all_labels['fold'] == 'val']['label_path'].values\n",
    "test_names_label = all_labels[all_labels['fold'] == 'test']['label_path'].values\n",
    "\n",
    "# ============================ #\n",
    "\n",
    "run_africa(country, train_names, val_names, test_names,\n",
    "           train_names_label, val_names_label, test_names_label,\n",
    "           image_directories,\n",
    "           trained_model=trained_model,\n",
    "           epochs=epochs, lr=lr, lr_decay=lr_decay, \n",
    "           model_type=model_type, n_filters=n_filters, depth=depth, n_classes=n_classes,\n",
    "           batch_size=batch_size, month=month_name,\n",
    "           codes_to_keep=codes_to_keep, \n",
    "           ctx_name=ctx_name,\n",
    "           gpu_id=gpu_id, \n",
    "           folder_suffix=folder_suffix,\n",
    "           boundary_kernel_size=boundary_kernel_size,\n",
    "           n_channels=n_channels,\n",
    "           image_suffixes=image_suffixes,\n",
    "           shuffle_directories=shuffle_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet1.6.0)",
   "language": "python",
   "name": "conda_mxnet1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
