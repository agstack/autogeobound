{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import shapefile\n",
    "import json\n",
    "from json import dumps\n",
    "import fiona\n",
    "from pyproj import Proj#, transform\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import shape\n",
    "from functools import partial\n",
    "from shapely.ops import transform\n",
    "from shapely.strtree import STRtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally shp2geo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(shape_file, readCSV):\n",
    "    \"\"\"Read the coordinate of the bounding boxes and constructs and R-Tree data structure\n",
    "\n",
    "    Args:\n",
    "      shape_file : polygons\n",
    "      readCSV: pandas dataframe containing bounding boxes\n",
    "\n",
    "    Returns:\n",
    "    dict, r-tree: dict of bounding boxes for each image id and r-tree\n",
    "    \"\"\"\n",
    "    shapes = fiona.open(shape_file)\n",
    "    if len(shapes.crs) != 0:\n",
    "        destination = Proj(shapes.crs)\n",
    "    else:\n",
    "        destination = Proj('+init=EPSG:4326')\n",
    "    # original = Proj('+init=EPSG:4326')\n",
    "    original = Proj('+init=EPSG:3857')\n",
    "\n",
    "    grid = dict()\n",
    "    keys = ['max_lat', 'max_lon', 'min_lat', 'min_lon']\n",
    "    poly_list = []\n",
    "    \n",
    "    for index, row in readCSV.iterrows():\n",
    "        if index not in grid:\n",
    "            grid[index] = dict()\n",
    "        grid[index]['image_id'] = row['image_id']\n",
    "        grid[index]['max_lat'] = float(row['max_lat'])\n",
    "        grid[index]['max_lon'] = float(row['max_lon'])\n",
    "        grid[index]['min_lat'] = float(row['min_lat'])\n",
    "        grid[index]['min_lon'] = float(row['min_lon'])\n",
    "\n",
    "        grid[index]['poly'] = shapely.geometry.box(\n",
    "            grid[index]['min_lon'], grid[index]['min_lat'], grid[index]['max_lon'], grid[index]['max_lat'])\n",
    "        \n",
    "        # project boxes from WSG 84 to parcel projection\n",
    "        project = partial(pyproj.transform, original, destination)\n",
    "        grid[index]['poly'] = transform(project, grid[index]['poly'])\n",
    "\n",
    "        # populating r-tree\n",
    "        poly_obj = grid[index]['poly']\n",
    "        poly_obj.name = grid[index]['image_id'] # useful for retrival in search phase\n",
    "        poly_list.append(poly_obj)\n",
    "        \n",
    "    tree = STRtree(poly_list) # constructing R-Tree\n",
    "    return grid, tree\n",
    "\n",
    "def listit(t):\n",
    "    # convert to appropriate list type \n",
    "    return list(map(listit, t)) if isinstance(t, (list, tuple)) else t\n",
    "\n",
    "\n",
    "def check_polygon_in_bounds(poly, tree):\n",
    "    \"\"\"\n",
    "    find image corrspinding to the existance of a field in the list of \n",
    "    image bounding boxes\n",
    "\n",
    "    Args:\n",
    "      poly (polygon): field\n",
    "      tree (r-tree): r-tree of images\n",
    "\n",
    "    Returns:\n",
    "      List: List of intersecting images with a field\n",
    "    \"\"\"\n",
    "    results = tree.query(poly)\n",
    "    return results\n",
    "\n",
    "\n",
    "def field_imageId_list(polys, count_parcels):\n",
    "    \"\"\"\n",
    "    extract name of the intersecting polygons\n",
    "\n",
    "    Args:\n",
    "      polys (polygons): intersecting fields\n",
    "      count_parcels (dict): the sanity check summary of # of fields in image ids  \n",
    "    Returns:\n",
    "      list: list of the image ids\n",
    "    \"\"\"\n",
    "    list_image_ids = []\n",
    "    for element in polys:\n",
    "        list_image_ids.append(element.name)\n",
    "        count_parcels[element.name] += 1\n",
    "    return list_image_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find intersecting polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_shp_to_json(shape_file, grid, tree, output_json='../data/planet/france/sherrie10k/test_json'):\n",
    "    \"\"\"\n",
    "    find intersecting polygons in the list of available images and save the GeoJSON\n",
    "\n",
    "    Args:\n",
    "      shape_file (polygons): fields\n",
    "      grid (dict): image bounding boxes \n",
    "      tree (r-tre): r-tree of images\n",
    "      output_json (str): output path of json file\n",
    "    \"\"\"\n",
    "    # coordinate transformation\n",
    "    reader = shapefile.Reader(shape_file)\n",
    "    shapes = fiona.open(shape_file)\n",
    "    if len(shapes.crs) != 0:\n",
    "        original = Proj(shapes.crs)\n",
    "    else:\n",
    "        original = Proj('+init=EPSG:4326')\n",
    "#     print(fiona.open(shape_file).crs)\n",
    "\n",
    "    # list of properties of features\n",
    "    fields = reader.fields[1:]\n",
    "    field_names = [field[0] for field in fields]\n",
    "    field_names.append('image_id')\n",
    "\n",
    "    buffer = []\n",
    "    # sanity check counters\n",
    "    count_parcels = defaultdict(int)\n",
    "#     index = 0\n",
    "    counter_method1 = 0\n",
    "    counter_method2 = 0\n",
    "    num_matched = 0\n",
    "    failed_projection = 0\n",
    "  \n",
    "    # loop through the polygon fields\n",
    "    for sr in tqdm(reader.iterShapeRecords(), total=9517878):\n",
    "#         if index % 100000 == 0:\n",
    "#             print('Parsed ', index)\n",
    "#         index += 1\n",
    "        geom = sr.shape.__geo_interface__\n",
    "        shp_geom = shape(geom)\n",
    "        intersect = check_polygon_in_bounds(shp_geom, tree)\n",
    "#         print(intersect)\n",
    "        if len(intersect) != 0:\n",
    "            num_matched += len(intersect)\n",
    "#             print(\"Matched:\", str(index))\n",
    "#             print(\"Number matched:\", num_matched)\n",
    "      \n",
    "            id_list = field_imageId_list(intersect, count_parcels)\n",
    "            sr.record.append(id_list)\n",
    "            atr = dict(zip(field_names, sr.record))\n",
    "            \n",
    "            geom['coordinates'] = listit(geom['coordinates'])\n",
    "            try: # protection at polygons that fail at projection\n",
    "                if len(geom['coordinates']) == 1: # for single polygon\n",
    "                    counter_method1 += 1\n",
    "                    x, y = zip(*geom['coordinates'][0])\n",
    "                    lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                    geom['coordinates'] = [listit(list(zip(lat, long)))]\n",
    "                else: # for multipolygons\n",
    "                    counter_method2 += 1\n",
    "                    for index_coord in range(0, len(geom['coordinates'])):\n",
    "                        for counter in range(0,len(geom['coordinates'][index_coord])):\n",
    "                            x, y = geom['coordinates'][index_coord][counter]\n",
    "                            lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                            geom['coordinates'][index_coord][counter] = [lat, long] #(long, lat)\n",
    "            except:\n",
    "                failed_projection =+ 1\n",
    "#                 print(geom['coordinates'])\n",
    "            buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr))\n",
    "            \n",
    "#             if num_matched > 10:\n",
    "#                 break\n",
    "      \n",
    "      \n",
    "    # write the GeoJSON file\n",
    "    output_json_interval = output_json + str(num_matched) + '.json'\n",
    "    print(\"saving json\")\n",
    "    with open(output_json_interval, 'w') as geojson:\n",
    "        geojson.write(dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2) + \"\\n\")\n",
    "        geojson.close()\n",
    "        print('saved', output_json_interval)\n",
    "    \n",
    "    # print summary\n",
    "    print('method one count:', counter_method1)\n",
    "    print('method two count:', counter_method2)\n",
    "    print(\"Number matched:\", num_matched)\n",
    "    print('failed count', failed_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fiona.open(shape_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID_PARCEL',\n",
       " 'SURF_PARC',\n",
       " 'CODE_CULTU',\n",
       " 'CODE_GROUP',\n",
       " 'CULTURE_D1',\n",
       " 'CULTURE_D2']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.schema['properties'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'MultiPolygon', 'coordinates': [[[(701200.320100002, 6883238.830900002), (700676.8660000041, 6882785.608600002), (700657.6099999994, 6882831.337000001), (700652.1803000048, 6882844.3105), (700646.9530000016, 6882853.198800001), (700627.3748000041, 6882878.875500001), (700617.5866999999, 6882893.216300003), (700613.4576999992, 6882901.178100001), (700585.1190000027, 6882964.341000002), (700568.450000003, 6883002.176000003), (700553.0282000005, 6883039.002), (700539.0053000003, 6883078.2927), (700540.7251000032, 6883084.113500003), (700639.8117000014, 6883181.4804), (700742.3322999999, 6883282.026700001), (701020.0553000048, 6883553.359200001), (701148.2096000016, 6883518.824800003), (701147.2836000025, 6883498.9810000025), (701143.9200000018, 6883446.650000002), (701143.1825000048, 6883398.5715), (701146.2990000024, 6883375.501900002), (701154.2087000012, 6883351.177100003), (701166.5234000012, 6883320.895800002), (701164.6138000041, 6883310.729600001), (701174.5357000008, 6883287.975400001), (701184.1930000037, 6883263.501400001), (701194.644100003, 6883242.8638), (701200.320100002, 6883238.830900002)]]]}\n"
     ]
    }
   ],
   "source": [
    "for t in test:\n",
    "    print(t['geometry'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = '../data/planet/france/sherrie10k/'\n",
    "# csv_file = os.path.join(base_dir, 'bbox10k.csv')\n",
    "# csv_file = os.path.join(base_dir, 'bbox10k_1250px.csv')\n",
    "csv_file = os.path.join(base_dir, 'bbox10k_2500px.csv')\n",
    "\n",
    "shape_file = '../data/parcels/france/RPG_2-0__SHP_LAMB93_FR-2018_2018-01-15/RPG/1_DONNEES_LIVRAISON_2018/RPG_2-0_SHP_LAMB93_FR-2018/PARCELLES_GRAPHIQUES.shp'\n",
    "# TODO: update shape file to 2019\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "for start in np.arange(0, 1500, 250): # np.arange(1500, 10000, 250):\n",
    "    end = start + 250\n",
    "    images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "    images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "    grid, tree = read_csv(shape_file, images_df)\n",
    "    \n",
    "    dump_shp_to_json(shape_file, grid, tree, \n",
    "                     '../data/planet/france/sherrie10k/json_polys/bbox10k_2500px_{}_'.format(int(start/250)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019 Geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_shp_to_json(shape_file, grid, tree, output_json='../data/planet/france/sherrie10k/test_json'):\n",
    "    \"\"\"\n",
    "    find intersecting polygons in the list of available images and save the GeoJSON\n",
    "\n",
    "    Args:\n",
    "      shape_file (polygons): fields\n",
    "      grid (dict): image bounding boxes \n",
    "      tree (r-tre): r-tree of images\n",
    "      output_json (str): output path of json file\n",
    "    \"\"\"\n",
    "    # coordinate transformation\n",
    "    shapes = fiona.open(shape_file)\n",
    "    if len(shapes.crs) != 0:\n",
    "        original = Proj(shapes.crs)\n",
    "    else:\n",
    "        original = Proj('+init=EPSG:4326')\n",
    "\n",
    "    # list of properties of features\n",
    "#     field_names = shapes.schema['properties'].keys()\n",
    "#     field_names.append('image_id')\n",
    "    \n",
    "    # sanity check counters\n",
    "    buffer = []\n",
    "    count_parcels = defaultdict(int)\n",
    "    index = 0\n",
    "    counter_method1 = 0\n",
    "    counter_method2 = 0\n",
    "    num_matched = 0\n",
    "    failed_projection = 0\n",
    "  \n",
    "    # loop through the polygon fields\n",
    "    for sr in tqdm(shapes, total=9517878):\n",
    "#         if index % 100000 == 0:\n",
    "#             print('Parsed ', index)\n",
    "#         index += 1\n",
    "        geom = sr['geometry']\n",
    "        shp_geom = shape(geom)\n",
    "        intersect = check_polygon_in_bounds(shp_geom, tree)\n",
    "#         print(intersect)\n",
    "        if len(intersect) != 0:\n",
    "            num_matched += len(intersect)\n",
    "#             print(\"Matched:\", str(index))\n",
    "#             print(\"Number matched:\", num_matched)\n",
    "      \n",
    "            id_list = field_imageId_list(intersect, count_parcels)\n",
    "            atr = dict(sr['properties'])\n",
    "            atr['image_id'] = id_list\n",
    "#             sr.record.append(id_list)\n",
    "#             atr = dict(zip(field_names, sr.record))\n",
    "            \n",
    "            geom['coordinates'] = listit(geom['coordinates'])\n",
    "            try: # protection at polygons that fail at projection\n",
    "                if len(geom['coordinates']) == 1: # for single polygon\n",
    "                    counter_method1 += 1\n",
    "                    x, y = zip(*geom['coordinates'][0][0])\n",
    "                    lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                    geom['coordinates'] = [listit(list(zip(lat, long)))]\n",
    "                else: # for multipolygons\n",
    "                    counter_method2 += 1\n",
    "                    for index_coord in range(0, len(geom['coordinates'])):\n",
    "                        for counter in range(0,len(geom['coordinates'][index_coord][0])):\n",
    "                            x, y = geom['coordinates'][index_coord][0][counter]\n",
    "                            lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                            geom['coordinates'][index_coord][counter] = [lat, long] #(long, lat)\n",
    "            except:\n",
    "                failed_projection += 1\n",
    "#                 print(geom['coordinates'])\n",
    "            buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr))\n",
    "            \n",
    "            # for debugging\n",
    "#             if num_matched > 10:\n",
    "#                 break\n",
    "      \n",
    "      \n",
    "    # write the GeoJSON file\n",
    "    output_json_interval = output_json + str(num_matched) + '.json'\n",
    "    print(\"saving json\")\n",
    "    with open(output_json_interval, 'w') as geojson:\n",
    "        geojson.write(dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2) + \"\\n\")\n",
    "        geojson.close()\n",
    "        print('saved', output_json_interval)\n",
    "    \n",
    "    # print summary\n",
    "    print('method one count:', counter_method1)\n",
    "    print('method two count:', counter_method2)\n",
    "    print(\"Number matched:\", num_matched)\n",
    "    print('failed count', failed_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = gpd.read_file('../data/parcels/france/RPG_2-0_GPKG_LAMB93_FR-2019/RPG/1_DONNEES_LIVRAISON_2019/RPG_2-0_GPKG_LAMB93_FR-2019/PARCELLES_GRAPHIQUES.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:2154'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcels.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GeoDataFrame' object has no attribute 'set_crs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0bc080e71115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparcels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparcels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4326\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/geopython/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GeoDataFrame' object has no attribute 'set_crs'"
     ]
    }
   ],
   "source": [
    "parcels = parcels.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "b'no arguments in initialization list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8cf752a7f15a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparcels3857\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparcels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPSG:3857\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/geopython/lib/python3.7/site-packages/geopandas/geodataframe.py\u001b[0m in \u001b[0;36mto_crs\u001b[0;34m(self, crs, epsg, inplace)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mgeom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_crs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geopython/lib/python3.7/site-packages/geopandas/geoseries.py\u001b[0m in \u001b[0;36mto_crs\u001b[0;34m(self, crs, epsg)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mproj_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mproj_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/geopython/lib/python3.7/site-packages/pyproj/__init__.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(self, projparams, preserve_units, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# on case-insensitive filesystems).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mprojstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EPSG'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'epsg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_proj.pyx\u001b[0m in \u001b[0;36m_proj.Proj.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: b'no arguments in initialization list'"
     ]
    }
   ],
   "source": [
    "parcels3857 = parcels.to_crs(\"EPSG:3857\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels3857.to_file('../data/parcels/france/RPG_2-0_GPKG_LAMB93_FR-2019/PARCELLES_GRAPHIQUES_3857.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9604463it [11:09, 14341.47it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/france/json_polys/bbox10k_1250px_0_285237.json\n",
      "method one count: 281794\n",
      "method two count: 0\n",
      "Number matched: 285237\n",
      "failed count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9604463it [10:54, 14676.94it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/france/json_polys/bbox10k_1250px_1_304170.json\n",
      "method one count: 299950\n",
      "method two count: 0\n",
      "Number matched: 304170\n",
      "failed count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9604463it [10:48, 14804.16it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/france/json_polys/bbox10k_1250px_2_292643.json\n",
      "method one count: 286693\n",
      "method two count: 0\n",
      "Number matched: 292643\n",
      "failed count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9604463it [10:51, 14747.04it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/france/json_polys/bbox10k_1250px_3_287396.json\n",
      "method one count: 283358\n",
      "method two count: 2\n",
      "Number matched: 287396\n",
      "failed count 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 8979870/9517878 [10:04<00:36, 14667.36it/s]"
     ]
    }
   ],
   "source": [
    "# base_dir = '../data/planet/france/sherrie10k/'\n",
    "base_dir = '../data/planet/france/'\n",
    "# csv_file = os.path.join(base_dir, 'bbox10k.csv')\n",
    "# csv_file = os.path.join(base_dir, 'bbox10k_1250px.csv')\n",
    "# csv_file = os.path.join(base_dir, 'bbox10k_2500px.csv')\n",
    "csv_file = os.path.join(base_dir, 'bbox_1250px_epsg3857.csv')\n",
    "\n",
    "# shape_file = '../data/parcels/france/RPG_2-0__SHP_LAMB93_FR-2018_2018-01-15/RPG/1_DONNEES_LIVRAISON_2018/RPG_2-0_SHP_LAMB93_FR-2018/PARCELLES_GRAPHIQUES.shp'\n",
    "# TODO: update shape file to 2019\n",
    "shape_file = '../data/parcels/france/RPG_2-0_GPKG_LAMB93_FR-2019/RPG/1_DONNEES_LIVRAISON_2019/RPG_2-0_GPKG_LAMB93_FR-2019/PARCELLES_GRAPHIQUES.gpkg'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "# 300px and 1250px images\n",
    "images_per_file = 1000\n",
    "for start in np.arange(0, 10000, images_per_file):\n",
    "    end = start + images_per_file\n",
    "    images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "    images_df = images_df.rename({'min_x': 'min_lon', 'max_x': 'max_lon', 'min_y': 'min_lat', 'max_y': 'max_lat'}, axis=1)\n",
    "    images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "    grid, tree = read_csv(shape_file, images_df)\n",
    "\n",
    "    dump_shp_to_json(shape_file, grid, tree, \n",
    "                     '../data/planet/france/json_polys/bbox10k_1250px_{}_'.format(int(start/images_per_file)))\n",
    "\n",
    "# 2500px images\n",
    "# images_per_file = 250 \n",
    "# for start in np.arange(250, 10000, images_per_file):\n",
    "#     end = start + images_per_file\n",
    "#     images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "#     images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "#     grid, tree = read_csv(shape_file, images_df)\n",
    "    \n",
    "#     dump_shp_to_json(shape_file, grid, tree, \n",
    "#                      '../data/planet/france/sherrie10k/json_polys_2019/bbox10k_2500px_{}_'.format(int(start/images_per_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined a new dump_shp_to_json function for india because the projection wasn't working...\n",
    "# the parcels should already be in LAT, LON and don't need to be reprojected\n",
    "# but somehow the inverse projection was messing things up\n",
    "# TODO: fix this in a general way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_shp_to_json(shape_file, grid, tree, output_json='../data/planet/france/sherrie10k/test_json'):\n",
    "    \"\"\"\n",
    "    find intersecting polygons in the list of available images and save the GeoJSON\n",
    "\n",
    "    Args:\n",
    "      shape_file (polygons): fields\n",
    "      grid (dict): image bounding boxes \n",
    "      tree (r-tre): r-tree of images\n",
    "      output_json (str): output path of json file\n",
    "    \"\"\"\n",
    "    # coordinate transformation\n",
    "    reader = shapefile.Reader(shape_file)\n",
    "    shapes = fiona.open(shape_file)\n",
    "    if len(shapes.crs) != 0:\n",
    "        original = Proj(shapes.crs)\n",
    "    else:\n",
    "        original = Proj('+init=EPSG:4326')\n",
    "    print(fiona.open(shape_file).crs)\n",
    "    print(original)\n",
    "\n",
    "    # list of properties of features\n",
    "    fields = reader.fields[1:]\n",
    "    field_names = [field[0] for field in fields]\n",
    "    field_names.append('image_id')\n",
    "\n",
    "    buffer = []\n",
    "    # sanity check counters\n",
    "    count_parcels = defaultdict(int)\n",
    "#     index = 0\n",
    "    counter_method1 = 0\n",
    "    counter_method2 = 0\n",
    "    num_matched = 0\n",
    "    failed_projection = 0\n",
    "  \n",
    "    # loop through the polygon fields\n",
    "    for sr in tqdm(reader.iterShapeRecords(), total=10000):\n",
    "#         if index % 100000 == 0:\n",
    "#             print('Parsed ', index)\n",
    "#         index += 1\n",
    "        geom = sr.shape.__geo_interface__\n",
    "        shp_geom = shape(geom)\n",
    "        intersect = check_polygon_in_bounds(shp_geom, tree)\n",
    "#         print(intersect)\n",
    "        if len(intersect) != 0:\n",
    "            num_matched += len(intersect)\n",
    "#             print(\"Matched:\", str(index))\n",
    "#             print(\"Number matched:\", num_matched)\n",
    "      \n",
    "            id_list = field_imageId_list(intersect, count_parcels)\n",
    "            sr.record.append(id_list)\n",
    "            atr = dict(zip(field_names, sr.record))\n",
    "            \n",
    "            geom['coordinates'] = listit(geom['coordinates'])\n",
    "#             print(geom)\n",
    "            try: # protection at polygons that fail at projection\n",
    "                if len(geom['coordinates']) == 1: # for single polygon\n",
    "                    counter_method1 += 1\n",
    "                    x, y = zip(*geom['coordinates'][0])\n",
    "#                     lat,long = x, y\n",
    "                    lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                    geom['coordinates'] = [listit(list(zip(lat, long)))]\n",
    "                else: # for multipolygons\n",
    "                    counter_method2 += 1\n",
    "                    for index_coord in range(0, len(geom['coordinates'])):\n",
    "                        for counter in range(0,len(geom['coordinates'][index_coord])):\n",
    "                            x, y = geom['coordinates'][index_coord][counter]\n",
    "                            lat, long = original(x, y, inverse=True) # coordinate transformation\n",
    "                            geom['coordinates'][index_coord][counter] = [lat, long] #(long, lat)\n",
    "            except:\n",
    "                failed_projection =+ 1\n",
    "#                 print(geom['coordinates'])\n",
    "            buffer.append(dict(type=\"Feature\", geometry=geom, properties=atr))\n",
    "            \n",
    "#             if num_matched > 10:\n",
    "#                 break\n",
    "      \n",
    "      \n",
    "    # write the GeoJSON file\n",
    "    output_json_interval = output_json + str(num_matched) + '.json'\n",
    "    print(\"saving json\")\n",
    "    with open(output_json_interval, 'w') as geojson:\n",
    "        geojson.write(dumps({\"type\": \"FeatureCollection\", \"features\": buffer}, indent=2) + \"\\n\")\n",
    "        geojson.close()\n",
    "        print('saved', output_json_interval)\n",
    "    \n",
    "    # print summary\n",
    "    print('method one count:', counter_method1)\n",
    "    print('method two count:', counter_method2)\n",
    "    print(\"Number matched:\", num_matched)\n",
    "    print('failed count', failed_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/planet/india/'\n",
    "csv_file = os.path.join(base_dir, 'bbox1000.csv')\n",
    "\n",
    "shape_file = '../mount/data/india_parcels/india_parcels_with_area.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "# for start in np.arange(0, 1500, 250): # np.arange(1500, 10000, 250):\n",
    "#     end = start + 250\n",
    "#     images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "\n",
    "images_df = pd.read_csv(csv_file)\n",
    "images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "images_df = images_df[images_df['image_id'].isin(['00064', '00126'])]\n",
    "\n",
    "grid, tree = read_csv(shape_file, images_df)\n",
    "\n",
    "dump_shp_to_json(shape_file, grid, tree, \n",
    "                 '../data/planet/india/json_polys/bbox1000_labeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoWiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 266/9517878 [00:00<19:01, 8339.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/india/geowiki/json_polys/geowiki_labeled259.json\n",
      "method one count: 259\n",
      "method two count: 0\n",
      "Number matched: 259\n",
      "failed count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data/planet/india/geowiki/'\n",
    "csv_file = os.path.join(base_dir, 'geowiki_maharashtra.csv')\n",
    "\n",
    "shape_file = '../mount/data/india_parcels/india_geowiki_parcels_with_area.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "images_df = pd.read_csv(csv_file)\n",
    "images_df = images_df[images_df['image_id'].isin([960228])]\n",
    "\n",
    "grid, tree = read_csv(shape_file, images_df)\n",
    "\n",
    "dump_shp_to_json(shape_file, grid, tree, \n",
    "                 '../data/planet/india/geowiki/json_polys/geowiki_labeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Blockchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10013/9517878 [00:09<2:22:33, 1111.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/india/GeneralBlockchain/json_polys/bbox_images26405.json\n",
      "method one count: 8788\n",
      "method two count: 15\n",
      "Number matched: 26405\n",
      "failed count 0\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data/planet/india/GeneralBlockchain/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_india_GB_v1.csv')\n",
    "\n",
    "shape_file = '../mount/data/GeneralBlockchain/campaign_results/india_fields_with_area.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "grid, tree = read_csv(shape_file, df)\n",
    "\n",
    "dump_shp_to_json(shape_file, grid, tree, \n",
    "                 '../data/planet/india/GeneralBlockchain/json_polys/bbox_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../data/planet/india/GeneralBlockchain/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_india_GB_download_v2.csv')\n",
    "\n",
    "shape_file = '../mount/data/GeneralBlockchain/campaign_results/india_fields_with_area.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "grid, tree = read_csv(shape_file, df)\n",
    "\n",
    "dump_shp_to_json(shape_file, grid, tree, \n",
    "                 '../data/planet/india/GeneralBlockchain/json_polys/bbox_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10013/9517878 [00:04<1:03:30, 2494.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/india/GeneralBlockchain/json_polys/bbox_images10027.json\n",
      "method one count: 9997\n",
      "method two count: 16\n",
      "Number matched: 10027\n",
      "failed count 10013\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data/planet/india/GeneralBlockchain/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_india_GB_download_actual.csv')\n",
    "\n",
    "shape_file = '../mount/data/GeneralBlockchain/campaign_results/india_fields_with_area.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "grid, tree = read_csv(shape_file, df)\n",
    "\n",
    "dump_shp_to_json(shape_file, grid, tree, \n",
    "                 '../data/planet/india/GeneralBlockchain/json_polys/bbox_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = fiona.open(shape_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:4326'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Airbus images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9966/9517878 [00:04<1:09:28, 2280.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/general_blockchain/json_polys/bbox_Airbus_large9975.json\n",
      "method one count: 9898\n",
      "method two count: 68\n",
      "Number matched: 9975\n",
      "failed count 0\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../data/general_blockchain/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_india_GB_large_Airbus.csv')\n",
    "\n",
    "shape_file = '../mount/data/GeneralBlockchain/campaign_results/india_fields_with_area.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "grid, tree = read_csv(shape_file, df)\n",
    "\n",
    "dump_shp_to_json(shape_file, grid, tree, \n",
    "                 '../data/general_blockchain/json_polys/bbox_Airbus_large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14543, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '../data/planet/senegal/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_tiles_all.csv')\n",
    "\n",
    "shape_file = '../mount/data/senegal_parcels/SenegalFields_03_26.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2590/9517878 [00:01<1:17:12, 2053.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/senegal/json_polys/bbox_tiles_0_3293.json\n",
      "method one count: 2579\n",
      "method two count: 10\n",
      "Number matched: 3293\n",
      "failed count 1\n"
     ]
    }
   ],
   "source": [
    "increment = 20000\n",
    "    \n",
    "for start in np.arange(0, df.shape[0], increment):\n",
    "    end = start + increment\n",
    "    images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "\n",
    "    images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "    grid, tree = read_csv(shape_file, images_df)\n",
    "    \n",
    "    dump_shp_to_json(shape_file, grid, tree, \n",
    "                     '../data/planet/senegal/json_polys/bbox_tiles_{}_'.format(int(start/increment)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ghana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18023, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '../data/planet/ghana/udry/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_tiles_all.csv')\n",
    "\n",
    "shape_file = '../mount/data/udry_parcels/udry_fields_2017.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8938/9517878 [00:04<1:23:23, 1900.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/ghana/udry/json_polys/bbox_tiles_0_111334.json\n",
      "method one count: 8938\n",
      "method two count: 0\n",
      "Number matched: 11334\n",
      "failed count 0\n"
     ]
    }
   ],
   "source": [
    "increment = 20000\n",
    "    \n",
    "for start in np.arange(0, df.shape[0], increment):\n",
    "    end = start + increment\n",
    "    images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "\n",
    "    images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "    grid, tree = read_csv(shape_file, images_df)\n",
    "    \n",
    "    dump_shp_to_json(shape_file, grid, tree, \n",
    "                     '../data/planet/ghana/udry/json_polys/bbox_tiles_{}_1'.format(\n",
    "                         int(start/increment)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Malawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2372, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = '../data/planet/malawi/'\n",
    "csv_file = os.path.join(base_dir, 'bbox_tiles_all.csv')\n",
    "\n",
    "shape_file = '../mount/data/malawi_parcels/malawi_WFP_fields_2018.shp'\n",
    "\n",
    "if os.path.exists(os.path.join(base_dir, 'json_polys')) == False:\n",
    "    os.makedirs(os.path.join(base_dir, 'json_polys'))\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 423/9517878 [00:00<1:16:26, 2074.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving json\n",
      "saved ../data/planet/malawi/json_polys/bbox_tiles_0_499.json\n",
      "method one count: 423\n",
      "method two count: 0\n",
      "Number matched: 499\n",
      "failed count 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "increment = 20000\n",
    "    \n",
    "for start in np.arange(0, df.shape[0], increment):\n",
    "    end = start + increment\n",
    "    images_df = pd.read_csv(csv_file).iloc[start:end]\n",
    "\n",
    "    images_df['image_id'] = images_df['image_id'].astype(str).str.zfill(5)\n",
    "    grid, tree = read_csv(shape_file, images_df)\n",
    "    \n",
    "    dump_shp_to_json(shape_file, grid, tree, \n",
    "                     '../data/planet/malawi/json_polys/bbox_tiles_{}_'.format(int(start/increment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_geopython)",
   "language": "python",
   "name": "conda_geopython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
